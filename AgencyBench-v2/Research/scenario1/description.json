{
  "subtask_count": 5,
  "subtask1": "Query:\nPerform a comprehensive orientation of the task by the sample schemas in `data/test_data_1.json`, `data/test_data_2.json`, and `data/test_data_3.json`. Summarize every constraint (workspace-only operations, README + random sample extraction, JSON schema expectations) and outline the execution flow that will be used for the downstream subtasks. The model must operate inside the current working directory, produce an English briefing that captures dataset requirements, toolchain assumptions, and a high-level checklist, and enumerate the files that must exist before any search begins. Final output is an English markdown report documenting the understanding and plan.\nDeliverables:\n- `datacontext_brief.md` containing sections for Environment, Mandatory Files, Dataset Requirements (per subtask), and Execution Plan.\nRubric:\n- Score 10: briefing covers every constraint from the JSON spec and AGENTS instructions, references all initial files with correct paths, and provides a step-by-step plan with risk mitigation notes.\n- Score 5: briefing captures the major requirements but misses at least one constraint (e.g., workspace-only rule or JSON field definition) or lacks an actionable plan.\n- Score 0: briefing missing, written outside the workspace, or omits most requirements.",
  "subtask2": "Query:\nIdentify a Hugging Face dataset that matches the AI-consciousness synthetic QA criteria (machine-generated, ~30-100 prompt/answer pairs about AI consciousness and emotions). Within the workspace, implement a script that fetches the README and a random sample from the selected dataset_id, stores the artifacts locally, and builds metadata conforming to the test_data_1 schema. Inputs include the orientation report from subtask1 and any reusable helper scripts; no external files outside `data` may be touched. Final goal: produce a validated `search_data_1.json` summarizing introduction, task, question, input, output, example, domain/source tags, and sample counts.\nDeliverables:\n- `datascripts/fetch_consciousness_dataset.py` (or equivalent) that accepts the dataset_id and exports README text plus at least one random record.\n- `dataraw/EpistemeAI_readme.md` (or equivalent) storing the pulled README for traceability.\n- `datasearch_data_1.json` matching the sample schema.\nRubric:\n- Score 10: dataset is public and synthetic, README + random sample captured via script, JSON fields perfectly mirror the schema with precise descriptions tied to the dataset, and the script reruns without manual edits.\n- Score 7: dataset and JSON mostly correct but script has minor reproducibility gaps (e.g., hard-coded paths) or metadata omits one secondary field.\n- Score 3: dataset mismatches requirements or README/sample missing; JSON incomplete or poorly aligned with actual data.\n- Score 0: no deliverables or dataset not sourced from Hugging Face.",
  "subtask3": "Query:\nDiscover a Hugging Face dataset containing roughly 10K-15K academic multiple-choice questions transformed into different English varieties, each with four options and integer labels 0-3. Operate entirely inside the workspace to script the README/sample extraction, document provenance, and generate `search_data_2.json` consistent with the provided schema. Inputs consist of the orientation deliverable, prior helper utilities, and any dataset notes gathered during research. Output must clearly state subject tagging, dialect transformation nature, label semantics, and dataset size.\nDeliverables:\n- `datascripts/fetch_transenv_dataset.py` that downloads the README and randomly sampled records while handling pagination/shards if needed.\n- `dataraw/TransEnV_readme.md` (or similar) containing the README snapshot.\n- `datasearch_data_2.json` summarizing the dataset per schema.\nRubric:\n- Score 10: chosen dataset satisfies all structural requirements (subject tags, four choices, integer labels, ~10K-15K samples) and the generated JSON precisely documents the task, with script + README artifacts reproducible.\n- Score 6: dataset mostly compliant but size or documentation slightly outside spec, or JSON misses one descriptive element.\n- Score 2: dataset choice invalid (wrong task, size far off) or deliverables incomplete.\n- Score 0: nothing produced or dataset not from Hugging Face.",
  "subtask4": "Query:\nLocate a non-gated Hugging Face dataset of Danish tweets labeled OFF/NOT for offensive-language detection (few-hundred samples). Inside the workspace, automate README retrieval, take a random sample, and build `search_data_3.json` describing the dataset across the six metadata dimensions plus supporting fields. Must confirm the data is real-world (human-authored tweets) and highlight sociological relevance.\nDeliverables:\n- `datascripts/fetch_danish_offense_dataset.py` capturing README and sample.\n- `dataraw/Danish_offense_readme.md` (or similar) storing the documentation snapshot.\n- `datasearch_data_3.json` aligned to the schema.\nRubric:\n- Score 10: dataset fully matches language/label/size constraints, README + sample saved, metadata descriptions grounded in evidence, and script reruns deterministically.\n- Score 6: dataset fits but metadata is partially generic or sample handling is fragile.\n- Score 2: dataset gated, wrong language, or JSON fails validation.\n- Score 0: deliverables missing.\n",
  "subtask5": "Query:\nConsolidate all prior work by validating JSON structure, referencing README snippets, and producing a final QA log. Working solely within workspace, run linting/validation commands (e.g., `python -m json.tool` or custom checks) against `search_data_1.json`, `search_data_2.json`, and `search_data_3.json`, summarize the verification steps, and note any manual inspection performed on the random samples. Final goal is an auditable record proving that deliverables meet requirements and referencing how to reproduce the extraction scripts.\nDeliverables:\n- `datavalidation_report.md` detailing validation commands, dataset_ids used, README file paths, sample records reviewed, and any TODOs.\n- Updated helper scripts (if needed) with inline usage instructions.\nRubric:\n- Score 10: report enumerates every validation command with outcomes, links to all deliverables, and confirms schema, reproducibility, and requirement coverage.\n- Score 5: report exists but omits one JSON or lacks reproduction notes.\n- Score 1: superficial report without concrete commands or evidence.\n- Score 0: no report or validation performed."
}