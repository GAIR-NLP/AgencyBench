{
    "subtask_count": 5,
    "subtask1": "Query:\nYou are working in a Linux environment where the working directory is `/workspace/`. Under `/workspace/datasets/`, you are given two splits of graph-based algorithmic problems:\n- `/workspace/datasets/develop/graph_connectivity/`\n- `/workspace/datasets/test/graph_connectivity/`\n\nEach graph connectivity sample is stored in a JSON file named `example.json` with the following fields:\n```json\n{\n  \"adjacency_matrix\": \"[[0, 0, 1], [0, 0, 0], [1, 0, 0]]\",\n  \"query_node_1\": 12,\n  \"query_node_2\": 10,\n  \"label\": false,\n  \"id\": \"isobench/algorithm/connectivity_008\"\n}\n```\nFor the **develop** split, the `label` field is present and indicates whether `query_node_1` and `query_node_2` are connected in the directed graph defined by `adjacency_matrix`. For the **test** split, the `label` field is **omitted** and you must predict it.\n\nYour task in this subtask is to build the **first, simplest version** of a mathematical reasoning pipeline that:\n1. Correctly parses graph connectivity problems.\n2. Uses a classical graph algorithm (e.g., BFS or DFS) to determine if the two queried nodes are connected.\n3. Uses the `gpt-4o` model only to generate **natural-language reasoning and solution steps**, while the actual boolean connectivity result is computed by your algorithm.\n4. Writes outputs for **all test connectivity problems** into a standardized JSON format under `/workspace/outputs/`.\n\nThe environment provides either OpenAI GPT-4o or Azure OpenAI GPT-4o via environment variables. Exactly one of the following will be available:\n- `OPENAI_API_KEY` and `OPENAI_BASE_URL` (for OpenAI API), or\n- `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, and `AZURE_OPENAI_API_VERSION` (for Azure OpenAI).\n\nYou must implement a robust client that detects which API is available and always uses model `gpt-4o` for text-only reasoning. You are also given example scripts `simple_api_example.py` and `simple_azure_example.py` that show how to call GPT-4o with and without images; you should follow their pattern but you only need **text-only** calls in this subtask.\n\nDeliverables:\n- Create a Python script at `/workspace/outputs/main.py` that can be executed with:\n  ```bash\n  python /workspace/outputs/main.py\n  ```\n\n- Inside `main.py`, implement the following components:\n  1. **API Client Wrapper**:\n     - A small module or class that:\n       - Detects at runtime whether to use OpenAI or Azure OpenAI based on the environment variables.\n       - Exposes a function `llm_explain_connectivity(problem_json, is_connected)` that:\n         - Takes the parsed problem JSON and the boolean result from your algorithm.\n         - Calls `gpt-4o` (text-only) to produce a concise explanation and a short list of solution steps, *without* asking the model to choose the boolean answer.\n  2. **Data Loader for graph_connectivity**:\n     - Recursively traverse `/workspace/datasets/test/graph_connectivity/`.\n     - For each problem directory, open `example.json`, parse the adjacency matrix (string) into a Python list of lists, and read `query_node_1`, `query_node_2`, and `id`.\n  3. **Graph Connectivity Solver**:\n     - Implement a standard BFS or DFS on the directed graph represented by the adjacency matrix.\n     - Determine whether there is a path from `query_node_1` to `query_node_2`.\n     - Return a Python boolean (`True` if connected, `False` otherwise).\n  4. **Result Writer**:\n     - For each test problem, create a directory following this pattern:\n       - `/workspace/outputs/graph_connectivity/{problem_id}/`\n       - Here `{problem_id}` is the trailing numeric ID derived from the `id` field, e.g., from `\"isobench/algorithm/connectivity_008\"` you must derive `\"8\"` or `\"008\"` and use a consistent convention. Clearly document your convention in code comments.\n     - Inside that directory, create a file `result.json` with the following structure:\n       ```json\n       {\n         \"task_type\": \"graph_connectivity\",\n         \"problem_id\": \"10\",\n         \"answer\": false,\n         \"reasoning\": \"Applied BFS on the directed adjacency matrix; node 11 is not reachable from node 4.\",\n         \"solution_steps\": [\n           \"Parse the adjacency matrix into a graph representation\",\n           \"Run BFS starting from query_node_1\",\n           \"Track all reachable nodes\",\n           \"Check if query_node_2 is in the reachable set\"\n         ],\n         \"confidence\": 0.99\n       }\n       ```\n       - `answer` must be a JSON boolean (`true` or `false`).\n       - `reasoning` and `solution_steps` should be based on GPT-4o's explanation for this specific instance.\n       - `confidence` can be a heuristic number between 0 and 1 (e.g., always 0.99 since algorithmic results are exact).\n  5. **Summary File**:\n     - After processing all test connectivity problems, generate `/workspace/outputs/summary.json` with at least:\n       ```json\n       {\n         \"total_problems\": 280,\n         \"completed_problems\": <number_of_test_problems_you_completed>,\n         \"task_results\": {\n           \"geometry\": {\"total\": 24, \"completed\": 0},\n           \"graph_connectivity\": {\"total\": 64, \"completed\": 64},\n           \"graph_maxflow\": {\"total\": 64, \"completed\": 0},\n           \"math_convexity\": {\"total\": 128, \"completed\": 0}\n         },\n         \"execution_time\": \"<fill with a rough human-readable estimate, e.g. '20 minutes'>\",\n         \"average_confidence\": <average_of_all_confidence_values_for_connectivity>\n       }\n       ```\n       - For this subtask, you only need to fully support **graph_connectivity**, but you must reserve the other tasks in the summary with correct totals and `completed` set to 0.\n\nRubric:\n- **File Structure (40%)**:\n  - `/workspace/outputs/main.py` and runs without crashing.\n  - For every test connectivity sample, a directory `/workspace/outputs/graph_connectivity/{problem_id}/` exists with a `result.json` file.\n  - `summary.json` exists in `/workspace/outputs/` and conforms to the required key structure.\n- **Answer Format (20%)**:\n  - Every `result.json` contains `task_type`, `problem_id`, `answer`, `reasoning`, `solution_steps`, and `confidence`.\n  - `answer` is a valid JSON boolean, not a string.\n  - No malformed JSON files.\n- **Algorithmic Correctness (25%)**:\n  - Your BFS/DFS implementation correctly matches the ground truth labels on the **develop** split when you test locally (you should validate it yourself, though this is not required to be coded into `main.py`).\n  - By construction, it should achieve near-100% accuracy on the test connectivity problems.\n- **GPT-4o Integration (15%)**:\n  - API client correctly detects OpenAI vs Azure environment.\n  - For at least a subset of problems (ideally all), `reasoning` and `solution_steps` clearly reflect per-instance explanations produced by GPT-4o.\n  - The model used is always `gpt-4o`.\n\nOnce this subtask is complete, you will have a working environment, a standardized output format, a summary file, and a fully solved `graph_connectivity` pipeline that later subtasks will extend to additional mathematical tasks.",
    "subtask2": "Query:\nYou now extend your existing mathematical reasoning pipeline to support **graph maximum flow** problems in addition to graph connectivity. You still work under the same environment:\n- Working directory: `/workspace/`\n- Dataset root: `/workspace/datasets/`\n- Output root: `/workspace/outputs/`\n- Model: `gpt-4o` via either OpenAI or Azure OpenAI API (detected from environment variables).\n\nThe dataset provides graph max-flow problems under:\n- `/workspace/datasets/develop/graph_maxflow/`\n- `/workspace/datasets/test/graph_maxflow/`\n\nEach sample is stored in an `example.json` file with the following structure in the **develop** split:\n```json\n{\n  \"source_node\": 0,\n  \"sink_node\": 2,\n  \"adjacency_matrix\": \"[[0, 2, 7], [0, 0, 3], [0, 0, 0]]\",\n  \"label\": 9,\n  \"id\": \"isobench/algorithm/maxflow_105\"\n}\n```\nFor the **test** split, the `label` field is omitted and you must predict it.\n\nYour task in this subtask is to build on the framework from Subtask 1 (connectivity) and:\n1. Add a classical **maximum flow solver** (e.g., Edmonds–Karp or Dinic) that operates on the adjacency matrix.\n2. Use your algorithm to compute the integer maximum flow from `source_node` to `sink_node` for each test problem.\n3. Use GPT-4o only to generate **natural-language reasoning and solution steps**, not to estimate the numeric max flow.\n4. Produce standardized output result files for **all test max-flow problems**, while preserving and reusing the graph connectivity pipeline from Subtask 1.\n\nDeliverables:\n- Reuse `/workspace/outputs/main.py` from Subtask 1 and extend it with the following new components:\n\n  1. **Max-Flow Data Loader**:\n     - Traverse `/workspace/datasets/test/graph_maxflow/`.\n     - For each problem directory, open `example.json`.\n     - Parse `adjacency_matrix` (string) into a capacity matrix (list of lists of nonnegative integers).\n     - Read `source_node`, `sink_node`, and `id`.\n\n  2. **Maximum Flow Solver**:\n     - Implement Edmonds–Karp (BFS-based Ford–Fulkerson) or a similar exact algorithm.\n     - Given the capacity matrix, `source_node`, and `sink_node`, compute the integer maximum flow.\n     - The algorithm should handle directed edges, assuming `adjacency_matrix[i][j]` is the capacity from node `i` to node `j`.\n\n  3. **GPT-4o Explanation Generator for Max-Flow**:\n     - Extend your API wrapper with a function `llm_explain_maxflow(problem_json, max_flow_value)` that:\n       - Receives both the parsed problem and the exact integer result from your algorithm.\n       - Calls `gpt-4o` (text-only) to generate:\n         - A one-paragraph explanation describing how the flow is decomposed into augmenting paths.\n         - A small ordered list of solution steps.\n       - Does *not* ask GPT-4o to compute or guess the max-flow value; you provide it.\n\n  4. **Result Writer for graph_maxflow**:\n     - For each test problem, create a directory:\n       - `/workspace/outputs/graph_maxflow/{problem_id}/`\n       - Derive `{problem_id}` consistently from the `id` field, analogous to Subtask 1 (e.g., from `\"maxflow_105\"` you might use `\"105\"`). Use the same extraction style across all tasks.\n     - Inside that directory, write `result.json` with the structure:\n       ```json\n       {\n         \"task_type\": \"graph_maxflow\",\n         \"problem_id\": \"10\",\n         \"answer\": 16,\n         \"reasoning\": \"Using the Edmonds–Karp algorithm, the network admits two augmenting paths 0→1→2 and 0→2 with capacities 9 and 7 respectively, but the overall bottleneck yields a maximum flow of 16.\",\n         \"solution_steps\": [\n           \"Parse the adjacency matrix into a capacity graph\",\n           \"Run Edmonds–Karp from source to sink, repeatedly finding augmenting paths\",\n           \"Sum the bottleneck capacities over all augmenting paths\",\n           \"Return the total as the maximum flow value\"\n         ],\n         \"confidence\": 0.98\n       }\n       ```\n       - `answer` must be an integer.\n       - `reasoning` and `solution_steps` must be consistent with the integer you computed.\n       - `confidence` can be a fixed high value (e.g., 0.98) since the algorithm is exact.\n\n  5. **Unified Orchestration in main.py**:\n     - Modify `main.py` so that a single run of:\n       ```bash\n       python /workspace/outputs/main.py\n       ```\n       performs the following steps in sequence:\n       - Solves all test `graph_connectivity` problems (as implemented in Subtask 1).\n       - Solves all test `graph_maxflow` problems.\n       - Writes all corresponding result files.\n       - Finally regenerates `/workspace/outputs/summary.json` with updated values, for example:\n         ```json\n         {\n           \"total_problems\": 280,\n           \"completed_problems\": <number_of_test_problems_you_completed>,\n           \"task_results\": {\n             \"geometry\": {\"total\": 24, \"completed\": 0},\n             \"graph_connectivity\": {\"total\": 64, \"completed\": 64},\n             \"graph_maxflow\": {\"total\": 64, \"completed\": 64},\n             \"math_convexity\": {\"total\": 128, \"completed\": 0}\n           },\n           \"execution_time\": \"<updated estimate covering both tasks>\",\n           \"average_confidence\": <average_confidence_over_all_solved_connectivity_and_maxflow_instances>\n         }\n         ```\n\nRubric:\n- **Backward Compatibility (20%)**:\n  - The connectivity pipeline from Subtask 1 continues to work unchanged: same directory layout, same file formats, same correctness.\n  - Running `main.py` still produces valid connectivity `result.json` files.\n\n- **Max-Flow Correctness (35%)**:\n  - Your max-flow implementation reproduces the known labels on the **develop** split when you test locally.\n  - The algorithm works for all test graphs without timeouts or errors.\n  - All max-flow `answer` fields are integers and match the algorithmic results.\n\n- **Integration & Output Structure (25%)**:\n  - For every test max-flow sample, a properly named directory and `result.json` file exists.\n  - `summary.json` reflects completion of both graph tasks and is still valid JSON with the required structure.\n\n- **GPT-4o Reasoning Quality (20%)**:\n  - Explanations for max-flow are coherent, align with the computed flow value, and describe augmenting-path logic or capacity reasoning.\n  - Model usage remains restricted to `gpt-4o` and continues to be routed through your API wrapper with environment detection.\n\nAfter completing this subtask, you will have a two-task framework (connectivity + max-flow) with robust algorithmic solvers and GPT-4o-generated explanations, ready to be extended to more abstract mathematical reasoning tasks.",
    "subtask3": "Query:\nYou now extend your mathematical reasoning framework to handle **function convexity** problems in addition to the two graph tasks. The environment and conventions remain the same:\n- Working directory: `/workspace/`\n- Dataset root: `/workspace/datasets/`\n- Output root: `/workspace/outputs/`\n- Model: `gpt-4o` (OpenAI or Azure, detected by environment variables)\n- Existing support: `graph_connectivity` and `graph_maxflow` are already fully implemented and producing correct outputs.\n\nFunction convexity problems are located at:\n- `/workspace/datasets/develop/math_convexity/`\n- `/workspace/datasets/test/math_convexity/`\n\nEach sample is stored in an `example.json` file with the following structure for the **develop** split:\n```json\n{\n  \"domain\": \"x > 0\",\n  \"latex\": \"$$f(x) = 0.19 \\\\log{\\\\left(x \\\\right)} - 1.52 \\\\left|{x}\\\\right| + 3.58$$\",\n  \"code\": \"f(x) = 0.19*log(x) - 1.52*Abs(x) + 3.58\",\n  \"label\": \"concave\",\n  \"id\": \"isobench/math/convexity_129\"\n}\n```\nFor the **test** split, `label` is omitted and you must predict whether the function is `\"convex\"`, `\"concave\"`, or `\"neither\"` over the specified `domain`.\n\nUnlike the graph tasks, these problems are well-suited to symbolic reasoning via GPT-4o. In this subtask, GPT-4o should be the **primary engine** for deciding the convexity label, but you are encouraged to use light-weight numeric or derivative checks in Python to sanity-check results when possible.\n\nYour goals:\n1. Design a prompting strategy that uses the **develop** split as in-context examples.\n2. Query GPT-4o to determine convexity for each test example.\n3. Produce standardized `result.json` files for all test convexity problems.\n4. Keep the existing graph pipelines intact and still driven from a single `main.py` entry point.\n\nDeliverables:\n- Extend `/workspace/outputs/main.py` with support for `math_convexity`:\n\n  1. **Few-shot Prompt Builder**:\n     - Implement a function that loads a **small subset** (e.g., 5–10) of examples from `/workspace/datasets/develop/math_convexity/`.\n     - For each selected example, extract `domain`, `latex`, `code`, and `label`.\n     - Build a reusable prompt template for GPT-4o that:\n       - First, provides several example input-output pairs:\n         - Each example includes domain description, LaTeX expression, code representation, and the correct label.\n       - Then asks GPT-4o to analyze a new function (from the test split) and return **only one of** `\"convex\"`, `\"concave\"`, or `\"neither\"`, plus a short explanation.\n       - Emphasizes careful reasoning about second derivatives, absolute values, exponentials, logarithms, and domain restrictions.\n\n  2. **Convexity Solver**:\n     - For each test file in `/workspace/datasets/test/math_convexity/`:\n       - Parse `domain`, `latex`, `code`, and `id`.\n       - Call GPT-4o using the prompt built above to obtain:\n         - A predicted label in `{ \"convex\", \"concave\", \"neither\" }`.\n         - A concise explanation text (which will later be stored as `reasoning`).\n       - Optionally (not required but encouraged), implement a lightweight numeric sanity-check function that:\n         - Evaluates the function at a few sample points within the domain.\n         - Approximates the second derivative numerically.\n         - Warns (through logs) if GPT-4o’s label seems inconsistent with numeric curvature, though the final label still comes from GPT-4o in this subtask.\n\n  3. **Result Writer for math_convexity**:\n     - For each test problem, create:\n       - `/workspace/outputs/math_convexity/{problem_id}/result.json`\n       - Derive `{problem_id}` from the `id` field (e.g., from `\"convexity_129\"` use `\"129\"`).\n     - The `result.json` must have the structure:\n       ```json\n       {\n         \"task_type\": \"math_convexity\",\n         \"problem_id\": \"126\",\n         \"answer\": \"convex\",\n         \"reasoning\": \"Analyzed the second derivative of the function on the domain x > 0. The exponential terms and |x| contribution dominate to keep the second derivative nonnegative.\",\n         \"solution_steps\": [\n           \"Parse the function and its domain from the code representation\",\n           \"Differentiate twice symbolically or conceptually\",\n           \"Analyze the sign of the second derivative over the domain\",\n           \"Conclude whether the function is convex, concave, or neither\"\n         ],\n         \"confidence\": 0.85\n       }\n       ```\n       - `answer` must be one of `\"convex\"`, `\"concave\"`, or `\"neither\"` in lowercase.\n       - `reasoning` and `solution_steps` should summarize GPT-4o’s explanation.\n       - `confidence` may be based on GPT-4o’s self-reported confidence (if you design the prompt that way) or a heuristic based on agreement with numeric checks.\n\n  4. **Updated Orchestration & Summary**:\n     - Modify `main.py` so that a single run now:\n       - Solves all test `graph_connectivity` problems.\n       - Solves all test `graph_maxflow` problems.\n       - Solves all test `math_convexity` problems.\n       - Writes all respective `result.json` files.\n     - Regenerate `/workspace/outputs/summary.json` to reflect convexity completion, for example:\n       ```json\n       {\n         \"total_problems\": 280,\n         \"completed_problems\": 280 - 24,\n         \"task_results\": {\n           \"geometry\": {\"total\": 24, \"completed\": 0},\n           \"graph_connectivity\": {\"total\": 64, \"completed\": 64},\n           \"graph_maxflow\": {\"total\": 64, \"completed\": 64},\n           \"math_convexity\": {\"total\": 128, \"completed\": 128}\n         },\n         \"execution_time\": \"<updated estimate including all three tasks>\",\n         \"average_confidence\": <average_confidence_across_all_three_task_types>\n       }\n       ```\n       - You may estimate `completed_problems` and `average_confidence` based on what your pipeline actually processed.\n\nRubric:\n- **Backward Compatibility (20%)**:\n  - The graph tasks continue to work as in Subtasks 1–2; outputs and formats are unchanged and correct.\n\n- **Prompt Design & Use of Develop Data (30%)**:\n  - Few-shot prompt includes realistic and representative examples from the develop split.\n  - Instructions clearly constrain GPT-4o to output exactly one of the three allowed labels.\n  - The prompt encourages explicit reasoning about the second derivative and the domain.\n\n- **Output Quality for math_convexity (30%)**:\n  - Every test convexity problem has a corresponding `result.json` with the required fields and valid `answer` values.\n  - Reasoning is coherent and mathematically sensible for typical functions.\n\n- **Summary Integration (20%)**:\n  - `summary.json` now counts convexity problems as completed.\n  - `average_confidence` is updated to aggregate across connectivity, max-flow, and convexity tasks.\n\nAfter completing this subtask, your framework can solve three different mathematical reasoning tasks, using exact algorithms where appropriate and GPT-4o reasoning for more abstract calculus-based questions.",
    "subtask4": "Query:\nYou now extend your mathematical reasoning framework to the fourth and most complex task type: **geometry problems with diagrams**. You must integrate **multimodal** usage of GPT-4o (image + text) and produce answers among multiple-choice options.\n\nEnvironment and existing components:\n- Working directory: `/workspace/`\n- Dataset root: `/workspace/datasets/`\n- Output root: `/workspace/outputs/`\n- Model: `gpt-4o` (OpenAI or Azure, detected by environment variables).\n- Existing support: `graph_connectivity`, `graph_maxflow`, and `math_convexity` are fully implemented, with a single `main.py` solving all their test problems and writing outputs.\n\nGeometry datasets:\n- `/workspace/datasets/develop/geometry/`\n- `/workspace/datasets/test/geometry/`\n\nEach geometry sample lives in a directory containing:\n- `ex.json` – structured problem description\n- `image.png` – the diagram\n\nA typical `ex.json` for the **develop** split looks like:\n```json\n{\n  \"problem_text\": \"In \\\\odot K, M N = 16 and m \\\\widehat M N = 98. Find the measure of L N.\",\n  \"choices\": [\"6.93\", \"7.50\", \"8.94\", \"10.00\"],\n  \"answer\": \"C\",\n  \"problem_type_graph\": [\"Circle\"],\n  \"problem_type_goal\": [\"Length\"],\n  \"logic_form\": {\n    \"text_logic_form\": [\"Circle(K)\", \"Equals(LengthOf(Line(M,N)),16)\", \"...\"]\n  }\n}\n```\nFor the **test** split, the `answer` field is omitted and you must choose the correct option label `\"A\"`, `\"B\"`, `\"C\"`, or `\"D\"`.\n\nYour goals in this subtask:\n1. Integrate multimodal GPT-4o calls that accept both `problem_text` and `image.png`.\n2. Use the model to reason about the diagram, text, and optional logic form to pick the correct letter.\n3. Generate standardized `result.json` files for all test geometry problems.\n4. Fully unify all four task types into a single, end-to-end pipeline controlled by `/workspace/outputs/main.py`.\n\nDeliverables:\n- Extend `/workspace/outputs/main.py` with support for `geometry`:\n\n  1. **Geometry Data Loader**:\n     - Traverse `/workspace/datasets/test/geometry/`.\n     - For each problem directory:\n       - Load `ex.json` and parse `problem_text`, `choices`, `problem_type_graph`, `problem_type_goal`, and `logic_form` (if present).\n       - Load `image.png` from the same directory as a binary image that can be passed to GPT-4o.\n       - Derive a `problem_id` from the directory name or from an explicit field, using a consistent convention across all geometry instances (e.g., if directories are named `8`, `67`, etc., reuse that).\n\n  2. **Multimodal Prompting & Inference**:\n     - Implement a function `solve_geometry_with_llm(image_path, ex_json)` that:\n       - Calls GPT-4o with both the diagram (`image.png`) and the text context.\n       - Supplies `problem_text`, the four numeric `choices`, and any useful logic form as input.\n       - Instructs GPT-4o to:\n         - Carefully analyze the diagram and the text.\n         - Show internal reasoning but ultimately output **only** a choice letter from `{\"A\", \"B\", \"C\", \"D\"}` plus a short explanation.\n       - Extracts from GPT-4o’s response:\n         - The letter answer.\n         - A coherent textual explanation.\n         - A sequence of high-level solution steps (to be stored as a list of strings).\n\n  3. **Result Writer for geometry**:\n     - For each test geometry problem, create:\n       - `/workspace/outputs/geometry/{problem_id}/result.json`\n     - The `result.json` must have the structure:\n       ```json\n       {\n         \"task_type\": \"geometry\",\n         \"problem_id\": \"8\",\n         \"answer\": \"C\",\n         \"reasoning\": \"Based on the arc-chord relationship in circle K, the length of LN corresponding to arc MN = 98° with chord MN = 16 is approximately 8.94, which matches option C.\",\n         \"solution_steps\": [\n           \"Identify the given circle and chords from the diagram\",\n           \"Relate the central angle and intercepted arc to chord length formulas\",\n           \"Substitute MN = 16 and arc MN = 98° into the appropriate relation\",\n           \"Compute and compare with the answer choices to select option C\"\n         ],\n         \"confidence\": 0.8\n       }\n       ```\n       - `answer` must be exactly one of `\"A\"`, `\"B\"`, `\"C\"`, or `\"D\"`.\n       - `reasoning` and `solution_steps` must be consistent with the chosen letter and reflect GPT-4o’s explanation.\n       - `confidence` can be set based on GPT-4o’s self-assessment or a heuristic (e.g., lower for visually complex diagrams).\n\n  4. **Full Orchestration & Robustness**:\n     - Ensure `main.py` now, in a single run, does all of the following:\n       - Solves all test problems for:\n         - `geometry`\n         - `graph_connectivity`\n         - `graph_maxflow`\n         - `math_convexity`\n       - Writes all `result.json` files in their respective task subdirectories.\n       - Regenerates `/workspace/outputs/summary.json` with all tasks marked as completed, for example:\n         ```json\n         {\n           \"total_problems\": 280,\n           \"completed_problems\": 280,\n           \"task_results\": {\n             \"geometry\": {\"total\": 24, \"completed\": 24},\n             \"graph_connectivity\": {\"total\": 64, \"completed\": 64},\n             \"graph_maxflow\": {\"total\": 64, \"completed\": 64},\n             \"math_convexity\": {\"total\": 128, \"completed\": 128}\n           },\n           \"execution_time\": \"<estimate for the full pipeline run>\",\n           \"average_confidence\": <average_over_all_four_task_types>\n         }\n         ```\n     - Add basic error handling:\n       - If GPT-4o fails on a geometry instance, retry once.\n       - If it still fails, write a fallback `result.json` with `answer` set to a placeholder choice (e.g., `\"A\"`), and set a low `confidence` indicating uncertainty.\n\nRubric:\n- **Multimodal Integration (35%)**:\n  - GPT-4o is correctly called with both `image.png` and textual context for geometry problems.\n  - Prompts are clear about output format and choice constraints.\n\n- **Output Correctness & Format (30%)**:\n  - Each geometry test problem has a corresponding `result.json` with the exact required fields.\n  - `answer` letters are valid and consistently derived from GPT-4o’s response.\n\n- **Pipeline Completeness (20%)**:\n  - A single run of `main.py` now processes all four task types and regenerates an accurate `summary.json`.\n\n- **Robustness & Error Handling (15%)**:\n  - Reasonable behavior on API failures (e.g., retries, low-confidence placeholders).\n  - No crashes when iterating through all geometry problems.\n\nAfter this subtask, you have a complete multimodal mathematical reasoning system that can solve geometry, graph connectivity, graph max-flow, and function convexity problems end-to-end.",
    "subtask5": "Query:\nYou have built a full mathematical reasoning pipeline solving four task types:\n- Geometry (multimodal GPT-4o with images and text)\n- Graph connectivity (algorithmic + GPT-4o explanations)\n- Graph max-flow (algorithmic + GPT-4o explanations)\n- Function convexity (GPT-4o based with few-shot examples)\n\nAll outputs are written under `/workspace/outputs/` with task-specific subdirectories and a global `summary.json`. You can run everything via:\n```bash\npython /workspace/outputs/main.py\n```\n\nIn this final subtask, you must **optimize and refine** the entire workflow to maximize accuracy and robustness, while preparing for up to **three separate submission variants** (e.g., baseline and two improved workflows). You must:\n1. Introduce configurable reasoning strategies (e.g., sampling, self-consistency, fallback heuristics).\n2. Use the **develop** split systematically to calibrate prompts and hyperparameters.\n3. Enhance logging and meta-data (confidence, timing, failure modes) in the output files.\n4. Still respect all existing directory and format constraints so the evaluation system can consume your results without modification.\n\nDeliverables:\n- Extend `/workspace/outputs/main.py` with the following advanced features:\n\n  1. **Configurable Run Modes**:\n     - Add a command-line argument, e.g., `--mode`, with allowed values:\n       - `baseline`\n       - `improved1`\n       - `improved2`\n     - Each mode must:\n       - Use the same directory structure and `result.json` formats.\n       - Differ in **internal reasoning strategy**, for example:\n         - `baseline`: a single GPT-4o call per problem with deterministic settings.\n         - `improved1`: multiple GPT-4o samples per problem (e.g., 3) with majority voting for geometry and convexity tasks.\n         - `improved2`: an adaptive strategy that retries uncertain answers (e.g., low confidence or internally inconsistent reasoning) with adjusted prompts or sampling temperature.\n     - Allow the caller to generate **up to three distinct runs** by changing `--mode`, while always writing the final outputs to the same required locations for evaluation.\n\n  2. **Calibration Using the Develop Split**:\n     - Implement a `calibrate()` routine (invoked when `--mode` is `improved1` or `improved2`) that:\n       - Samples a subset of problems from each **develop** task split:\n         - `/workspace/datasets/develop/geometry/`\n         - `/workspace/datasets/develop/graph_connectivity/`\n         - `/workspace/datasets/develop/graph_maxflow/`\n         - `/workspace/datasets/develop/math_convexity/`\n       - For geometry and convexity:\n         - Tests different GPT-4o prompting styles or sampling parameters (e.g., temperature, number of samples) on the develop subset.\n         - Tracks accuracy for each configuration.\n         - Selects the best strategy to apply to the **test** split in the current mode.\n       - For graph tasks:\n         - Confirms that algorithmic solvers are perfectly accurate.\n         - Optionally tunes how verbose GPT-4o explanations should be.\n       - Does **not** write outputs for develop data to the main `outputs/` directory, but may store temporary logs or calibration artifacts, e.g., under `/workspace/outputs/debug/`.\n\n  3. **Enhanced Confidence and Metadata**:\n     - Extend each `result.json` file to optionally include additional meta-fields (while keeping all previously required fields):\n       - `runtime_ms`: wall-clock time in milliseconds spent solving that instance (including GPT-4o calls).\n       - `num_llm_calls`: number of GPT-4o calls used for that problem.\n       - `strategy`: a short string describing the mode or strategy (e.g., `\"single_call\"`, `\"self_consistency_3_votes\"`, `\"fallback_to_numeric_check\"`).\n     - Keep `answer`, `reasoning`, `solution_steps`, and `confidence` exactly as previously specified and compatible with the evaluation.\n\n  4. **Global Summary Enhancements**:\n     - Update `/workspace/outputs/summary.json` so that it additionally records:\n       - `mode`: the current run mode (`\"baseline\"`, `\"improved1\"`, or `\"improved2\"`).\n       - `average_runtime_ms`: average per-problem runtime across all tasks.\n       - `total_llm_calls`: total number of GPT-4o calls across the whole run.\n       - Optional per-task breakdown, for example:\n         ```json\n         \"task_meta\": {\n           \"geometry\": {\"average_confidence\": 0.75, \"average_runtime_ms\": 4500, \"llm_calls\": 72},\n           \"graph_connectivity\": {\"average_confidence\": 0.99, \"average_runtime_ms\": 50, \"llm_calls\": 64},\n           \"graph_maxflow\": {\"average_confidence\": 0.98, \"average_runtime_ms\": 60, \"llm_calls\": 64},\n           \"math_convexity\": {\"average_confidence\": 0.9, \"average_runtime_ms\": 1200, \"llm_calls\": 128}\n         }\n         ```\n       - Ensure that the core keys required by previous subtasks remain present and valid.\n\n  5. **Performance and Robustness Guardrails**:\n     - Enforce a per-problem time budget of at most 3 minutes:\n       - If a multimodal or convexity call is stuck or delayed, implement a timeout and fallback (e.g., simplified prompt or single quick call).\n     - Make the pipeline resilient:\n       - Any failure on an individual instance must not crash the overall run.\n       - In case of failure, write a `result.json` with a best-effort guess and a low `confidence` (e.g., 0.1), and record the error in logs.\n\nRubric:\n- **Configurable Strategies & Multi-run Support (30%)**:\n  - `main.py` accepts `--mode` and behaves differently in `baseline`, `improved1`, and `improved2` while preserving output formats.\n  - It is possible to run all three modes sequentially to generate up to three different submissions, each more advanced than the previous.\n\n- **Effective Calibration (25%)**:\n  - Develop data is systematically used to select better prompts or parameters for geometry and convexity.\n  - Calibration decisions are clearly encoded in code (e.g., comments, configuration variables) and actually influence test-time behavior.\n\n- **Rich Metadata & Logging (20%)**:\n  - `result.json` files include additional meta-fields without breaking the required schema.\n  - `summary.json` contains mode, runtime, and LLM call statistics that can be used to compare modes.\n\n- **Robustness and Time Constraints (25%)**:\n  - The pipeline respects the 3-minute per-problem time limit.\n  - Failures are handled gracefully with fallbacks and error logging.\n  - The system remains stable when processing all test instances in any mode.\n\nAfter this subtask, you will have a highly optimized, configurable multimodal mathematical reasoning workflow that can be run in several modes to explore trade-offs between speed and accuracy, while always producing evaluation-ready outputs for all four task types."
  }
  