{
  "metadata": {
    "annotation_id": "auto-ann-microsoft/autogen:autogenerated-379:416d61b8",
    "chain_id": "microsoft/autogen:autogenerated-379:416d61b8",
    "generated_at": "2025-11-14T02:22:18.733791+00:00",
    "operator_notes": null,
    "sanitized_repository": "microsoft_autogen",
    "source_repository": "microsoft/autogen",
    "start_pr_id": "microsoft/autogen#379",
    "template_checksum": "58cb965ba38dcf2e4a48441fa821ead396118a1066a3386030772d43a1488f37",
    "template_id": "query/query_template",
    "template_version": "2024-11-05",
    "topic": "autogenerated-379",
    "validation_status": null
  },
  "stages": [
    {
      "base_commit_sha": "a8da3854c00cf8a4517d2572668a8b45077c63bc",
      "carry_forward_policy": "reapply_patch",
      "dependencies": [],
      "ground_truth_patch": "@@ -13,10 +13,9 @@ This project is a spinoff from [FLAML](https://github.com/microsoft/FLAML).\n     <br>\n </p> -->\n \n-:fire: autogen has graduated from [FLAML](https://github.com/microsoft/FLAML) into a new project.\n-\n-<!-- :fire: Heads-up: We're preparing to migrate [autogen](https://microsoft.github.io/FLAML/docs/Use-Cases/Autogen) into a dedicated Github repository. Alongside this move, we'll also launch a dedicated Discord server and a website for comprehensive documentation.\n+:fire: Heads-up: pyautogen v0.2 will switch to using openai v1.\n \n+<!--\n :fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n \n :fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n@@ -33,7 +32,7 @@ AutoGen is a framework that enables the development of LLM applications using mu\n - It supports **diverse conversation patterns** for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n   the number of agents, and agent conversation topology.\n - It provides a collection of working systems with different complexities. These systems span a **wide range of applications** from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n-- AutoGen provides a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` as an **enhanced inference API**. It allows easy performance tuning, utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n+- AutoGen provides **enhanced LLM inference**. It offers easy performance tuning, plus utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n \n AutoGen is powered by collaborative [research studies](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n \n@@ -111,7 +110,7 @@ Please find more [code examples](https://microsoft.github.io/autogen/docs/Exampl\n \n ## Enhanced LLM Inferences\n \n-Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` adding powerful functionalities like tuning, caching, error handling, and templating. For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n+Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, and templating. For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n \n ```python\n # perform tuning\n@@ -194,8 +194,13 @@\n     \"\\n\",\n     \"# 2. create the RetrieveUserProxyAgent instance named \\\"ragproxyagent\\\"\\n\",\n     \"# By default, the human_input_mode is \\\"ALWAYS\\\", which means the agent will ask for human input at every step. We set it to \\\"NEVER\\\" here.\\n\",\n-    \"# `docs_path` is the path to the docs directory. By default, it is set to \\\"./docs\\\". Here we generated the documentations from FLAML's docstrings.\\n\",\n-    \"# Navigate to the website folder and run `pydoc-markdown` and it will generate folder `reference` under `website/docs`.\\n\",\n+    \"# `docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default, \\n\",\n+    \"# it is set to None, which works only if the collection is already created.\\n\",\n+    \"# \\n\",\n+    \"# Here we generated the documentations from FLAML's docstrings. Not needed if you just want to try this notebook but not to reproduce the\\n\",\n+    \"# outputs. Clone the FLAML (https://github.com/microsoft/FLAML) repo and navigate to its website folder. Pip install and run `pydoc-markdown`\\n\",\n+    \"# and it will generate folder `reference` under `website/docs`.\\n\",\n+    \"#\\n\",\n     \"# `task` indicates the kind of task we're working on. In this example, it's a `code` task.\\n\",\n     \"# `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\\n\",\n     \"ragproxyagent = RetrieveUserProxyAgent(\\n\",\n@@ -204,11 +209,12 @@\n     \"    max_consecutive_auto_reply=10,\\n\",\n     \"    retrieve_config={\\n\",\n     \"        \\\"task\\\": \\\"code\\\",\\n\",\n-    \"        \\\"docs_path\\\": \\\"../website/docs/reference\\\",\\n\",\n+    \"        \\\"docs_path\\\": \\\"~/code/FLAML/website/docs/reference\\\",  # change this to your own path, such as https://raw.githubusercontent.com/microsoft/autogen/main/README.md\\n\",\n     \"        \\\"chunk_token_size\\\": 2000,\\n\",\n     \"        \\\"model\\\": config_list[0][\\\"model\\\"],\\n\",\n     \"        \\\"client\\\": chromadb.PersistentClient(path=\\\"/tmp/chromadb\\\"),\\n\",\n     \"        \\\"embedding_model\\\": \\\"all-mpnet-base-v2\\\",\\n\",\n+    \"        \\\"get_or_create\\\": False,  # set to True if you want to recreate the collection\\n\",\n     \"    },\\n\",\n     \")\"\n    ]\n@@ -23,6 +23,7 @@ Links to notebook examples:\n \n    - Automated Chess Game Playing & Chitchatting by GPT-4 Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_chess.ipynb)\n    - Automated Continual Learning from New Data - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_stream.ipynb)\n+   - [OptiGuide](https://github.com/microsoft/optiguide) - Large Language Models for Supply Chain Optimization.\n \n 4. **Tool Use**\n \n@@ -12,7 +12,7 @@ AutoGen is a framework that enables development of LLM applications using multip\n * It supports **diverse conversation patterns** for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n the number of agents, and agent conversation topology.\n * It provides a collection of working systems with different complexities. These systems span a **wide range of applications** from various domains and complexities. They demonstrate how AutoGen can easily support different conversation patterns.\n-* AutoGen provides a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` as an **enhanced inference API**. It allows easy performance tuning, utilities like API unification & caching, and advanced usage patterns, such as error handling, multi-config inference, context programming etc.\n+* AutoGen provides **enhanced LLM inference**. It offers easy performance tuning, plus utilities like API unification & caching, and advanced usage patterns, such as error handling, multi-config inference, context programming etc.\n \n AutoGen is powered by collaborative [research studies](/docs/Research) from Microsoft, Penn State University, and University of Washington.\n \n@@ -44,7 +44,7 @@ The figure below shows an example conversation flow with AutoGen.\n * [Documentation](/docs/Use-Cases/agent_chat).\n \n #### Enhanced LLM Inferences\n-Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\n+Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\n ```python\n # perform tuning\n config, analysis = autogen.Completion.tune(\n@@ -4,12 +4,16 @@\n \n When not using a docker container, we recommend using a virtual environment to install AutoGen. This will ensure that the dependencies for AutoGen are isolated from the rest of your system.\n \n+### Option 1: venv\n+\n You can create a virtual environment with `venv` as below:\n ```bash\n python3 -m venv autogen\n source autogen/bin/activate\n ```\n \n+### Option 2: conda\n+\n Another option is with `Conda`, Conda works better at solving dependency conflicts than pip. You can install it by following [this doc](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html),\n and then create a virtual environment as below:\n ```bash\n@@ -26,6 +30,9 @@ AutoGen requires **Python version >= 3.8**. It can be installed from pip:\n ```bash\n pip install pyautogen\n ```\n+\n+`pyautogen<0.2` requires `openai<1`. Starting from pyautogen v0.2, `openai>=1` is required.\n+\n <!--\n or conda:\n ```\n@@ -99,6 +99,7 @@ The figure below shows six examples of applications built using AutoGen.\n \n    - Automated Chess Game Playing & Chitchatting by GPT-4 Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_chess.ipynb)\n    - Automated Continual Learning from New Data - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_stream.ipynb)\n+   - [OptiGuide](https://github.com/microsoft/optiguide) - Large Language Models for Supply Chain Optimization.\n \n 4. **Tool Use**\n \n@@ -1,6 +1,6 @@\n # Enhanced Inference\n \n-`autogen.Completion` is a drop-in replacement of `openai.Completion` and `openai.ChatCompletion` as an enhanced inference API.\n+`autogen.Completion` is a drop-in replacement of `openai.Completion` and `openai.ChatCompletion` for enhanced LLM inference.\n There are a number of benefits of using `autogen` to perform inference: performance tuning, API unification, caching, error handling, multi-config inference, result filtering, templating and so on.\n \n ## Tune Inference Parameters\n",
      "head_commit_sha": "f9acb69aeabee8516acd3743454ca99798765d0e",
      "name": "Stage 1",
      "patches": [
        {
          "patch": "@@ -13,10 +13,9 @@ This project is a spinoff from [FLAML](https://github.com/microsoft/FLAML).\n     <br>\n </p> -->\n \n-:fire: autogen has graduated from [FLAML](https://github.com/microsoft/FLAML) into a new project.\n-\n-<!-- :fire: Heads-up: We're preparing to migrate [autogen](https://microsoft.github.io/FLAML/docs/Use-Cases/Autogen) into a dedicated Github repository. Alongside this move, we'll also launch a dedicated Discord server and a website for comprehensive documentation.\n+:fire: Heads-up: pyautogen v0.2 will switch to using openai v1.\n \n+<!--\n :fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n \n :fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n@@ -33,7 +32,7 @@ AutoGen is a framework that enables the development of LLM applications using mu\n - It supports **diverse conversation patterns** for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n   the number of agents, and agent conversation topology.\n - It provides a collection of working systems with different complexities. These systems span a **wide range of applications** from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n-- AutoGen provides a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` as an **enhanced inference API**. It allows easy performance tuning, utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n+- AutoGen provides **enhanced LLM inference**. It offers easy performance tuning, plus utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n \n AutoGen is powered by collaborative [research studies](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n \n@@ -111,7 +110,7 @@ Please find more [code examples](https://microsoft.github.io/autogen/docs/Exampl\n \n ## Enhanced LLM Inferences\n \n-Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` adding powerful functionalities like tuning, caching, error handling, and templating. For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n+Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, and templating. For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n \n ```python\n # perform tuning",
          "path": "README.md"
        },
        {
          "patch": "@@ -194,8 +194,13 @@\n     \"\\n\",\n     \"# 2. create the RetrieveUserProxyAgent instance named \\\"ragproxyagent\\\"\\n\",\n     \"# By default, the human_input_mode is \\\"ALWAYS\\\", which means the agent will ask for human input at every step. We set it to \\\"NEVER\\\" here.\\n\",\n-    \"# `docs_path` is the path to the docs directory. By default, it is set to \\\"./docs\\\". Here we generated the documentations from FLAML's docstrings.\\n\",\n-    \"# Navigate to the website folder and run `pydoc-markdown` and it will generate folder `reference` under `website/docs`.\\n\",\n+    \"# `docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default, \\n\",\n+    \"# it is set to None, which works only if the collection is already created.\\n\",\n+    \"# \\n\",\n+    \"# Here we generated the documentations from FLAML's docstrings. Not needed if you just want to try this notebook but not to reproduce the\\n\",\n+    \"# outputs. Clone the FLAML (https://github.com/microsoft/FLAML) repo and navigate to its website folder. Pip install and run `pydoc-markdown`\\n\",\n+    \"# and it will generate folder `reference` under `website/docs`.\\n\",\n+    \"#\\n\",\n     \"# `task` indicates the kind of task we're working on. In this example, it's a `code` task.\\n\",\n     \"# `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\\n\",\n     \"ragproxyagent = RetrieveUserProxyAgent(\\n\",\n@@ -204,11 +209,12 @@\n     \"    max_consecutive_auto_reply=10,\\n\",\n     \"    retrieve_config={\\n\",\n     \"        \\\"task\\\": \\\"code\\\",\\n\",\n-    \"        \\\"docs_path\\\": \\\"../website/docs/reference\\\",\\n\",\n+    \"        \\\"docs_path\\\": \\\"~/code/FLAML/website/docs/reference\\\",  # change this to your own path, such as https://raw.githubusercontent.com/microsoft/autogen/main/README.md\\n\",\n     \"        \\\"chunk_token_size\\\": 2000,\\n\",\n     \"        \\\"model\\\": config_list[0][\\\"model\\\"],\\n\",\n     \"        \\\"client\\\": chromadb.PersistentClient(path=\\\"/tmp/chromadb\\\"),\\n\",\n     \"        \\\"embedding_model\\\": \\\"all-mpnet-base-v2\\\",\\n\",\n+    \"        \\\"get_or_create\\\": False,  # set to True if you want to recreate the collection\\n\",\n     \"    },\\n\",\n     \")\"\n    ]",
          "path": "notebook/agentchat_RetrieveChat.ipynb"
        },
        {
          "patch": "@@ -23,6 +23,7 @@ Links to notebook examples:\n \n    - Automated Chess Game Playing & Chitchatting by GPT-4 Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_chess.ipynb)\n    - Automated Continual Learning from New Data - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_stream.ipynb)\n+   - [OptiGuide](https://github.com/microsoft/optiguide) - Large Language Models for Supply Chain Optimization.\n \n 4. **Tool Use**\n ",
          "path": "website/docs/Examples/AutoGen-AgentChat.md"
        },
        {
          "patch": "@@ -12,7 +12,7 @@ AutoGen is a framework that enables development of LLM applications using multip\n * It supports **diverse conversation patterns** for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n the number of agents, and agent conversation topology.\n * It provides a collection of working systems with different complexities. These systems span a **wide range of applications** from various domains and complexities. They demonstrate how AutoGen can easily support different conversation patterns.\n-* AutoGen provides a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` as an **enhanced inference API**. It allows easy performance tuning, utilities like API unification & caching, and advanced usage patterns, such as error handling, multi-config inference, context programming etc.\n+* AutoGen provides **enhanced LLM inference**. It offers easy performance tuning, plus utilities like API unification & caching, and advanced usage patterns, such as error handling, multi-config inference, context programming etc.\n \n AutoGen is powered by collaborative [research studies](/docs/Research) from Microsoft, Penn State University, and University of Washington.\n \n@@ -44,7 +44,7 @@ The figure below shows an example conversation flow with AutoGen.\n * [Documentation](/docs/Use-Cases/agent_chat).\n \n #### Enhanced LLM Inferences\n-Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\n+Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\n ```python\n # perform tuning\n config, analysis = autogen.Completion.tune(",
          "path": "website/docs/Getting-Started.md"
        },
        {
          "patch": "@@ -4,12 +4,16 @@\n \n When not using a docker container, we recommend using a virtual environment to install AutoGen. This will ensure that the dependencies for AutoGen are isolated from the rest of your system.\n \n+### Option 1: venv\n+\n You can create a virtual environment with `venv` as below:\n ```bash\n python3 -m venv autogen\n source autogen/bin/activate\n ```\n \n+### Option 2: conda\n+\n Another option is with `Conda`, Conda works better at solving dependency conflicts than pip. You can install it by following [this doc](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html),\n and then create a virtual environment as below:\n ```bash\n@@ -26,6 +30,9 @@ AutoGen requires **Python version >= 3.8**. It can be installed from pip:\n ```bash\n pip install pyautogen\n ```\n+\n+`pyautogen<0.2` requires `openai<1`. Starting from pyautogen v0.2, `openai>=1` is required.\n+\n <!--\n or conda:\n ```",
          "path": "website/docs/Installation.md"
        },
        {
          "patch": "@@ -99,6 +99,7 @@ The figure below shows six examples of applications built using AutoGen.\n \n    - Automated Chess Game Playing & Chitchatting by GPT-4 Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_chess.ipynb)\n    - Automated Continual Learning from New Data - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_stream.ipynb)\n+   - [OptiGuide](https://github.com/microsoft/optiguide) - Large Language Models for Supply Chain Optimization.\n \n 4. **Tool Use**\n ",
          "path": "website/docs/Use-Cases/agent_chat.md"
        },
        {
          "patch": "@@ -1,6 +1,6 @@\n # Enhanced Inference\n \n-`autogen.Completion` is a drop-in replacement of `openai.Completion` and `openai.ChatCompletion` as an enhanced inference API.\n+`autogen.Completion` is a drop-in replacement of `openai.Completion` and `openai.ChatCompletion` for enhanced LLM inference.\n There are a number of benefits of using `autogen` to perform inference: performance tuning, API unification, caching, error handling, multi-config inference, result filtering, templating and so on.\n \n ## Tune Inference Parameters",
          "path": "website/docs/Use-Cases/enhanced_inference.md"
        }
      ],
      "pr_number": "379",
      "stage_id": "stage-1",
      "test_query": "You need to modify the files in this repository according to the requirements below. During the analysis process, do not directly read the complete file; try to read the first 10 lines using head or use grep instead. You are NOT allowed to directly run scripts, such as python, pip install, dockerfile, etc.\n\nThe repository requires enhancements to improve documentation clarity and extend vector storage capabilities. First, update the RetrieveChat notebook and installation documentation to eliminate confusion about package relationships and provide clearer environment setup guidance for both venv and conda users. The documentation should explicitly clarify that this project is not a subpackage of FLAML and streamline the onboarding process for new users.\n\nNext, implement support for Qdrant as an additional vector store option by creating a new agent class that extends the existing retrieve user proxy agent functionality. This new integration should be placed in the contrib directory alongside other retrieve agents. The implementation should leverage the existing retrieval utilities while adding Qdrant-specific connection and query logic.\n\nAdditionally, resolve compatibility issues with the ChromaDB integration by adapting to recent class renames in version 0.4.15. The fix should ensure proper collection creation when no client is explicitly provided.\n\nFinally, update the CI workflow configuration and package dependencies to include Qdrant testing requirements, ensuring that the new functionality can be properly validated in automated testing pipelines. The example documentation should also be updated to reflect these new capabilities and provide clearer guidance for users."
    },
    {
      "base_commit_sha": "80954e4b8d0752fd4772f339dee419fbf1debc6f",
      "carry_forward_policy": "reapply_patch",
      "dependencies": [
        "stage-1"
      ],
      "ground_truth_patch": "@@ -0,0 +1,266 @@\n+from typing import Callable, Dict, List, Optional\n+\n+from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n+from autogen.retrieve_utils import get_files_from_dir, split_files_to_chunks\n+import logging\n+\n+logger = logging.getLogger(__name__)\n+\n+try:\n+    from qdrant_client import QdrantClient, models\n+    from qdrant_client.fastembed_common import QueryResponse\n+    import fastembed\n+except ImportError as e:\n+    logging.fatal(\"Failed to import qdrant_client with fastembed. Try running 'pip install qdrant_client[fastembed]'\")\n+    raise e\n+\n+\n+class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n+    def __init__(\n+        self,\n+        name=\"RetrieveChatAgent\",\n+        human_input_mode: str | None = \"ALWAYS\",\n+        is_termination_msg: Callable[[Dict], bool] | None = None,\n+        retrieve_config: Dict | None = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Args:\n+            name (str): name of the agent.\n+            human_input_mode (str): whether to ask for human inputs every time a message is received.\n+                Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n+                (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n+                    Under this mode, the conversation stops when the human input is \"exit\",\n+                    or when is_termination_msg is True and there is no human input.\n+                (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n+                    the number of auto reply reaches the max_consecutive_auto_reply.\n+                (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n+                    when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n+            is_termination_msg (function): a function that takes a message in the form of a dictionary\n+                and returns a boolean value indicating if this received message is a termination message.\n+                The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n+            retrieve_config (dict or None): config for the retrieve agent.\n+                To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n+                - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n+                    prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n+                - client (Optional, qdrant_client.QdrantClient(\":memory:\")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production.\n+                    will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.\n+                - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n+                    or the url to a single file. Default is None, which works only if the collection is already created.\n+                - collection_name (Optional, str): the name of the collection.\n+                    If key not provided, a default name `autogen-docs` will be used.\n+                - model (Optional, str): the model to use for the retrieve chat.\n+                    If key not provided, a default model `gpt-4` will be used.\n+                - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n+                    If key not provided, a default size `max_tokens * 0.4` will be used.\n+                - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n+                    If key not provided, a default size `max_tokens * 0.8` will be used.\n+                - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n+                    \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n+                - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n+                    If chunk_mode is \"one_line\", this parameter will be ignored.\n+                - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n+                    If key not provided, a default model `BAAI/bge-small-en-v1.5` will be used. All available models\n+                    can be found at `https://qdrant.github.io/fastembed/examples/Supported_Models/`.\n+                - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n+                - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n+                    If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n+                - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.\n+                - custom_token_count_function(Optional, Callable): a custom function to count the number of tokens in a string.\n+                    The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).\n+                    Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.\n+                - custom_text_split_function(Optional, Callable): a custom function to split a string into a list of strings.\n+                    Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.\n+                - parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.\n+                - on_disk (Optional, bool): Whether to store the collection on disk. Default is False.\n+                - quantization_config: Quantization configuration. If None, quantization will be disabled.\n+                - hnsw_config: HNSW configuration. If None, default configuration will be used.\n+                  You can find more info about the hnsw configuration options at https://qdrant.tech/documentation/concepts/indexing/#vector-index.\n+                  API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n+                - payload_indexing: Whether to create a payload index for the document field. Default is False.\n+                  You can find more info about the payload indexing options at https://qdrant.tech/documentation/concepts/indexing/#payload-index\n+                  API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index\n+             **kwargs (dict): other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n+\n+        \"\"\"\n+        super().__init__(name, human_input_mode, is_termination_msg, retrieve_config, **kwargs)\n+        self._client = self._retrieve_config.get(\"client\", QdrantClient(\":memory:\"))\n+        self._embedding_model = self._retrieve_config.get(\"embedding_model\", \"BAAI/bge-small-en-v1.5\")\n+        # Uses all available CPU cores to encode data when set to 0\n+        self._parallel = self._retrieve_config.get(\"parallel\", 0)\n+        self._on_disk = self._retrieve_config.get(\"on_disk\", False)\n+        self._quantization_config = self._retrieve_config.get(\"quantization_config\", None)\n+        self._hnsw_config = self._retrieve_config.get(\"hnsw_config\", None)\n+        self._payload_indexing = self._retrieve_config.get(\"payload_indexing\", False)\n+\n+    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\"):\n+        \"\"\"\n+        Args:\n+            problem (str): the problem to be solved.\n+            n_results (int): the number of results to be retrieved.\n+            search_string (str): only docs containing this string will be retrieved.\n+        \"\"\"\n+        if not self._collection:\n+            print(\"Trying to create collection.\")\n+            create_qdrant_from_dir(\n+                dir_path=self._docs_path,\n+                max_tokens=self._chunk_token_size,\n+                client=self._client,\n+                collection_name=self._collection_name,\n+                chunk_mode=self._chunk_mode,\n+                must_break_at_empty_line=self._must_break_at_empty_line,\n+                embedding_model=self._embedding_model,\n+                custom_text_split_function=self.custom_text_split_function,\n+                parallel=self._parallel,\n+                on_disk=self._on_disk,\n+                quantization_config=self._quantization_config,\n+                hnsw_config=self._hnsw_config,\n+                payload_indexing=self._payload_indexing,\n+            )\n+            self._collection = True\n+\n+        results = query_qdrant(\n+            query_texts=problem,\n+            n_results=n_results,\n+            search_string=search_string,\n+            client=self._client,\n+            collection_name=self._collection_name,\n+            embedding_model=self._embedding_model,\n+        )\n+        self._results = results\n+\n+\n+def create_qdrant_from_dir(\n+    dir_path: str,\n+    max_tokens: int = 4000,\n+    client: QdrantClient = None,\n+    collection_name: str = \"all-my-documents\",\n+    chunk_mode: str = \"multi_lines\",\n+    must_break_at_empty_line: bool = True,\n+    embedding_model: str = \"BAAI/bge-small-en-v1.5\",\n+    custom_text_split_function: Callable = None,\n+    parallel: int = 0,\n+    on_disk: bool = False,\n+    quantization_config: Optional[models.QuantizationConfig] = None,\n+    hnsw_config: Optional[models.HnswConfigDiff] = None,\n+    payload_indexing: bool = False,\n+    qdrant_client_options: Optional[Dict] = {},\n+):\n+    \"\"\"Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a url to\n+        a single file.\n+\n+    Args:\n+        dir_path (str): the path to the directory, file or url.\n+        max_tokens (Optional, int): the maximum number of tokens per chunk. Default is 4000.\n+        client (Optional, QdrantClient): the QdrantClient instance. Default is None.\n+        collection_name (Optional, str): the name of the collection. Default is \"all-my-documents\".\n+        chunk_mode (Optional, str): the chunk mode. Default is \"multi_lines\".\n+        must_break_at_empty_line (Optional, bool): Whether to break at empty line. Default is True.\n+        embedding_model (Optional, str): the embedding model to use. Default is \"BAAI/bge-small-en-v1.5\". The list of all the available models can be at https://qdrant.github.io/fastembed/examples/Supported_Models/.\n+        parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores\n+        on_disk (Optional, bool): Whether to store the collection on disk. Default is False.\n+        quantization_config: Quantization configuration. If None, quantization will be disabled. Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n+        hnsw_config: HNSW configuration. If None, default configuration will be used. Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n+        payload_indexing: Whether to create a payload index for the document field. Default is False.\n+        qdrant_client_options: (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.\n+    \"\"\"\n+    if client is None:\n+        client = QdrantClient(**qdrant_client_options)\n+        client.set_model(embedding_model)\n+\n+    if custom_text_split_function is not None:\n+        chunks = split_files_to_chunks(\n+            get_files_from_dir(dir_path), custom_text_split_function=custom_text_split_function\n+        )\n+    else:\n+        chunks = split_files_to_chunks(get_files_from_dir(dir_path), max_tokens, chunk_mode, must_break_at_empty_line)\n+    logger.info(f\"Found {len(chunks)} chunks.\")\n+\n+    # Check if collection by same name exists, if not, create it with custom options\n+    try:\n+        client.get_collection(collection_name=collection_name)\n+    except Exception:\n+        client.create_collection(\n+            collection_name=collection_name,\n+            vectors_config=client.get_fastembed_vector_params(\n+                on_disk=on_disk, quantization_config=quantization_config, hnsw_config=hnsw_config\n+            ),\n+        )\n+        client.get_collection(collection_name=collection_name)\n+\n+    # Upsert in batch of 100 or less if the total number of chunks is less than 100\n+    for i in range(0, len(chunks), min(100, len(chunks))):\n+        end_idx = i + min(100, len(chunks) - i)\n+        client.add(collection_name, documents=chunks[i:end_idx], ids=[j for j in range(i, end_idx)], parallel=parallel)\n+\n+    # Create a payload index for the document field\n+    # Enables highly efficient payload filtering. Reference: https://qdrant.tech/documentation/concepts/indexing/#indexing\n+    # Creating an index requires additional computational resources and memory.\n+    # If filtering performance is critical, we can consider creating an index.\n+    if payload_indexing:\n+        client.create_payload_index(\n+            collection_name=collection_name,\n+            field_name=\"document\",\n+            field_schema=models.TextIndexParams(\n+                type=\"text\",\n+                tokenizer=models.TokenizerType.WORD,\n+                min_token_len=2,\n+                max_token_len=15,\n+            ),\n+        )\n+\n+\n+def query_qdrant(\n+    query_texts: List[str],\n+    n_results: int = 10,\n+    client: QdrantClient = None,\n+    collection_name: str = \"all-my-documents\",\n+    search_string: str = \"\",\n+    embedding_model: str = \"BAAI/bge-small-en-v1.5\",\n+    qdrant_client_options: Optional[Dict] = {},\n+) -> List[List[QueryResponse]]:\n+    \"\"\"Perform a similarity search with filters on a Qdrant collection\n+\n+    Args:\n+        query_texts (List[str]): the query texts.\n+        n_results (Optional, int): the number of results to return. Default is 10.\n+        client (Optional, API): the QdrantClient instance. A default in-memory client will be instantiated if None.\n+        collection_name (Optional, str): the name of the collection. Default is \"all-my-documents\".\n+        search_string (Optional, str): the search string. Default is \"\".\n+        embedding_model (Optional, str): the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if embedding_function is not None.\n+        qdrant_client_options: (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.\n+\n+    Returns:\n+        List[List[QueryResponse]]: the query result. The format is:\n+            class QueryResponse(BaseModel, extra=\"forbid\"):  # type: ignore\n+                id: Union[str, int]\n+                embedding: Optional[List[float]]\n+                metadata: Dict[str, Any]\n+                document: str\n+                score: float\n+    \"\"\"\n+    if client is None:\n+        client = QdrantClient(**qdrant_client_options)\n+        client.set_model(embedding_model)\n+\n+    results = client.query_batch(\n+        collection_name,\n+        query_texts,\n+        limit=n_results,\n+        query_filter=models.Filter(\n+            must=[\n+                models.FieldCondition(\n+                    key=\"document\",\n+                    match=models.MatchText(text=search_string),\n+                )\n+            ]\n+        )\n+        if search_string\n+        else None,\n+    )\n+\n+    data = {\n+        \"ids\": [[result.id for result in sublist] for sublist in results],\n+        \"documents\": [[result.document for result in sublist] for sublist in results],\n+    }\n+    return data\n@@ -0,0 +1,1234 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<a href=\\\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<a id=\\\"toc\\\"></a>\\n\",\n+    \"# Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering\\n\",\n+    \"\\n\",\n+    \"[Qdrant](https://qdrant.tech/) is a high-performance vector search engine/database.\\n\",\n+    \"\\n\",\n+    \"This notebook demonstrates the usage of `QdrantRetrieveUserProxyAgent` for RAG, based on [agentchat_RetrieveChat.ipynb](https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb).\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"RetrieveChat is a conversational system for retrieve augmented code generation and question answering. In this notebook, we demonstrate how to utilize RetrieveChat to generate code and answer questions based on customized documentations that are not present in the LLM's training dataset. RetrieveChat uses the `RetrieveAssistantAgent` and `QdrantRetrieveUserProxyAgent`, which is similar to the usage of `AssistantAgent` and `UserProxyAgent` in other notebooks (e.g., [Automated Task Solving with Code Generation, Execution & Debugging](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)).\\n\",\n+    \"\\n\",\n+    \"We'll demonstrate usage of RetrieveChat with Qdrant for code generation and question answering w/ human feedback.\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"## Requirements\\n\",\n+    \"\\n\",\n+    \"AutoGen requires `Python>=3.8`. To run this notebook example, please install the [retrievechat] option.\\n\",\n+    \"```bash\\n\",\n+    \"pip install \\\"pyautogen[retrievechat] flaml[automl] qdrant_client[fastembed]\\\"\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Set your API Endpoint\\n\",\n+    \"\\n\",\n+    \"The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 6,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"models to use:  ['gpt-3.5-turbo']\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import autogen\\n\",\n+    \"\\n\",\n+    \"config_list = autogen.config_list_from_json(\\n\",\n+    \"    env_or_file=\\\"OAI_CONFIG_LIST\\\",\\n\",\n+    \"    file_location=\\\".\\\",\\n\",\n+    \"    filter_dict={\\n\",\n+    \"        \\\"model\\\": {\\n\",\n+    \"            \\\"gpt-4\\\",\\n\",\n+    \"            \\\"gpt4\\\",\\n\",\n+    \"            \\\"gpt-4-32k\\\",\\n\",\n+    \"            \\\"gpt-4-32k-0314\\\",\\n\",\n+    \"            \\\"gpt-35-turbo\\\",\\n\",\n+    \"            \\\"gpt-3.5-turbo\\\",\\n\",\n+    \"        }\\n\",\n+    \"    },\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"assert len(config_list) > 0\\n\",\n+    \"print(\\\"models to use: \\\", [config_list[i][\\\"model\\\"] for i in range(len(config_list))])\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"It first looks for environment variable \\\"OAI_CONFIG_LIST\\\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \\\"OAI_CONFIG_LIST\\\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 and gpt-3.5-turbo models are kept in the list based on the filter condition.\\n\",\n+    \"\\n\",\n+    \"The config list looks like the following:\\n\",\n+    \"```python\\n\",\n+    \"config_list = [\\n\",\n+    \"    {\\n\",\n+    \"        'model': 'gpt-4',\\n\",\n+    \"        'api_key': '<your OpenAI API key here>',\\n\",\n+    \"    },\\n\",\n+    \"    {\\n\",\n+    \"        'model': 'gpt-4',\\n\",\n+    \"        'api_key': '<your Azure OpenAI API key here>',\\n\",\n+    \"        'api_base': '<your Azure OpenAI API base here>',\\n\",\n+    \"        'api_type': 'azure',\\n\",\n+    \"        'api_version': '2023-06-01-preview',\\n\",\n+    \"    },\\n\",\n+    \"    {\\n\",\n+    \"        'model': 'gpt-3.5-turbo',\\n\",\n+    \"        'api_key': '<your Azure OpenAI API key here>',\\n\",\n+    \"        'api_base': '<your Azure OpenAI API base here>',\\n\",\n+    \"        'api_type': 'azure',\\n\",\n+    \"        'api_version': '2023-06-01-preview',\\n\",\n+    \"    },\\n\",\n+    \"]\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \\\"upload file\\\" icon.\\n\",\n+    \"\\n\",\n+    \"You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 11,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Accepted file formats for `docs_path`:\\n\",\n+      \"['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"# Accepted file formats for that can be stored in \\n\",\n+    \"# a vector database instance\\n\",\n+    \"from autogen.retrieve_utils import TEXT_FORMATS\\n\",\n+    \"\\n\",\n+    \"print(\\\"Accepted file formats for `docs_path`:\\\")\\n\",\n+    \"print(TEXT_FORMATS)\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Construct agents for RetrieveChat\\n\",\n+    \"\\n\",\n+    \"We start by initialzing the `RetrieveAssistantAgent` and `QdrantRetrieveUserProxyAgent`. The system message needs to be set to \\\"You are a helpful assistant.\\\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `QdrantRetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a retrieval augmented generation task for an initial prompt to be sent to the LLM assistant.\\n\",\n+    \"\\n\",\n+    \"### You can find the list of all the embedding models supported by Qdrant [here](https://qdrant.github.io/fastembed/examples/Supported_Models/).\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 17,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\\n\",\n+    \"from autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent import QdrantRetrieveUserProxyAgent\\n\",\n+    \"from qdrant_client import QdrantClient\\n\",\n+    \"\\n\",\n+    \"autogen.ChatCompletion.start_logging()\\n\",\n+    \"\\n\",\n+    \"# 1. create an RetrieveAssistantAgent instance named \\\"assistant\\\"\\n\",\n+    \"assistant = RetrieveAssistantAgent(\\n\",\n+    \"    name=\\\"assistant\\\", \\n\",\n+    \"    system_message=\\\"You are a helpful assistant.\\\",\\n\",\n+    \"    llm_config={\\n\",\n+    \"        \\\"request_timeout\\\": 600,\\n\",\n+    \"        \\\"seed\\\": 42,\\n\",\n+    \"        \\\"config_list\\\": config_list,\\n\",\n+    \"    },\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# 2. create the QdrantRetrieveUserProxyAgent instance named \\\"ragproxyagent\\\"\\n\",\n+    \"# By default, the human_input_mode is \\\"ALWAYS\\\", which means the agent will ask for human input at every step. We set it to \\\"NEVER\\\" here.\\n\",\n+    \"# `docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default, \\n\",\n+    \"# it is set to None, which works only if the collection is already created.\\n\",\n+    \"# \\n\",\n+    \"# Here we generated the documentations from FLAML's docstrings. Not needed if you just want to try this notebook but not to reproduce the\\n\",\n+    \"# outputs. Clone the FLAML (https://github.com/microsoft/FLAML) repo and navigate to its website folder. Pip install and run `pydoc-markdown`\\n\",\n+    \"# and it will generate folder `reference` under `website/docs`.\\n\",\n+    \"#\\n\",\n+    \"# `task` indicates the kind of task we're working on. In this example, it's a `code` task.\\n\",\n+    \"# `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\\n\",\n+    \"# We use an in-memory QdrantClient instance here. Not recommended for production.\\n\",\n+    \"# Get the installation instructions here: https://qdrant.tech/documentation/guides/installation/\\n\",\n+    \"ragproxyagent = QdrantRetrieveUserProxyAgent(\\n\",\n+    \"    name=\\\"ragproxyagent\\\",\\n\",\n+    \"    human_input_mode=\\\"NEVER\\\",\\n\",\n+    \"    max_consecutive_auto_reply=10,\\n\",\n+    \"    retrieve_config={\\n\",\n+    \"        \\\"task\\\": \\\"code\\\",\\n\",\n+    \"        \\\"docs_path\\\": \\\"~/path/to/FLAML/website/docs/reference\\\",  # change this to your own path, such as https://raw.githubusercontent.com/microsoft/autogen/main/README.md\\n\",\n+    \"        \\\"chunk_token_size\\\": 2000,\\n\",\n+    \"        \\\"model\\\": config_list[0][\\\"model\\\"],\\n\",\n+    \"        \\\"client\\\": QdrantClient(\\\":memory:\\\"),\\n\",\n+    \"        \\\"embedding_model\\\": \\\"BAAI/bge-small-en-v1.5\\\",\\n\",\n+    \"    },\\n\",\n+    \")\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<a id=\\\"example-1\\\"></a>\\n\",\n+    \"### Example 1\\n\",\n+    \"\\n\",\n+    \"[back to top](#toc)\\n\",\n+    \"\\n\",\n+    \"Use RetrieveChat to answer a question and ask for human-in-loop feedbacks.\\n\",\n+    \"\\n\",\n+    \"Problem: Is there a function named `tune_automl` in FLAML?\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 20,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\u001b[32mAdding doc_id 69 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 0 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 47 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 64 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 65 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 21 to context.\\u001b[0m\\n\",\n+      \"\\u001b[33mragproxyagent\\u001b[0m (to assistant):\\n\",\n+      \"\\n\",\n+      \"You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\\n\",\n+      \"context provided by the user.\\n\",\n+      \"If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\\n\",\n+      \"For code generation, you must obey the following rules:\\n\",\n+      \"Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\\n\",\n+      \"Rule 2. You must follow the formats below to write your code:\\n\",\n+      \"```language\\n\",\n+      \"# your code\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"User's question is: Is there a function called tune_automl?\\n\",\n+      \"\\n\",\n+      \"Context is: {\\n\",\n+      \"  \\\"items\\\": [\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"items\\\": [\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/math_user_proxy_agent\\\",\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/retrieve_assistant_agent\\\",\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/retrieve_user_proxy_agent\\\"\\n\",\n+      \"              ],\\n\",\n+      \"              \\\"label\\\": \\\"autogen.agentchat.contrib\\\",\\n\",\n+      \"              \\\"type\\\": \\\"category\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"reference/autogen/agentchat/agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/assistant_agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/conversable_agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/groupchat\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/user_proxy_agent\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"autogen.agentchat\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/autogen/oai/completion\\\",\\n\",\n+      \"            \\\"reference/autogen/oai/openai_utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"autogen.oai\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/autogen/code_utils\\\",\\n\",\n+      \"        \\\"reference/autogen/math_utils\\\",\\n\",\n+      \"        \\\"reference/autogen/retrieve_utils\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"autogen\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"items\\\": [\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/trainer\\\",\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/training_args\\\",\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/utils\\\"\\n\",\n+      \"              ],\\n\",\n+      \"              \\\"label\\\": \\\"automl.nlp.huggingface\\\",\\n\",\n+      \"              \\\"type\\\": \\\"category\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"reference/automl/nlp/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.nlp\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/spark/metrics\\\",\\n\",\n+      \"            \\\"reference/automl/spark/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.spark\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/task/task\\\",\\n\",\n+      \"            \\\"reference/automl/task/time_series_task\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.task\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/time_series/sklearn\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/tft\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/ts_data\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/ts_model\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.time_series\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/automl/automl\\\",\\n\",\n+      \"        \\\"reference/automl/data\\\",\\n\",\n+      \"        \\\"reference/automl/ml\\\",\\n\",\n+      \"        \\\"reference/automl/model\\\",\\n\",\n+      \"        \\\"reference/automl/state\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"automl\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        \\\"reference/default/estimator\\\",\\n\",\n+      \"        \\\"reference/default/greedy\\\",\\n\",\n+      \"        \\\"reference/default/portfolio\\\",\\n\",\n+      \"        \\\"reference/default/suggest\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"default\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        \\\"reference/onlineml/autovw\\\",\\n\",\n+      \"        \\\"reference/onlineml/trial\\\",\\n\",\n+      \"        \\\"reference/onlineml/trial_runner\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"onlineml\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/scheduler/online_scheduler\\\",\\n\",\n+      \"            \\\"reference/tune/scheduler/trial_scheduler\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.scheduler\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/searcher/blendsearch\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/cfo_cat\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/flow2\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/online_searcher\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/search_thread\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/suggestion\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/variant_generator\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.searcher\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/spark/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.spark\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/tune/analysis\\\",\\n\",\n+      \"        \\\"reference/tune/sample\\\",\\n\",\n+      \"        \\\"reference/tune/space\\\",\\n\",\n+      \"        \\\"reference/tune/trial\\\",\\n\",\n+      \"        \\\"reference/tune/trial_runner\\\",\\n\",\n+      \"        \\\"reference/tune/tune\\\",\\n\",\n+      \"        \\\"reference/tune/utils\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"tune\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    \\\"reference/config\\\"\\n\",\n+      \"  ],\\n\",\n+      \"  \\\"label\\\": \\\"Reference\\\",\\n\",\n+      \"  \\\"type\\\": \\\"category\\\"\\n\",\n+      \"}\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: config\\n\",\n+      \"title: config\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"!\\n\",\n+      \"* Copyright (c) Microsoft Corporation. All rights reserved.\\n\",\n+      \"* Licensed under the MIT License.\\n\",\n+      \"\\n\",\n+      \"#### PENALTY\\n\",\n+      \"\\n\",\n+      \"penalty term for constraints\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: trial_scheduler\\n\",\n+      \"title: tune.scheduler.trial_scheduler\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## TrialScheduler Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class TrialScheduler()\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Interface for implementing a Trial Scheduler class.\\n\",\n+      \"\\n\",\n+      \"#### CONTINUE\\n\",\n+      \"\\n\",\n+      \"Status for continuing trial execution\\n\",\n+      \"\\n\",\n+      \"#### PAUSE\\n\",\n+      \"\\n\",\n+      \"Status for pausing trial execution\\n\",\n+      \"\\n\",\n+      \"#### STOP\\n\",\n+      \"\\n\",\n+      \"Status for stopping trial execution\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: retrieve_user_proxy_agent\\n\",\n+      \"title: autogen.agentchat.contrib.retrieve_user_proxy_agent\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## RetrieveUserProxyAgent Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class RetrieveUserProxyAgent(UserProxyAgent)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(name=\\\"RetrieveChatAgent\\\",\\n\",\n+      \"             is_termination_msg: Optional[Callable[\\n\",\n+      \"                 [Dict], bool]] = _is_termination_msg_retrievechat,\\n\",\n+      \"             human_input_mode: Optional[str] = \\\"ALWAYS\\\",\\n\",\n+      \"             retrieve_config: Optional[Dict] = None,\\n\",\n+      \"             **kwargs)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `name` _str_ - name of the agent.\\n\",\n+      \"- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\\n\",\n+      \"  Possible values are \\\"ALWAYS\\\", \\\"TERMINATE\\\", \\\"NEVER\\\".\\n\",\n+      \"  (1) When \\\"ALWAYS\\\", the agent prompts for human input every time a message is received.\\n\",\n+      \"  Under this mode, the conversation stops when the human input is \\\"exit\\\",\\n\",\n+      \"  or when is_termination_msg is True and there is no human input.\\n\",\n+      \"  (2) When \\\"TERMINATE\\\", the agent only prompts for human input only when a termination message is received or\\n\",\n+      \"  the number of auto reply reaches the max_consecutive_auto_reply.\\n\",\n+      \"  (3) When \\\"NEVER\\\", the agent will never prompt for human input. Under this mode, the conversation stops\\n\",\n+      \"  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\\n\",\n+      \"- `retrieve_config` _dict or None_ - config for the retrieve agent.\\n\",\n+      \"  To use default config, set to None. Otherwise, set to a dictionary with the following keys:\\n\",\n+      \"  - task (Optional, str): the task of the retrieve chat. Possible values are \\\"code\\\", \\\"qa\\\" and \\\"default\\\". System\\n\",\n+      \"  prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\\n\",\n+      \"  - client (Optional, chromadb.Client): the chromadb client.\\n\",\n+      \"  If key not provided, a default client `chromadb.Client()` will be used.\\n\",\n+      \"  - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\\n\",\n+      \"  or the url to a single file. If key not provided, a default path `./docs` will be used.\\n\",\n+      \"  - collection_name (Optional, str): the name of the collection.\\n\",\n+      \"  If key not provided, a default name `flaml-docs` will be used.\\n\",\n+      \"  - model (Optional, str): the model to use for the retrieve chat.\\n\",\n+      \"  If key not provided, a default model `gpt-4` will be used.\\n\",\n+      \"  - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\\n\",\n+      \"  If key not provided, a default size `max_tokens * 0.4` will be used.\\n\",\n+      \"  - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\\n\",\n+      \"  If key not provided, a default size `max_tokens * 0.8` will be used.\\n\",\n+      \"  - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\\n\",\n+      \"  \\\"multi_lines\\\" and \\\"one_line\\\". If key not provided, a default mode `multi_lines` will be used.\\n\",\n+      \"  - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\\n\",\n+      \"  If chunk_mode is \\\"one_line\\\", this parameter will be ignored.\\n\",\n+      \"  - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\\n\",\n+      \"  If key not provided, a default model `all-MiniLM-L6-v2` will be used. All available models\\n\",\n+      \"  can be found at `https://www.sbert.net/docs/pretrained_models.html`. The default model is a\\n\",\n+      \"  fast model. If you want to use a high performance model, `all-mpnet-base-v2` is recommended.\\n\",\n+      \"  - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\\n\",\n+      \"- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](user_proxy_agent#__init__).\\n\",\n+      \"\\n\",\n+      \"#### generate\\\\_init\\\\_message\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def generate_init_message(problem: str,\\n\",\n+      \"                          n_results: int = 20,\\n\",\n+      \"                          search_string: str = \\\"\\\")\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Generate an initial message with the given problem and prompt.\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `problem` _str_ - the problem to be solved.\\n\",\n+      \"- `n_results` _int_ - the number of results to be retrieved.\\n\",\n+      \"- `search_string` _str_ - only docs containing this string will be retrieved.\\n\",\n+      \"  \\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"- `str` - the generated prompt ready to be sent to the assistant agent.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: retrieve_assistant_agent\\n\",\n+      \"title: autogen.agentchat.contrib.retrieve_assistant_agent\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## RetrieveAssistantAgent Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class RetrieveAssistantAgent(AssistantAgent)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"(Experimental) Retrieve Assistant agent, designed to solve a task with LLM.\\n\",\n+      \"\\n\",\n+      \"RetrieveAssistantAgent is a subclass of AssistantAgent configured with a default system message.\\n\",\n+      \"The default system message is designed to solve a task with LLM,\\n\",\n+      \"including suggesting python code blocks and debugging.\\n\",\n+      \"`human_input_mode` is default to \\\"NEVER\\\"\\n\",\n+      \"and `code_execution_config` is default to False.\\n\",\n+      \"This agent doesn't execute code by default, and expects the user to execute the code.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: utils\\n\",\n+      \"title: automl.nlp.huggingface.utils\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### todf\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def todf(X, Y, column_name)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"todf converts Y from any format (list, pandas.Series, numpy array) to a DataFrame before being returned\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------------\\n\",\n+      \"\\u001b[33massistant\\u001b[0m (to ragproxyagent):\\n\",\n+      \"\\n\",\n+      \"No, there is no function called `tune_automl` in the given context.\\n\",\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------------\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"# reset the assistant. Always reset the assistant before starting a new conversation.\\n\",\n+    \"assistant.reset()\\n\",\n+    \"\\n\",\n+    \"qa_problem = \\\"Is there a function called tune_automl?\\\"\\n\",\n+    \"ragproxyagent.initiate_chat(assistant, problem=qa_problem)\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<a id=\\\"example-2\\\"></a>\\n\",\n+    \"### Example 2\\n\",\n+    \"\\n\",\n+    \"[back to top](#toc)\\n\",\n+    \"\\n\",\n+    \"Use RetrieveChat to answer a question that is not related to code generation.\\n\",\n+    \"\\n\",\n+    \"Problem: Who is the author of FLAML?\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 14,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\u001b[32mAdding doc_id 0 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 21 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 47 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 35 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 41 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 69 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 34 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 22 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 51 to context.\\u001b[0m\\n\",\n+      \"\\u001b[33mragproxyagent\\u001b[0m (to assistant):\\n\",\n+      \"\\n\",\n+      \"You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\\n\",\n+      \"context provided by the user.\\n\",\n+      \"If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\\n\",\n+      \"For code generation, you must obey the following rules:\\n\",\n+      \"Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\\n\",\n+      \"Rule 2. You must follow the formats below to write your code:\\n\",\n+      \"```language\\n\",\n+      \"# your code\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"User's question is: Who is the author of FLAML?\\n\",\n+      \"\\n\",\n+      \"Context is: ---\\n\",\n+      \"sidebar_label: config\\n\",\n+      \"title: config\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"!\\n\",\n+      \"* Copyright (c) Microsoft Corporation. All rights reserved.\\n\",\n+      \"* Licensed under the MIT License.\\n\",\n+      \"\\n\",\n+      \"#### PENALTY\\n\",\n+      \"\\n\",\n+      \"penalty term for constraints\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: utils\\n\",\n+      \"title: automl.nlp.huggingface.utils\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### todf\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def todf(X, Y, column_name)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"todf converts Y from any format (list, pandas.Series, numpy array) to a DataFrame before being returned\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: trial_scheduler\\n\",\n+      \"title: tune.scheduler.trial_scheduler\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## TrialScheduler Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class TrialScheduler()\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Interface for implementing a Trial Scheduler class.\\n\",\n+      \"\\n\",\n+      \"#### CONTINUE\\n\",\n+      \"\\n\",\n+      \"Status for continuing trial execution\\n\",\n+      \"\\n\",\n+      \"#### PAUSE\\n\",\n+      \"\\n\",\n+      \"Status for pausing trial execution\\n\",\n+      \"\\n\",\n+      \"#### STOP\\n\",\n+      \"\\n\",\n+      \"Status for stopping trial execution\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: space\\n\",\n+      \"title: tune.space\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### is\\\\_constant\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def is_constant(space: Union[Dict, List]) -> bool\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Whether the search space is all constant.\\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"  A bool of whether the search space is all constant.\\n\",\n+      \"\\n\",\n+      \"#### define\\\\_by\\\\_run\\\\_func\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def define_by_run_func(trial,\\n\",\n+      \"                       space: Dict,\\n\",\n+      \"                       path: str = \\\"\\\") -> Optional[Dict[str, Any]]\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Define-by-run function to create the search space.\\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"  A dict with constant values.\\n\",\n+      \"\\n\",\n+      \"#### unflatten\\\\_hierarchical\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def unflatten_hierarchical(config: Dict, space: Dict) -> Tuple[Dict, Dict]\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Unflatten hierarchical config.\\n\",\n+      \"\\n\",\n+      \"#### add\\\\_cost\\\\_to\\\\_space\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def add_cost_to_space(space: Dict, low_cost_point: Dict, choice_cost: Dict)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Update the space in place by adding low_cost_point and choice_cost.\\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"  A dict with constant values.\\n\",\n+      \"\\n\",\n+      \"#### normalize\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def normalize(config: Dict,\\n\",\n+      \"              space: Dict,\\n\",\n+      \"              reference_config: Dict,\\n\",\n+      \"              normalized_reference_config: Dict,\\n\",\n+      \"              recursive: bool = False)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Normalize config in space according to reference_config.\\n\",\n+      \"\\n\",\n+      \"Normalize each dimension in config to [0,1].\\n\",\n+      \"\\n\",\n+      \"#### indexof\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def indexof(domain: Dict, config: Dict) -> int\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Find the index of config in domain.categories.\\n\",\n+      \"\\n\",\n+      \"#### complete\\\\_config\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def complete_config(partial_config: Dict,\\n\",\n+      \"                    space: Dict,\\n\",\n+      \"                    flow2,\\n\",\n+      \"                    disturb: bool = False,\\n\",\n+      \"                    lower: Optional[Dict] = None,\\n\",\n+      \"                    upper: Optional[Dict] = None) -> Tuple[Dict, Dict]\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Complete partial config in space.\\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"  config, space.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: search_thread\\n\",\n+      \"title: tune.searcher.search_thread\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## SearchThread Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class SearchThread()\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Class of global or local search thread.\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(mode: str = \\\"min\\\",\\n\",\n+      \"             search_alg: Optional[Searcher] = None,\\n\",\n+      \"             cost_attr: Optional[str] = TIME_TOTAL_S,\\n\",\n+      \"             eps: Optional[float] = 1.0)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"When search_alg is omitted, use local search FLOW2.\\n\",\n+      \"\\n\",\n+      \"#### suggest\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def suggest(trial_id: str) -> Optional[Dict]\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Use the suggest() of the underlying search algorithm.\\n\",\n+      \"\\n\",\n+      \"#### on\\\\_trial\\\\_complete\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def on_trial_complete(trial_id: str,\\n\",\n+      \"                      result: Optional[Dict] = None,\\n\",\n+      \"                      error: bool = False)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Update the statistics of the thread.\\n\",\n+      \"\\n\",\n+      \"#### reach\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def reach(thread) -> bool\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Whether the incumbent can reach the incumbent of thread.\\n\",\n+      \"\\n\",\n+      \"#### can\\\\_suggest\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"@property\\n\",\n+      \"def can_suggest() -> bool\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Whether the thread can suggest new configs.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"{\\n\",\n+      \"  \\\"items\\\": [\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"items\\\": [\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/math_user_proxy_agent\\\",\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/retrieve_assistant_agent\\\",\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/retrieve_user_proxy_agent\\\"\\n\",\n+      \"              ],\\n\",\n+      \"              \\\"label\\\": \\\"autogen.agentchat.contrib\\\",\\n\",\n+      \"              \\\"type\\\": \\\"category\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"reference/autogen/agentchat/agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/assistant_agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/conversable_agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/groupchat\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/user_proxy_agent\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"autogen.agentchat\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/autogen/oai/completion\\\",\\n\",\n+      \"            \\\"reference/autogen/oai/openai_utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"autogen.oai\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/autogen/code_utils\\\",\\n\",\n+      \"        \\\"reference/autogen/math_utils\\\",\\n\",\n+      \"        \\\"reference/autogen/retrieve_utils\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"autogen\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"items\\\": [\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/trainer\\\",\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/training_args\\\",\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/utils\\\"\\n\",\n+      \"              ],\\n\",\n+      \"              \\\"label\\\": \\\"automl.nlp.huggingface\\\",\\n\",\n+      \"              \\\"type\\\": \\\"category\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"reference/automl/nlp/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.nlp\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/spark/metrics\\\",\\n\",\n+      \"            \\\"reference/automl/spark/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.spark\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/task/task\\\",\\n\",\n+      \"            \\\"reference/automl/task/time_series_task\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.task\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/time_series/sklearn\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/tft\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/ts_data\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/ts_model\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.time_series\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/automl/automl\\\",\\n\",\n+      \"        \\\"reference/automl/data\\\",\\n\",\n+      \"        \\\"reference/automl/ml\\\",\\n\",\n+      \"        \\\"reference/automl/model\\\",\\n\",\n+      \"        \\\"reference/automl/state\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"automl\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        \\\"reference/default/estimator\\\",\\n\",\n+      \"        \\\"reference/default/greedy\\\",\\n\",\n+      \"        \\\"reference/default/portfolio\\\",\\n\",\n+      \"        \\\"reference/default/suggest\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"default\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        \\\"reference/onlineml/autovw\\\",\\n\",\n+      \"        \\\"reference/onlineml/trial\\\",\\n\",\n+      \"        \\\"reference/onlineml/trial_runner\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"onlineml\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/scheduler/online_scheduler\\\",\\n\",\n+      \"            \\\"reference/tune/scheduler/trial_scheduler\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.scheduler\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/searcher/blendsearch\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/cfo_cat\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/flow2\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/online_searcher\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/search_thread\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/suggestion\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/variant_generator\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.searcher\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/spark/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.spark\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/tune/analysis\\\",\\n\",\n+      \"        \\\"reference/tune/sample\\\",\\n\",\n+      \"        \\\"reference/tune/space\\\",\\n\",\n+      \"        \\\"reference/tune/trial\\\",\\n\",\n+      \"        \\\"reference/tune/trial_runner\\\",\\n\",\n+      \"        \\\"reference/tune/tune\\\",\\n\",\n+      \"        \\\"reference/tune/utils\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"tune\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    \\\"reference/config\\\"\\n\",\n+      \"  ],\\n\",\n+      \"  \\\"label\\\": \\\"Reference\\\",\\n\",\n+      \"  \\\"type\\\": \\\"category\\\"\\n\",\n+      \"}\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: utils\\n\",\n+      \"title: tune.utils\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### choice\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def choice(categories: Sequence, order=None)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Sample a categorical value.\\n\",\n+      \"Sampling from ``tune.choice([1, 2])`` is equivalent to sampling from\\n\",\n+      \"``np.random.choice([1, 2])``\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `categories` _Sequence_ - Sequence of categories to sample from.\\n\",\n+      \"- `order` _bool_ - Whether the categories have an order. If None, will be decided autoamtically:\\n\",\n+      \"  Numerical categories have an order, while string categories do not.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: trainer\\n\",\n+      \"title: automl.nlp.huggingface.trainer\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## TrainerForAuto Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class TrainerForAuto(Seq2SeqTrainer)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"#### evaluate\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def evaluate(eval_dataset=None, ignore_keys=None, metric_key_prefix=\\\"eval\\\")\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Overriding transformers.Trainer.evaluate by saving metrics and checkpoint path.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: trial\\n\",\n+      \"title: onlineml.trial\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### get\\\\_ns\\\\_feature\\\\_dim\\\\_from\\\\_vw\\\\_example\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def get_ns_feature_dim_from_vw_example(vw_example) -> dict\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Get a dictionary of feature dimensionality for each namespace singleton.\\n\",\n+      \"\\n\",\n+      \"## OnlineResult Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class OnlineResult()\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Class for managing the result statistics of a trial.\\n\",\n+      \"\\n\",\n+      \"#### CB\\\\_COEF\\n\",\n+      \"\\n\",\n+      \"0.001 for mse\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(result_type_name: str,\\n\",\n+      \"             cb_coef: Optional[float] = None,\\n\",\n+      \"             init_loss: Optional[float] = 0.0,\\n\",\n+      \"             init_cb: Optional[float] = 100.0,\\n\",\n+      \"             mode: Optional[str] = \\\"min\\\",\\n\",\n+      \"             sliding_window_size: Optional[int] = 100)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Constructor.\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `result_type_name` - A String to specify the name of the result type.\\n\",\n+      \"- `cb_coef` - a string to specify the coefficient on the confidence bound.\\n\",\n+      \"- `init_loss` - a float to specify the inital loss.\\n\",\n+      \"- `init_cb` - a float to specify the intial confidence bound.\\n\",\n+      \"- `mode` - A string in ['min', 'max'] to specify the objective as\\n\",\n+      \"  minimization or maximization.\\n\",\n+      \"- `sliding_window_size` - An int to specify the size of the sliding window\\n\",\n+      \"  (for experimental purpose).\\n\",\n+      \"\\n\",\n+      \"#### update\\\\_result\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def update_result(new_loss,\\n\",\n+      \"                  new_resource_used,\\n\",\n+      \"                  data_dimension,\\n\",\n+      \"                  bound_of_range=1.0,\\n\",\n+      \"                  new_observation_count=1.0)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Update result statistics.\\n\",\n+      \"\\n\",\n+      \"## BaseOnlineTrial Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class BaseOnlineTrial(Trial)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Class for the online trial.\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(config: dict,\\n\",\n+      \"             min_resource_lease: float,\\n\",\n+      \"             is_champion: Optional[bool] = False,\\n\",\n+      \"             is_checked_under_current_champion: Optional[bool] = True,\\n\",\n+      \"             custom_trial_name: Optional[str] = \\\"mae\\\",\\n\",\n+      \"             trial_id: Optional[str] = None)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Constructor.\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `config` - The configuration dictionary.\\n\",\n+      \"- `min_resource_lease` - A float specifying the minimum resource lease.\\n\",\n+      \"- `is_champion` - A bool variable indicating whether the trial is champion.\\n\",\n+      \"- `is_checked_under_current_champion` - A bool indicating whether the trial\\n\",\n+      \"  has been used under the current champion.\\n\",\n+      \"- `custom_trial_name` - A string of a custom trial name.\\n\",\n+      \"- `trial_id` - A string for the trial id.\\n\",\n+      \"\\n\",\n+      \"#### set\\\\_resource\\\\_lease\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def set_resource_lease(resource: float)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Sets the resource lease accordingly.\\n\",\n+      \"\\n\",\n+      \"#### set\\\\_status\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def set_status(status)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Sets the status of the trial and record the start time.\\n\",\n+      \"\\n\",\n+      \"## VowpalWabbitTrial Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class VowpalWabbitTrial(BaseOnlineTrial)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"The class for Vowpal Wabbit online trials.\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(config: dict,\\n\",\n+      \"             min_resource_lease: float,\\n\",\n+      \"             metric: str = \\\"mae\\\",\\n\",\n+      \"             is_champion: Optional[bool] = False,\\n\",\n+      \"             is_checked_under_current_champion: Optional[bool] = True,\\n\",\n+      \"             custom_trial_name: Optional[str] = \\\"vw_mae_clipped\\\",\\n\",\n+      \"             trial_id: Optional[str] = None,\\n\",\n+      \"             cb_coef: Optional[float] = None)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Constructor.\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `config` _dict_ - the config of the trial (note that the config is a set\\n\",\n+      \"  because the hyperparameters are).\\n\",\n+      \"- `min_resource_lease` _float_ - the minimum resource lease.\\n\",\n+      \"- `metric` _str_ - the loss metric.\\n\",\n+      \"- `is_champion` _bool_ - indicates whether the trial is the current champion or not.\\n\",\n+      \"- `is_checked_under_current_champion` _bool_ - indicates whether this trials has\\n\",\n+      \"  been paused under the current champion.\\n\",\n+      \"- `trial_id` _str_ - id of the trial (if None, it will be generated in the constructor).\\n\",\n+      \"\\n\",\n+      \"#### train\\\\_eval\\\\_model\\\\_online\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def train_eval_model_online(data_sample, y_pred)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Train and evaluate model online.\\n\",\n+      \"\\n\",\n+      \"#### predict\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def predict(x)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Predict using the model.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------------\\n\",\n+      \"\\u001b[33massistant\\u001b[0m (to ragproxyagent):\\n\",\n+      \"\\n\",\n+      \"The author of FLAML is Microsoft Corporation.\\n\",\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------------\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"# reset the assistant. Always reset the assistant before starting a new conversation.\\n\",\n+    \"assistant.reset()\\n\",\n+    \"\\n\",\n+    \"qa_problem = \\\"Who is the author of FLAML?\\\"\\n\",\n+    \"ragproxyagent.initiate_chat(assistant, problem=qa_problem)\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3 (ipykernel)\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.11.6\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 4\n+}\n@@ -0,0 +1,102 @@\n+import os\n+\n+import pytest\n+\n+from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n+from autogen import ChatCompletion, config_list_from_json\n+from test_assistant_agent import KEY_LOC, OAI_CONFIG_LIST\n+\n+try:\n+    from qdrant_client import QdrantClient\n+    from autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent import (\n+        create_qdrant_from_dir,\n+        QdrantRetrieveUserProxyAgent,\n+        query_qdrant,\n+    )\n+    import fastembed\n+\n+    QDRANT_INSTALLED = True\n+except ImportError:\n+    QDRANT_INSTALLED = False\n+\n+test_dir = os.path.join(os.path.dirname(__file__), \"..\", \"test_files\")\n+\n+\n+@pytest.mark.skipif(not QDRANT_INSTALLED, reason=\"qdrant_client is not installed\")\n+def test_retrievechat():\n+    try:\n+        import openai\n+    except ImportError:\n+        return\n+\n+    conversations = {}\n+    ChatCompletion.start_logging(conversations)\n+\n+    config_list = config_list_from_json(\n+        OAI_CONFIG_LIST,\n+        file_location=KEY_LOC,\n+        filter_dict={\n+            \"model\": [\"gpt-4\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\"],\n+        },\n+    )\n+\n+    assistant = RetrieveAssistantAgent(\n+        name=\"assistant\",\n+        system_message=\"You are a helpful assistant.\",\n+        llm_config={\n+            \"request_timeout\": 600,\n+            \"seed\": 42,\n+            \"config_list\": config_list,\n+        },\n+    )\n+\n+    client = QdrantClient(\":memory:\")\n+    ragproxyagent = QdrantRetrieveUserProxyAgent(\n+        name=\"ragproxyagent\",\n+        human_input_mode=\"NEVER\",\n+        max_consecutive_auto_reply=2,\n+        retrieve_config={\n+            \"client\": client,\n+            \"docs_path\": \"./website/docs\",\n+            \"chunk_token_size\": 2000,\n+        },\n+    )\n+\n+    assistant.reset()\n+\n+    code_problem = \"How can I use FLAML to perform a classification task, set use_spark=True, train 30 seconds and force cancel jobs if time limit is reached.\"\n+    ragproxyagent.initiate_chat(assistant, problem=code_problem, silent=True)\n+    print(conversations)\n+\n+\n+@pytest.mark.skipif(not QDRANT_INSTALLED, reason=\"qdrant_client is not installed\")\n+def test_qdrant_filter():\n+    client = QdrantClient(\":memory:\")\n+    create_qdrant_from_dir(dir_path=\"./website/docs\", client=client, collection_name=\"autogen-docs\")\n+    results = query_qdrant(\n+        query_texts=[\"How can I use AutoGen UserProxyAgent and AssistantAgent to do code generation?\"],\n+        n_results=4,\n+        client=client,\n+        collection_name=\"autogen-docs\",\n+        # Return only documents with \"AutoGen\" in the string\n+        search_string=\"AutoGen\",\n+    )\n+    assert len(results[\"ids\"][0]) == 4\n+\n+\n+@pytest.mark.skipif(not QDRANT_INSTALLED, reason=\"qdrant_client is not installed\")\n+def test_qdrant_search():\n+    client = QdrantClient(\":memory:\")\n+    create_qdrant_from_dir(test_dir, client=client)\n+\n+    assert client.get_collection(\"all-my-documents\")\n+\n+    # Perform a semantic search without any filter\n+    results = query_qdrant([\"autogen\"], client=client)\n+    assert isinstance(results, dict) and any(\"autogen\" in res[0].lower() for res in results.get(\"documents\", []))\n+\n+\n+if __name__ == \"__main__\":\n+    test_retrievechat()\n+    test_qdrant_filter()\n+    test_qdrant_search()\n@@ -5,11 +5,13 @@ Please find documentation about this feature [here](/docs/Use-Cases/agent_chat).\n \n Links to notebook examples:\n \n+\n 1. **Code Generation, Execution, and Debugging**\n \n    - Automated Task Solving with Code Generation, Execution & Debugging - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)\n    - Auto Code Generation, Execution, Debugging and Human Feedback - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_human_feedback.ipynb)\n    - Automated Code Generation and Question Answering with Retrieval Augmented Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb)\n+   - Automated Code Generation and Question Answering with [Qdrant](https://qdrant.tech/) based Retrieval Augmented Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb)\n \n 2. **Multi-Agent Collaboration (>3 Agents)**\n \n",
      "head_commit_sha": "50ac5476377c1b41330589a6cfc5c4e65b93079f",
      "name": "Stage 2",
      "patches": [
        {
          "patch": "@@ -0,0 +1,266 @@\n+from typing import Callable, Dict, List, Optional\n+\n+from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n+from autogen.retrieve_utils import get_files_from_dir, split_files_to_chunks\n+import logging\n+\n+logger = logging.getLogger(__name__)\n+\n+try:\n+    from qdrant_client import QdrantClient, models\n+    from qdrant_client.fastembed_common import QueryResponse\n+    import fastembed\n+except ImportError as e:\n+    logging.fatal(\"Failed to import qdrant_client with fastembed. Try running 'pip install qdrant_client[fastembed]'\")\n+    raise e\n+\n+\n+class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n+    def __init__(\n+        self,\n+        name=\"RetrieveChatAgent\",\n+        human_input_mode: str | None = \"ALWAYS\",\n+        is_termination_msg: Callable[[Dict], bool] | None = None,\n+        retrieve_config: Dict | None = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Args:\n+            name (str): name of the agent.\n+            human_input_mode (str): whether to ask for human inputs every time a message is received.\n+                Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n+                (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n+                    Under this mode, the conversation stops when the human input is \"exit\",\n+                    or when is_termination_msg is True and there is no human input.\n+                (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n+                    the number of auto reply reaches the max_consecutive_auto_reply.\n+                (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n+                    when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n+            is_termination_msg (function): a function that takes a message in the form of a dictionary\n+                and returns a boolean value indicating if this received message is a termination message.\n+                The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n+            retrieve_config (dict or None): config for the retrieve agent.\n+                To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n+                - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n+                    prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n+                - client (Optional, qdrant_client.QdrantClient(\":memory:\")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production.\n+                    will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.\n+                - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n+                    or the url to a single file. Default is None, which works only if the collection is already created.\n+                - collection_name (Optional, str): the name of the collection.\n+                    If key not provided, a default name `autogen-docs` will be used.\n+                - model (Optional, str): the model to use for the retrieve chat.\n+                    If key not provided, a default model `gpt-4` will be used.\n+                - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n+                    If key not provided, a default size `max_tokens * 0.4` will be used.\n+                - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n+                    If key not provided, a default size `max_tokens * 0.8` will be used.\n+                - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n+                    \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n+                - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n+                    If chunk_mode is \"one_line\", this parameter will be ignored.\n+                - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n+                    If key not provided, a default model `BAAI/bge-small-en-v1.5` will be used. All available models\n+                    can be found at `https://qdrant.github.io/fastembed/examples/Supported_Models/`.\n+                - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n+                - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n+                    If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n+                - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.\n+                - custom_token_count_function(Optional, Callable): a custom function to count the number of tokens in a string.\n+                    The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).\n+                    Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.\n+                - custom_text_split_function(Optional, Callable): a custom function to split a string into a list of strings.\n+                    Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.\n+                - parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.\n+                - on_disk (Optional, bool): Whether to store the collection on disk. Default is False.\n+                - quantization_config: Quantization configuration. If None, quantization will be disabled.\n+                - hnsw_config: HNSW configuration. If None, default configuration will be used.\n+                  You can find more info about the hnsw configuration options at https://qdrant.tech/documentation/concepts/indexing/#vector-index.\n+                  API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n+                - payload_indexing: Whether to create a payload index for the document field. Default is False.\n+                  You can find more info about the payload indexing options at https://qdrant.tech/documentation/concepts/indexing/#payload-index\n+                  API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index\n+             **kwargs (dict): other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n+\n+        \"\"\"\n+        super().__init__(name, human_input_mode, is_termination_msg, retrieve_config, **kwargs)\n+        self._client = self._retrieve_config.get(\"client\", QdrantClient(\":memory:\"))\n+        self._embedding_model = self._retrieve_config.get(\"embedding_model\", \"BAAI/bge-small-en-v1.5\")\n+        # Uses all available CPU cores to encode data when set to 0\n+        self._parallel = self._retrieve_config.get(\"parallel\", 0)\n+        self._on_disk = self._retrieve_config.get(\"on_disk\", False)\n+        self._quantization_config = self._retrieve_config.get(\"quantization_config\", None)\n+        self._hnsw_config = self._retrieve_config.get(\"hnsw_config\", None)\n+        self._payload_indexing = self._retrieve_config.get(\"payload_indexing\", False)\n+\n+    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\"):\n+        \"\"\"\n+        Args:\n+            problem (str): the problem to be solved.\n+            n_results (int): the number of results to be retrieved.\n+            search_string (str): only docs containing this string will be retrieved.\n+        \"\"\"\n+        if not self._collection:\n+            print(\"Trying to create collection.\")\n+            create_qdrant_from_dir(\n+                dir_path=self._docs_path,\n+                max_tokens=self._chunk_token_size,\n+                client=self._client,\n+                collection_name=self._collection_name,\n+                chunk_mode=self._chunk_mode,\n+                must_break_at_empty_line=self._must_break_at_empty_line,\n+                embedding_model=self._embedding_model,\n+                custom_text_split_function=self.custom_text_split_function,\n+                parallel=self._parallel,\n+                on_disk=self._on_disk,\n+                quantization_config=self._quantization_config,\n+                hnsw_config=self._hnsw_config,\n+                payload_indexing=self._payload_indexing,\n+            )\n+            self._collection = True\n+\n+        results = query_qdrant(\n+            query_texts=problem,\n+            n_results=n_results,\n+            search_string=search_string,\n+            client=self._client,\n+            collection_name=self._collection_name,\n+            embedding_model=self._embedding_model,\n+        )\n+        self._results = results\n+\n+\n+def create_qdrant_from_dir(\n+    dir_path: str,\n+    max_tokens: int = 4000,\n+    client: QdrantClient = None,\n+    collection_name: str = \"all-my-documents\",\n+    chunk_mode: str = \"multi_lines\",\n+    must_break_at_empty_line: bool = True,\n+    embedding_model: str = \"BAAI/bge-small-en-v1.5\",\n+    custom_text_split_function: Callable = None,\n+    parallel: int = 0,\n+    on_disk: bool = False,\n+    quantization_config: Optional[models.QuantizationConfig] = None,\n+    hnsw_config: Optional[models.HnswConfigDiff] = None,\n+    payload_indexing: bool = False,\n+    qdrant_client_options: Optional[Dict] = {},\n+):\n+    \"\"\"Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a url to\n+        a single file.\n+\n+    Args:\n+        dir_path (str): the path to the directory, file or url.\n+        max_tokens (Optional, int): the maximum number of tokens per chunk. Default is 4000.\n+        client (Optional, QdrantClient): the QdrantClient instance. Default is None.\n+        collection_name (Optional, str): the name of the collection. Default is \"all-my-documents\".\n+        chunk_mode (Optional, str): the chunk mode. Default is \"multi_lines\".\n+        must_break_at_empty_line (Optional, bool): Whether to break at empty line. Default is True.\n+        embedding_model (Optional, str): the embedding model to use. Default is \"BAAI/bge-small-en-v1.5\". The list of all the available models can be at https://qdrant.github.io/fastembed/examples/Supported_Models/.\n+        parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores\n+        on_disk (Optional, bool): Whether to store the collection on disk. Default is False.\n+        quantization_config: Quantization configuration. If None, quantization will be disabled. Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n+        hnsw_config: HNSW configuration. If None, default configuration will be used. Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n+        payload_indexing: Whether to create a payload index for the document field. Default is False.\n+        qdrant_client_options: (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.\n+    \"\"\"\n+    if client is None:\n+        client = QdrantClient(**qdrant_client_options)\n+        client.set_model(embedding_model)\n+\n+    if custom_text_split_function is not None:\n+        chunks = split_files_to_chunks(\n+            get_files_from_dir(dir_path), custom_text_split_function=custom_text_split_function\n+        )\n+    else:\n+        chunks = split_files_to_chunks(get_files_from_dir(dir_path), max_tokens, chunk_mode, must_break_at_empty_line)\n+    logger.info(f\"Found {len(chunks)} chunks.\")\n+\n+    # Check if collection by same name exists, if not, create it with custom options\n+    try:\n+        client.get_collection(collection_name=collection_name)\n+    except Exception:\n+        client.create_collection(\n+            collection_name=collection_name,\n+            vectors_config=client.get_fastembed_vector_params(\n+                on_disk=on_disk, quantization_config=quantization_config, hnsw_config=hnsw_config\n+            ),\n+        )\n+        client.get_collection(collection_name=collection_name)\n+\n+    # Upsert in batch of 100 or less if the total number of chunks is less than 100\n+    for i in range(0, len(chunks), min(100, len(chunks))):\n+        end_idx = i + min(100, len(chunks) - i)\n+        client.add(collection_name, documents=chunks[i:end_idx], ids=[j for j in range(i, end_idx)], parallel=parallel)\n+\n+    # Create a payload index for the document field\n+    # Enables highly efficient payload filtering. Reference: https://qdrant.tech/documentation/concepts/indexing/#indexing\n+    # Creating an index requires additional computational resources and memory.\n+    # If filtering performance is critical, we can consider creating an index.\n+    if payload_indexing:\n+        client.create_payload_index(\n+            collection_name=collection_name,\n+            field_name=\"document\",\n+            field_schema=models.TextIndexParams(\n+                type=\"text\",\n+                tokenizer=models.TokenizerType.WORD,\n+                min_token_len=2,\n+                max_token_len=15,\n+            ),\n+        )\n+\n+\n+def query_qdrant(\n+    query_texts: List[str],\n+    n_results: int = 10,\n+    client: QdrantClient = None,\n+    collection_name: str = \"all-my-documents\",\n+    search_string: str = \"\",\n+    embedding_model: str = \"BAAI/bge-small-en-v1.5\",\n+    qdrant_client_options: Optional[Dict] = {},\n+) -> List[List[QueryResponse]]:\n+    \"\"\"Perform a similarity search with filters on a Qdrant collection\n+\n+    Args:\n+        query_texts (List[str]): the query texts.\n+        n_results (Optional, int): the number of results to return. Default is 10.\n+        client (Optional, API): the QdrantClient instance. A default in-memory client will be instantiated if None.\n+        collection_name (Optional, str): the name of the collection. Default is \"all-my-documents\".\n+        search_string (Optional, str): the search string. Default is \"\".\n+        embedding_model (Optional, str): the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if embedding_function is not None.\n+        qdrant_client_options: (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.\n+\n+    Returns:\n+        List[List[QueryResponse]]: the query result. The format is:\n+            class QueryResponse(BaseModel, extra=\"forbid\"):  # type: ignore\n+                id: Union[str, int]\n+                embedding: Optional[List[float]]\n+                metadata: Dict[str, Any]\n+                document: str\n+                score: float\n+    \"\"\"\n+    if client is None:\n+        client = QdrantClient(**qdrant_client_options)\n+        client.set_model(embedding_model)\n+\n+    results = client.query_batch(\n+        collection_name,\n+        query_texts,\n+        limit=n_results,\n+        query_filter=models.Filter(\n+            must=[\n+                models.FieldCondition(\n+                    key=\"document\",\n+                    match=models.MatchText(text=search_string),\n+                )\n+            ]\n+        )\n+        if search_string\n+        else None,\n+    )\n+\n+    data = {\n+        \"ids\": [[result.id for result in sublist] for sublist in results],\n+        \"documents\": [[result.document for result in sublist] for sublist in results],\n+    }\n+    return data",
          "path": "autogen/agentchat/contrib/qdrant_retrieve_user_proxy_agent.py"
        },
        {
          "patch": "@@ -0,0 +1,1234 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<a href=\\\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<a id=\\\"toc\\\"></a>\\n\",\n+    \"# Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering\\n\",\n+    \"\\n\",\n+    \"[Qdrant](https://qdrant.tech/) is a high-performance vector search engine/database.\\n\",\n+    \"\\n\",\n+    \"This notebook demonstrates the usage of `QdrantRetrieveUserProxyAgent` for RAG, based on [agentchat_RetrieveChat.ipynb](https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb).\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"RetrieveChat is a conversational system for retrieve augmented code generation and question answering. In this notebook, we demonstrate how to utilize RetrieveChat to generate code and answer questions based on customized documentations that are not present in the LLM's training dataset. RetrieveChat uses the `RetrieveAssistantAgent` and `QdrantRetrieveUserProxyAgent`, which is similar to the usage of `AssistantAgent` and `UserProxyAgent` in other notebooks (e.g., [Automated Task Solving with Code Generation, Execution & Debugging](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)).\\n\",\n+    \"\\n\",\n+    \"We'll demonstrate usage of RetrieveChat with Qdrant for code generation and question answering w/ human feedback.\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"## Requirements\\n\",\n+    \"\\n\",\n+    \"AutoGen requires `Python>=3.8`. To run this notebook example, please install the [retrievechat] option.\\n\",\n+    \"```bash\\n\",\n+    \"pip install \\\"pyautogen[retrievechat] flaml[automl] qdrant_client[fastembed]\\\"\\n\",\n+    \"```\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Set your API Endpoint\\n\",\n+    \"\\n\",\n+    \"The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 6,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"models to use:  ['gpt-3.5-turbo']\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"import autogen\\n\",\n+    \"\\n\",\n+    \"config_list = autogen.config_list_from_json(\\n\",\n+    \"    env_or_file=\\\"OAI_CONFIG_LIST\\\",\\n\",\n+    \"    file_location=\\\".\\\",\\n\",\n+    \"    filter_dict={\\n\",\n+    \"        \\\"model\\\": {\\n\",\n+    \"            \\\"gpt-4\\\",\\n\",\n+    \"            \\\"gpt4\\\",\\n\",\n+    \"            \\\"gpt-4-32k\\\",\\n\",\n+    \"            \\\"gpt-4-32k-0314\\\",\\n\",\n+    \"            \\\"gpt-35-turbo\\\",\\n\",\n+    \"            \\\"gpt-3.5-turbo\\\",\\n\",\n+    \"        }\\n\",\n+    \"    },\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"assert len(config_list) > 0\\n\",\n+    \"print(\\\"models to use: \\\", [config_list[i][\\\"model\\\"] for i in range(len(config_list))])\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"It first looks for environment variable \\\"OAI_CONFIG_LIST\\\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \\\"OAI_CONFIG_LIST\\\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 and gpt-3.5-turbo models are kept in the list based on the filter condition.\\n\",\n+    \"\\n\",\n+    \"The config list looks like the following:\\n\",\n+    \"```python\\n\",\n+    \"config_list = [\\n\",\n+    \"    {\\n\",\n+    \"        'model': 'gpt-4',\\n\",\n+    \"        'api_key': '<your OpenAI API key here>',\\n\",\n+    \"    },\\n\",\n+    \"    {\\n\",\n+    \"        'model': 'gpt-4',\\n\",\n+    \"        'api_key': '<your Azure OpenAI API key here>',\\n\",\n+    \"        'api_base': '<your Azure OpenAI API base here>',\\n\",\n+    \"        'api_type': 'azure',\\n\",\n+    \"        'api_version': '2023-06-01-preview',\\n\",\n+    \"    },\\n\",\n+    \"    {\\n\",\n+    \"        'model': 'gpt-3.5-turbo',\\n\",\n+    \"        'api_key': '<your Azure OpenAI API key here>',\\n\",\n+    \"        'api_base': '<your Azure OpenAI API base here>',\\n\",\n+    \"        'api_type': 'azure',\\n\",\n+    \"        'api_version': '2023-06-01-preview',\\n\",\n+    \"    },\\n\",\n+    \"]\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \\\"upload file\\\" icon.\\n\",\n+    \"\\n\",\n+    \"You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 11,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Accepted file formats for `docs_path`:\\n\",\n+      \"['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"# Accepted file formats for that can be stored in \\n\",\n+    \"# a vector database instance\\n\",\n+    \"from autogen.retrieve_utils import TEXT_FORMATS\\n\",\n+    \"\\n\",\n+    \"print(\\\"Accepted file formats for `docs_path`:\\\")\\n\",\n+    \"print(TEXT_FORMATS)\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Construct agents for RetrieveChat\\n\",\n+    \"\\n\",\n+    \"We start by initialzing the `RetrieveAssistantAgent` and `QdrantRetrieveUserProxyAgent`. The system message needs to be set to \\\"You are a helpful assistant.\\\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `QdrantRetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a retrieval augmented generation task for an initial prompt to be sent to the LLM assistant.\\n\",\n+    \"\\n\",\n+    \"### You can find the list of all the embedding models supported by Qdrant [here](https://qdrant.github.io/fastembed/examples/Supported_Models/).\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 17,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\\n\",\n+    \"from autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent import QdrantRetrieveUserProxyAgent\\n\",\n+    \"from qdrant_client import QdrantClient\\n\",\n+    \"\\n\",\n+    \"autogen.ChatCompletion.start_logging()\\n\",\n+    \"\\n\",\n+    \"# 1. create an RetrieveAssistantAgent instance named \\\"assistant\\\"\\n\",\n+    \"assistant = RetrieveAssistantAgent(\\n\",\n+    \"    name=\\\"assistant\\\", \\n\",\n+    \"    system_message=\\\"You are a helpful assistant.\\\",\\n\",\n+    \"    llm_config={\\n\",\n+    \"        \\\"request_timeout\\\": 600,\\n\",\n+    \"        \\\"seed\\\": 42,\\n\",\n+    \"        \\\"config_list\\\": config_list,\\n\",\n+    \"    },\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# 2. create the QdrantRetrieveUserProxyAgent instance named \\\"ragproxyagent\\\"\\n\",\n+    \"# By default, the human_input_mode is \\\"ALWAYS\\\", which means the agent will ask for human input at every step. We set it to \\\"NEVER\\\" here.\\n\",\n+    \"# `docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default, \\n\",\n+    \"# it is set to None, which works only if the collection is already created.\\n\",\n+    \"# \\n\",\n+    \"# Here we generated the documentations from FLAML's docstrings. Not needed if you just want to try this notebook but not to reproduce the\\n\",\n+    \"# outputs. Clone the FLAML (https://github.com/microsoft/FLAML) repo and navigate to its website folder. Pip install and run `pydoc-markdown`\\n\",\n+    \"# and it will generate folder `reference` under `website/docs`.\\n\",\n+    \"#\\n\",\n+    \"# `task` indicates the kind of task we're working on. In this example, it's a `code` task.\\n\",\n+    \"# `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\\n\",\n+    \"# We use an in-memory QdrantClient instance here. Not recommended for production.\\n\",\n+    \"# Get the installation instructions here: https://qdrant.tech/documentation/guides/installation/\\n\",\n+    \"ragproxyagent = QdrantRetrieveUserProxyAgent(\\n\",\n+    \"    name=\\\"ragproxyagent\\\",\\n\",\n+    \"    human_input_mode=\\\"NEVER\\\",\\n\",\n+    \"    max_consecutive_auto_reply=10,\\n\",\n+    \"    retrieve_config={\\n\",\n+    \"        \\\"task\\\": \\\"code\\\",\\n\",\n+    \"        \\\"docs_path\\\": \\\"~/path/to/FLAML/website/docs/reference\\\",  # change this to your own path, such as https://raw.githubusercontent.com/microsoft/autogen/main/README.md\\n\",\n+    \"        \\\"chunk_token_size\\\": 2000,\\n\",\n+    \"        \\\"model\\\": config_list[0][\\\"model\\\"],\\n\",\n+    \"        \\\"client\\\": QdrantClient(\\\":memory:\\\"),\\n\",\n+    \"        \\\"embedding_model\\\": \\\"BAAI/bge-small-en-v1.5\\\",\\n\",\n+    \"    },\\n\",\n+    \")\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<a id=\\\"example-1\\\"></a>\\n\",\n+    \"### Example 1\\n\",\n+    \"\\n\",\n+    \"[back to top](#toc)\\n\",\n+    \"\\n\",\n+    \"Use RetrieveChat to answer a question and ask for human-in-loop feedbacks.\\n\",\n+    \"\\n\",\n+    \"Problem: Is there a function named `tune_automl` in FLAML?\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 20,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\u001b[32mAdding doc_id 69 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 0 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 47 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 64 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 65 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 21 to context.\\u001b[0m\\n\",\n+      \"\\u001b[33mragproxyagent\\u001b[0m (to assistant):\\n\",\n+      \"\\n\",\n+      \"You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\\n\",\n+      \"context provided by the user.\\n\",\n+      \"If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\\n\",\n+      \"For code generation, you must obey the following rules:\\n\",\n+      \"Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\\n\",\n+      \"Rule 2. You must follow the formats below to write your code:\\n\",\n+      \"```language\\n\",\n+      \"# your code\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"User's question is: Is there a function called tune_automl?\\n\",\n+      \"\\n\",\n+      \"Context is: {\\n\",\n+      \"  \\\"items\\\": [\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"items\\\": [\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/math_user_proxy_agent\\\",\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/retrieve_assistant_agent\\\",\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/retrieve_user_proxy_agent\\\"\\n\",\n+      \"              ],\\n\",\n+      \"              \\\"label\\\": \\\"autogen.agentchat.contrib\\\",\\n\",\n+      \"              \\\"type\\\": \\\"category\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"reference/autogen/agentchat/agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/assistant_agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/conversable_agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/groupchat\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/user_proxy_agent\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"autogen.agentchat\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/autogen/oai/completion\\\",\\n\",\n+      \"            \\\"reference/autogen/oai/openai_utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"autogen.oai\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/autogen/code_utils\\\",\\n\",\n+      \"        \\\"reference/autogen/math_utils\\\",\\n\",\n+      \"        \\\"reference/autogen/retrieve_utils\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"autogen\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"items\\\": [\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/trainer\\\",\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/training_args\\\",\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/utils\\\"\\n\",\n+      \"              ],\\n\",\n+      \"              \\\"label\\\": \\\"automl.nlp.huggingface\\\",\\n\",\n+      \"              \\\"type\\\": \\\"category\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"reference/automl/nlp/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.nlp\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/spark/metrics\\\",\\n\",\n+      \"            \\\"reference/automl/spark/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.spark\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/task/task\\\",\\n\",\n+      \"            \\\"reference/automl/task/time_series_task\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.task\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/time_series/sklearn\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/tft\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/ts_data\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/ts_model\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.time_series\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/automl/automl\\\",\\n\",\n+      \"        \\\"reference/automl/data\\\",\\n\",\n+      \"        \\\"reference/automl/ml\\\",\\n\",\n+      \"        \\\"reference/automl/model\\\",\\n\",\n+      \"        \\\"reference/automl/state\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"automl\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        \\\"reference/default/estimator\\\",\\n\",\n+      \"        \\\"reference/default/greedy\\\",\\n\",\n+      \"        \\\"reference/default/portfolio\\\",\\n\",\n+      \"        \\\"reference/default/suggest\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"default\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        \\\"reference/onlineml/autovw\\\",\\n\",\n+      \"        \\\"reference/onlineml/trial\\\",\\n\",\n+      \"        \\\"reference/onlineml/trial_runner\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"onlineml\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/scheduler/online_scheduler\\\",\\n\",\n+      \"            \\\"reference/tune/scheduler/trial_scheduler\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.scheduler\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/searcher/blendsearch\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/cfo_cat\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/flow2\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/online_searcher\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/search_thread\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/suggestion\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/variant_generator\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.searcher\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/spark/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.spark\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/tune/analysis\\\",\\n\",\n+      \"        \\\"reference/tune/sample\\\",\\n\",\n+      \"        \\\"reference/tune/space\\\",\\n\",\n+      \"        \\\"reference/tune/trial\\\",\\n\",\n+      \"        \\\"reference/tune/trial_runner\\\",\\n\",\n+      \"        \\\"reference/tune/tune\\\",\\n\",\n+      \"        \\\"reference/tune/utils\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"tune\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    \\\"reference/config\\\"\\n\",\n+      \"  ],\\n\",\n+      \"  \\\"label\\\": \\\"Reference\\\",\\n\",\n+      \"  \\\"type\\\": \\\"category\\\"\\n\",\n+      \"}\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: config\\n\",\n+      \"title: config\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"!\\n\",\n+      \"* Copyright (c) Microsoft Corporation. All rights reserved.\\n\",\n+      \"* Licensed under the MIT License.\\n\",\n+      \"\\n\",\n+      \"#### PENALTY\\n\",\n+      \"\\n\",\n+      \"penalty term for constraints\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: trial_scheduler\\n\",\n+      \"title: tune.scheduler.trial_scheduler\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## TrialScheduler Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class TrialScheduler()\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Interface for implementing a Trial Scheduler class.\\n\",\n+      \"\\n\",\n+      \"#### CONTINUE\\n\",\n+      \"\\n\",\n+      \"Status for continuing trial execution\\n\",\n+      \"\\n\",\n+      \"#### PAUSE\\n\",\n+      \"\\n\",\n+      \"Status for pausing trial execution\\n\",\n+      \"\\n\",\n+      \"#### STOP\\n\",\n+      \"\\n\",\n+      \"Status for stopping trial execution\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: retrieve_user_proxy_agent\\n\",\n+      \"title: autogen.agentchat.contrib.retrieve_user_proxy_agent\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## RetrieveUserProxyAgent Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class RetrieveUserProxyAgent(UserProxyAgent)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(name=\\\"RetrieveChatAgent\\\",\\n\",\n+      \"             is_termination_msg: Optional[Callable[\\n\",\n+      \"                 [Dict], bool]] = _is_termination_msg_retrievechat,\\n\",\n+      \"             human_input_mode: Optional[str] = \\\"ALWAYS\\\",\\n\",\n+      \"             retrieve_config: Optional[Dict] = None,\\n\",\n+      \"             **kwargs)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `name` _str_ - name of the agent.\\n\",\n+      \"- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\\n\",\n+      \"  Possible values are \\\"ALWAYS\\\", \\\"TERMINATE\\\", \\\"NEVER\\\".\\n\",\n+      \"  (1) When \\\"ALWAYS\\\", the agent prompts for human input every time a message is received.\\n\",\n+      \"  Under this mode, the conversation stops when the human input is \\\"exit\\\",\\n\",\n+      \"  or when is_termination_msg is True and there is no human input.\\n\",\n+      \"  (2) When \\\"TERMINATE\\\", the agent only prompts for human input only when a termination message is received or\\n\",\n+      \"  the number of auto reply reaches the max_consecutive_auto_reply.\\n\",\n+      \"  (3) When \\\"NEVER\\\", the agent will never prompt for human input. Under this mode, the conversation stops\\n\",\n+      \"  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\\n\",\n+      \"- `retrieve_config` _dict or None_ - config for the retrieve agent.\\n\",\n+      \"  To use default config, set to None. Otherwise, set to a dictionary with the following keys:\\n\",\n+      \"  - task (Optional, str): the task of the retrieve chat. Possible values are \\\"code\\\", \\\"qa\\\" and \\\"default\\\". System\\n\",\n+      \"  prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\\n\",\n+      \"  - client (Optional, chromadb.Client): the chromadb client.\\n\",\n+      \"  If key not provided, a default client `chromadb.Client()` will be used.\\n\",\n+      \"  - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\\n\",\n+      \"  or the url to a single file. If key not provided, a default path `./docs` will be used.\\n\",\n+      \"  - collection_name (Optional, str): the name of the collection.\\n\",\n+      \"  If key not provided, a default name `flaml-docs` will be used.\\n\",\n+      \"  - model (Optional, str): the model to use for the retrieve chat.\\n\",\n+      \"  If key not provided, a default model `gpt-4` will be used.\\n\",\n+      \"  - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\\n\",\n+      \"  If key not provided, a default size `max_tokens * 0.4` will be used.\\n\",\n+      \"  - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\\n\",\n+      \"  If key not provided, a default size `max_tokens * 0.8` will be used.\\n\",\n+      \"  - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\\n\",\n+      \"  \\\"multi_lines\\\" and \\\"one_line\\\". If key not provided, a default mode `multi_lines` will be used.\\n\",\n+      \"  - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\\n\",\n+      \"  If chunk_mode is \\\"one_line\\\", this parameter will be ignored.\\n\",\n+      \"  - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\\n\",\n+      \"  If key not provided, a default model `all-MiniLM-L6-v2` will be used. All available models\\n\",\n+      \"  can be found at `https://www.sbert.net/docs/pretrained_models.html`. The default model is a\\n\",\n+      \"  fast model. If you want to use a high performance model, `all-mpnet-base-v2` is recommended.\\n\",\n+      \"  - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\\n\",\n+      \"- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](user_proxy_agent#__init__).\\n\",\n+      \"\\n\",\n+      \"#### generate\\\\_init\\\\_message\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def generate_init_message(problem: str,\\n\",\n+      \"                          n_results: int = 20,\\n\",\n+      \"                          search_string: str = \\\"\\\")\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Generate an initial message with the given problem and prompt.\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `problem` _str_ - the problem to be solved.\\n\",\n+      \"- `n_results` _int_ - the number of results to be retrieved.\\n\",\n+      \"- `search_string` _str_ - only docs containing this string will be retrieved.\\n\",\n+      \"  \\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"- `str` - the generated prompt ready to be sent to the assistant agent.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: retrieve_assistant_agent\\n\",\n+      \"title: autogen.agentchat.contrib.retrieve_assistant_agent\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## RetrieveAssistantAgent Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class RetrieveAssistantAgent(AssistantAgent)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"(Experimental) Retrieve Assistant agent, designed to solve a task with LLM.\\n\",\n+      \"\\n\",\n+      \"RetrieveAssistantAgent is a subclass of AssistantAgent configured with a default system message.\\n\",\n+      \"The default system message is designed to solve a task with LLM,\\n\",\n+      \"including suggesting python code blocks and debugging.\\n\",\n+      \"`human_input_mode` is default to \\\"NEVER\\\"\\n\",\n+      \"and `code_execution_config` is default to False.\\n\",\n+      \"This agent doesn't execute code by default, and expects the user to execute the code.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: utils\\n\",\n+      \"title: automl.nlp.huggingface.utils\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### todf\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def todf(X, Y, column_name)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"todf converts Y from any format (list, pandas.Series, numpy array) to a DataFrame before being returned\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------------\\n\",\n+      \"\\u001b[33massistant\\u001b[0m (to ragproxyagent):\\n\",\n+      \"\\n\",\n+      \"No, there is no function called `tune_automl` in the given context.\\n\",\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------------\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"# reset the assistant. Always reset the assistant before starting a new conversation.\\n\",\n+    \"assistant.reset()\\n\",\n+    \"\\n\",\n+    \"qa_problem = \\\"Is there a function called tune_automl?\\\"\\n\",\n+    \"ragproxyagent.initiate_chat(assistant, problem=qa_problem)\"\n+   ]\n+  },\n+  {\n+   \"attachments\": {},\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<a id=\\\"example-2\\\"></a>\\n\",\n+    \"### Example 2\\n\",\n+    \"\\n\",\n+    \"[back to top](#toc)\\n\",\n+    \"\\n\",\n+    \"Use RetrieveChat to answer a question that is not related to code generation.\\n\",\n+    \"\\n\",\n+    \"Problem: Who is the author of FLAML?\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 14,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\u001b[32mAdding doc_id 0 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 21 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 47 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 35 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 41 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 69 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 34 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 22 to context.\\u001b[0m\\n\",\n+      \"\\u001b[32mAdding doc_id 51 to context.\\u001b[0m\\n\",\n+      \"\\u001b[33mragproxyagent\\u001b[0m (to assistant):\\n\",\n+      \"\\n\",\n+      \"You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\\n\",\n+      \"context provided by the user.\\n\",\n+      \"If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\\n\",\n+      \"For code generation, you must obey the following rules:\\n\",\n+      \"Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\\n\",\n+      \"Rule 2. You must follow the formats below to write your code:\\n\",\n+      \"```language\\n\",\n+      \"# your code\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"User's question is: Who is the author of FLAML?\\n\",\n+      \"\\n\",\n+      \"Context is: ---\\n\",\n+      \"sidebar_label: config\\n\",\n+      \"title: config\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"!\\n\",\n+      \"* Copyright (c) Microsoft Corporation. All rights reserved.\\n\",\n+      \"* Licensed under the MIT License.\\n\",\n+      \"\\n\",\n+      \"#### PENALTY\\n\",\n+      \"\\n\",\n+      \"penalty term for constraints\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: utils\\n\",\n+      \"title: automl.nlp.huggingface.utils\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### todf\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def todf(X, Y, column_name)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"todf converts Y from any format (list, pandas.Series, numpy array) to a DataFrame before being returned\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: trial_scheduler\\n\",\n+      \"title: tune.scheduler.trial_scheduler\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## TrialScheduler Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class TrialScheduler()\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Interface for implementing a Trial Scheduler class.\\n\",\n+      \"\\n\",\n+      \"#### CONTINUE\\n\",\n+      \"\\n\",\n+      \"Status for continuing trial execution\\n\",\n+      \"\\n\",\n+      \"#### PAUSE\\n\",\n+      \"\\n\",\n+      \"Status for pausing trial execution\\n\",\n+      \"\\n\",\n+      \"#### STOP\\n\",\n+      \"\\n\",\n+      \"Status for stopping trial execution\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: space\\n\",\n+      \"title: tune.space\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### is\\\\_constant\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def is_constant(space: Union[Dict, List]) -> bool\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Whether the search space is all constant.\\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"  A bool of whether the search space is all constant.\\n\",\n+      \"\\n\",\n+      \"#### define\\\\_by\\\\_run\\\\_func\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def define_by_run_func(trial,\\n\",\n+      \"                       space: Dict,\\n\",\n+      \"                       path: str = \\\"\\\") -> Optional[Dict[str, Any]]\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Define-by-run function to create the search space.\\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"  A dict with constant values.\\n\",\n+      \"\\n\",\n+      \"#### unflatten\\\\_hierarchical\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def unflatten_hierarchical(config: Dict, space: Dict) -> Tuple[Dict, Dict]\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Unflatten hierarchical config.\\n\",\n+      \"\\n\",\n+      \"#### add\\\\_cost\\\\_to\\\\_space\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def add_cost_to_space(space: Dict, low_cost_point: Dict, choice_cost: Dict)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Update the space in place by adding low_cost_point and choice_cost.\\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"  A dict with constant values.\\n\",\n+      \"\\n\",\n+      \"#### normalize\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def normalize(config: Dict,\\n\",\n+      \"              space: Dict,\\n\",\n+      \"              reference_config: Dict,\\n\",\n+      \"              normalized_reference_config: Dict,\\n\",\n+      \"              recursive: bool = False)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Normalize config in space according to reference_config.\\n\",\n+      \"\\n\",\n+      \"Normalize each dimension in config to [0,1].\\n\",\n+      \"\\n\",\n+      \"#### indexof\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def indexof(domain: Dict, config: Dict) -> int\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Find the index of config in domain.categories.\\n\",\n+      \"\\n\",\n+      \"#### complete\\\\_config\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def complete_config(partial_config: Dict,\\n\",\n+      \"                    space: Dict,\\n\",\n+      \"                    flow2,\\n\",\n+      \"                    disturb: bool = False,\\n\",\n+      \"                    lower: Optional[Dict] = None,\\n\",\n+      \"                    upper: Optional[Dict] = None) -> Tuple[Dict, Dict]\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Complete partial config in space.\\n\",\n+      \"\\n\",\n+      \"**Returns**:\\n\",\n+      \"\\n\",\n+      \"  config, space.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: search_thread\\n\",\n+      \"title: tune.searcher.search_thread\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## SearchThread Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class SearchThread()\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Class of global or local search thread.\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(mode: str = \\\"min\\\",\\n\",\n+      \"             search_alg: Optional[Searcher] = None,\\n\",\n+      \"             cost_attr: Optional[str] = TIME_TOTAL_S,\\n\",\n+      \"             eps: Optional[float] = 1.0)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"When search_alg is omitted, use local search FLOW2.\\n\",\n+      \"\\n\",\n+      \"#### suggest\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def suggest(trial_id: str) -> Optional[Dict]\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Use the suggest() of the underlying search algorithm.\\n\",\n+      \"\\n\",\n+      \"#### on\\\\_trial\\\\_complete\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def on_trial_complete(trial_id: str,\\n\",\n+      \"                      result: Optional[Dict] = None,\\n\",\n+      \"                      error: bool = False)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Update the statistics of the thread.\\n\",\n+      \"\\n\",\n+      \"#### reach\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def reach(thread) -> bool\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Whether the incumbent can reach the incumbent of thread.\\n\",\n+      \"\\n\",\n+      \"#### can\\\\_suggest\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"@property\\n\",\n+      \"def can_suggest() -> bool\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Whether the thread can suggest new configs.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"{\\n\",\n+      \"  \\\"items\\\": [\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"items\\\": [\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/math_user_proxy_agent\\\",\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/retrieve_assistant_agent\\\",\\n\",\n+      \"                \\\"reference/autogen/agentchat/contrib/retrieve_user_proxy_agent\\\"\\n\",\n+      \"              ],\\n\",\n+      \"              \\\"label\\\": \\\"autogen.agentchat.contrib\\\",\\n\",\n+      \"              \\\"type\\\": \\\"category\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"reference/autogen/agentchat/agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/assistant_agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/conversable_agent\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/groupchat\\\",\\n\",\n+      \"            \\\"reference/autogen/agentchat/user_proxy_agent\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"autogen.agentchat\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/autogen/oai/completion\\\",\\n\",\n+      \"            \\\"reference/autogen/oai/openai_utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"autogen.oai\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/autogen/code_utils\\\",\\n\",\n+      \"        \\\"reference/autogen/math_utils\\\",\\n\",\n+      \"        \\\"reference/autogen/retrieve_utils\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"autogen\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            {\\n\",\n+      \"              \\\"items\\\": [\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/trainer\\\",\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/training_args\\\",\\n\",\n+      \"                \\\"reference/automl/nlp/huggingface/utils\\\"\\n\",\n+      \"              ],\\n\",\n+      \"              \\\"label\\\": \\\"automl.nlp.huggingface\\\",\\n\",\n+      \"              \\\"type\\\": \\\"category\\\"\\n\",\n+      \"            },\\n\",\n+      \"            \\\"reference/automl/nlp/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.nlp\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/spark/metrics\\\",\\n\",\n+      \"            \\\"reference/automl/spark/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.spark\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/task/task\\\",\\n\",\n+      \"            \\\"reference/automl/task/time_series_task\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.task\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/automl/time_series/sklearn\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/tft\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/ts_data\\\",\\n\",\n+      \"            \\\"reference/automl/time_series/ts_model\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"automl.time_series\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/automl/automl\\\",\\n\",\n+      \"        \\\"reference/automl/data\\\",\\n\",\n+      \"        \\\"reference/automl/ml\\\",\\n\",\n+      \"        \\\"reference/automl/model\\\",\\n\",\n+      \"        \\\"reference/automl/state\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"automl\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        \\\"reference/default/estimator\\\",\\n\",\n+      \"        \\\"reference/default/greedy\\\",\\n\",\n+      \"        \\\"reference/default/portfolio\\\",\\n\",\n+      \"        \\\"reference/default/suggest\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"default\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        \\\"reference/onlineml/autovw\\\",\\n\",\n+      \"        \\\"reference/onlineml/trial\\\",\\n\",\n+      \"        \\\"reference/onlineml/trial_runner\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"onlineml\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    {\\n\",\n+      \"      \\\"items\\\": [\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/scheduler/online_scheduler\\\",\\n\",\n+      \"            \\\"reference/tune/scheduler/trial_scheduler\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.scheduler\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/searcher/blendsearch\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/cfo_cat\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/flow2\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/online_searcher\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/search_thread\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/suggestion\\\",\\n\",\n+      \"            \\\"reference/tune/searcher/variant_generator\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.searcher\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        {\\n\",\n+      \"          \\\"items\\\": [\\n\",\n+      \"            \\\"reference/tune/spark/utils\\\"\\n\",\n+      \"          ],\\n\",\n+      \"          \\\"label\\\": \\\"tune.spark\\\",\\n\",\n+      \"          \\\"type\\\": \\\"category\\\"\\n\",\n+      \"        },\\n\",\n+      \"        \\\"reference/tune/analysis\\\",\\n\",\n+      \"        \\\"reference/tune/sample\\\",\\n\",\n+      \"        \\\"reference/tune/space\\\",\\n\",\n+      \"        \\\"reference/tune/trial\\\",\\n\",\n+      \"        \\\"reference/tune/trial_runner\\\",\\n\",\n+      \"        \\\"reference/tune/tune\\\",\\n\",\n+      \"        \\\"reference/tune/utils\\\"\\n\",\n+      \"      ],\\n\",\n+      \"      \\\"label\\\": \\\"tune\\\",\\n\",\n+      \"      \\\"type\\\": \\\"category\\\"\\n\",\n+      \"    },\\n\",\n+      \"    \\\"reference/config\\\"\\n\",\n+      \"  ],\\n\",\n+      \"  \\\"label\\\": \\\"Reference\\\",\\n\",\n+      \"  \\\"type\\\": \\\"category\\\"\\n\",\n+      \"}\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: utils\\n\",\n+      \"title: tune.utils\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### choice\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def choice(categories: Sequence, order=None)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Sample a categorical value.\\n\",\n+      \"Sampling from ``tune.choice([1, 2])`` is equivalent to sampling from\\n\",\n+      \"``np.random.choice([1, 2])``\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `categories` _Sequence_ - Sequence of categories to sample from.\\n\",\n+      \"- `order` _bool_ - Whether the categories have an order. If None, will be decided autoamtically:\\n\",\n+      \"  Numerical categories have an order, while string categories do not.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: trainer\\n\",\n+      \"title: automl.nlp.huggingface.trainer\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"## TrainerForAuto Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class TrainerForAuto(Seq2SeqTrainer)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"#### evaluate\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def evaluate(eval_dataset=None, ignore_keys=None, metric_key_prefix=\\\"eval\\\")\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Overriding transformers.Trainer.evaluate by saving metrics and checkpoint path.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"---\\n\",\n+      \"sidebar_label: trial\\n\",\n+      \"title: onlineml.trial\\n\",\n+      \"---\\n\",\n+      \"\\n\",\n+      \"#### get\\\\_ns\\\\_feature\\\\_dim\\\\_from\\\\_vw\\\\_example\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def get_ns_feature_dim_from_vw_example(vw_example) -> dict\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Get a dictionary of feature dimensionality for each namespace singleton.\\n\",\n+      \"\\n\",\n+      \"## OnlineResult Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class OnlineResult()\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Class for managing the result statistics of a trial.\\n\",\n+      \"\\n\",\n+      \"#### CB\\\\_COEF\\n\",\n+      \"\\n\",\n+      \"0.001 for mse\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(result_type_name: str,\\n\",\n+      \"             cb_coef: Optional[float] = None,\\n\",\n+      \"             init_loss: Optional[float] = 0.0,\\n\",\n+      \"             init_cb: Optional[float] = 100.0,\\n\",\n+      \"             mode: Optional[str] = \\\"min\\\",\\n\",\n+      \"             sliding_window_size: Optional[int] = 100)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Constructor.\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `result_type_name` - A String to specify the name of the result type.\\n\",\n+      \"- `cb_coef` - a string to specify the coefficient on the confidence bound.\\n\",\n+      \"- `init_loss` - a float to specify the inital loss.\\n\",\n+      \"- `init_cb` - a float to specify the intial confidence bound.\\n\",\n+      \"- `mode` - A string in ['min', 'max'] to specify the objective as\\n\",\n+      \"  minimization or maximization.\\n\",\n+      \"- `sliding_window_size` - An int to specify the size of the sliding window\\n\",\n+      \"  (for experimental purpose).\\n\",\n+      \"\\n\",\n+      \"#### update\\\\_result\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def update_result(new_loss,\\n\",\n+      \"                  new_resource_used,\\n\",\n+      \"                  data_dimension,\\n\",\n+      \"                  bound_of_range=1.0,\\n\",\n+      \"                  new_observation_count=1.0)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Update result statistics.\\n\",\n+      \"\\n\",\n+      \"## BaseOnlineTrial Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class BaseOnlineTrial(Trial)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Class for the online trial.\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(config: dict,\\n\",\n+      \"             min_resource_lease: float,\\n\",\n+      \"             is_champion: Optional[bool] = False,\\n\",\n+      \"             is_checked_under_current_champion: Optional[bool] = True,\\n\",\n+      \"             custom_trial_name: Optional[str] = \\\"mae\\\",\\n\",\n+      \"             trial_id: Optional[str] = None)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Constructor.\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `config` - The configuration dictionary.\\n\",\n+      \"- `min_resource_lease` - A float specifying the minimum resource lease.\\n\",\n+      \"- `is_champion` - A bool variable indicating whether the trial is champion.\\n\",\n+      \"- `is_checked_under_current_champion` - A bool indicating whether the trial\\n\",\n+      \"  has been used under the current champion.\\n\",\n+      \"- `custom_trial_name` - A string of a custom trial name.\\n\",\n+      \"- `trial_id` - A string for the trial id.\\n\",\n+      \"\\n\",\n+      \"#### set\\\\_resource\\\\_lease\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def set_resource_lease(resource: float)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Sets the resource lease accordingly.\\n\",\n+      \"\\n\",\n+      \"#### set\\\\_status\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def set_status(status)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Sets the status of the trial and record the start time.\\n\",\n+      \"\\n\",\n+      \"## VowpalWabbitTrial Objects\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"class VowpalWabbitTrial(BaseOnlineTrial)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"The class for Vowpal Wabbit online trials.\\n\",\n+      \"\\n\",\n+      \"#### \\\\_\\\\_init\\\\_\\\\_\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def __init__(config: dict,\\n\",\n+      \"             min_resource_lease: float,\\n\",\n+      \"             metric: str = \\\"mae\\\",\\n\",\n+      \"             is_champion: Optional[bool] = False,\\n\",\n+      \"             is_checked_under_current_champion: Optional[bool] = True,\\n\",\n+      \"             custom_trial_name: Optional[str] = \\\"vw_mae_clipped\\\",\\n\",\n+      \"             trial_id: Optional[str] = None,\\n\",\n+      \"             cb_coef: Optional[float] = None)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Constructor.\\n\",\n+      \"\\n\",\n+      \"**Arguments**:\\n\",\n+      \"\\n\",\n+      \"- `config` _dict_ - the config of the trial (note that the config is a set\\n\",\n+      \"  because the hyperparameters are).\\n\",\n+      \"- `min_resource_lease` _float_ - the minimum resource lease.\\n\",\n+      \"- `metric` _str_ - the loss metric.\\n\",\n+      \"- `is_champion` _bool_ - indicates whether the trial is the current champion or not.\\n\",\n+      \"- `is_checked_under_current_champion` _bool_ - indicates whether this trials has\\n\",\n+      \"  been paused under the current champion.\\n\",\n+      \"- `trial_id` _str_ - id of the trial (if None, it will be generated in the constructor).\\n\",\n+      \"\\n\",\n+      \"#### train\\\\_eval\\\\_model\\\\_online\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def train_eval_model_online(data_sample, y_pred)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Train and evaluate model online.\\n\",\n+      \"\\n\",\n+      \"#### predict\\n\",\n+      \"\\n\",\n+      \"```python\\n\",\n+      \"def predict(x)\\n\",\n+      \"```\\n\",\n+      \"\\n\",\n+      \"Predict using the model.\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------------\\n\",\n+      \"\\u001b[33massistant\\u001b[0m (to ragproxyagent):\\n\",\n+      \"\\n\",\n+      \"The author of FLAML is Microsoft Corporation.\\n\",\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------------\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"# reset the assistant. Always reset the assistant before starting a new conversation.\\n\",\n+    \"assistant.reset()\\n\",\n+    \"\\n\",\n+    \"qa_problem = \\\"Who is the author of FLAML?\\\"\\n\",\n+    \"ragproxyagent.initiate_chat(assistant, problem=qa_problem)\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"Python 3 (ipykernel)\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.11.6\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 4\n+}",
          "path": "notebook/agentchat_qdrant_RetrieveChat.ipynb"
        },
        {
          "patch": "@@ -0,0 +1,102 @@\n+import os\n+\n+import pytest\n+\n+from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n+from autogen import ChatCompletion, config_list_from_json\n+from test_assistant_agent import KEY_LOC, OAI_CONFIG_LIST\n+\n+try:\n+    from qdrant_client import QdrantClient\n+    from autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent import (\n+        create_qdrant_from_dir,\n+        QdrantRetrieveUserProxyAgent,\n+        query_qdrant,\n+    )\n+    import fastembed\n+\n+    QDRANT_INSTALLED = True\n+except ImportError:\n+    QDRANT_INSTALLED = False\n+\n+test_dir = os.path.join(os.path.dirname(__file__), \"..\", \"test_files\")\n+\n+\n+@pytest.mark.skipif(not QDRANT_INSTALLED, reason=\"qdrant_client is not installed\")\n+def test_retrievechat():\n+    try:\n+        import openai\n+    except ImportError:\n+        return\n+\n+    conversations = {}\n+    ChatCompletion.start_logging(conversations)\n+\n+    config_list = config_list_from_json(\n+        OAI_CONFIG_LIST,\n+        file_location=KEY_LOC,\n+        filter_dict={\n+            \"model\": [\"gpt-4\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\"],\n+        },\n+    )\n+\n+    assistant = RetrieveAssistantAgent(\n+        name=\"assistant\",\n+        system_message=\"You are a helpful assistant.\",\n+        llm_config={\n+            \"request_timeout\": 600,\n+            \"seed\": 42,\n+            \"config_list\": config_list,\n+        },\n+    )\n+\n+    client = QdrantClient(\":memory:\")\n+    ragproxyagent = QdrantRetrieveUserProxyAgent(\n+        name=\"ragproxyagent\",\n+        human_input_mode=\"NEVER\",\n+        max_consecutive_auto_reply=2,\n+        retrieve_config={\n+            \"client\": client,\n+            \"docs_path\": \"./website/docs\",\n+            \"chunk_token_size\": 2000,\n+        },\n+    )\n+\n+    assistant.reset()\n+\n+    code_problem = \"How can I use FLAML to perform a classification task, set use_spark=True, train 30 seconds and force cancel jobs if time limit is reached.\"\n+    ragproxyagent.initiate_chat(assistant, problem=code_problem, silent=True)\n+    print(conversations)\n+\n+\n+@pytest.mark.skipif(not QDRANT_INSTALLED, reason=\"qdrant_client is not installed\")\n+def test_qdrant_filter():\n+    client = QdrantClient(\":memory:\")\n+    create_qdrant_from_dir(dir_path=\"./website/docs\", client=client, collection_name=\"autogen-docs\")\n+    results = query_qdrant(\n+        query_texts=[\"How can I use AutoGen UserProxyAgent and AssistantAgent to do code generation?\"],\n+        n_results=4,\n+        client=client,\n+        collection_name=\"autogen-docs\",\n+        # Return only documents with \"AutoGen\" in the string\n+        search_string=\"AutoGen\",\n+    )\n+    assert len(results[\"ids\"][0]) == 4\n+\n+\n+@pytest.mark.skipif(not QDRANT_INSTALLED, reason=\"qdrant_client is not installed\")\n+def test_qdrant_search():\n+    client = QdrantClient(\":memory:\")\n+    create_qdrant_from_dir(test_dir, client=client)\n+\n+    assert client.get_collection(\"all-my-documents\")\n+\n+    # Perform a semantic search without any filter\n+    results = query_qdrant([\"autogen\"], client=client)\n+    assert isinstance(results, dict) and any(\"autogen\" in res[0].lower() for res in results.get(\"documents\", []))\n+\n+\n+if __name__ == \"__main__\":\n+    test_retrievechat()\n+    test_qdrant_filter()\n+    test_qdrant_search()",
          "path": "test/agentchat/test_qdrant_retrievechat.py"
        },
        {
          "patch": "@@ -5,11 +5,13 @@ Please find documentation about this feature [here](/docs/Use-Cases/agent_chat).\n \n Links to notebook examples:\n \n+\n 1. **Code Generation, Execution, and Debugging**\n \n    - Automated Task Solving with Code Generation, Execution & Debugging - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)\n    - Auto Code Generation, Execution, Debugging and Human Feedback - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_human_feedback.ipynb)\n    - Automated Code Generation and Question Answering with Retrieval Augmented Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb)\n+   - Automated Code Generation and Question Answering with [Qdrant](https://qdrant.tech/) based Retrieval Augmented Agents - [View Notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb)\n \n 2. **Multi-Agent Collaboration (>3 Agents)**\n ",
          "path": "website/docs/Examples/AutoGen-AgentChat.md"
        }
      ],
      "pr_number": "303",
      "stage_id": "stage-2",
      "test_query": "You need to modify the files in this repository according to the requirements below. During the analysis process, do not directly read the complete file; try to read the first 10 lines using head or use grep instead. You are NOT allowed to directly run scripts, such as python, pip install, dockerfile, etc.\n\nThe repository requires enhancements to improve documentation clarity and extend vector storage capabilities. First, update the RetrieveChat notebook and installation documentation to eliminate confusion about package relationships and provide clearer environment setup guidance for both venv and conda users. The documentation should explicitly clarify that this project is not a subpackage of FLAML and streamline the onboarding process for new users.\n\nNext, implement support for Qdrant as an additional vector store option by creating a new agent class that extends the existing retrieve user proxy agent functionality. This new integration should be placed in the contrib directory alongside other retrieve agents. The implementation should leverage the existing retrieval utilities while adding Qdrant-specific connection and query logic.\n\nAdditionally, resolve compatibility issues with the ChromaDB integration by adapting to recent class renames in version 0.4.15. The fix should ensure proper collection creation when no client is explicitly provided.\n\nFinally, update the CI workflow configuration and package dependencies to include Qdrant testing requirements, ensuring that the new functionality can be properly validated in automated testing pipelines. The example documentation should also be updated to reflect these new capabilities and provide clearer guidance for users."
    },
    {
      "base_commit_sha": "50ac5476377c1b41330589a6cfc5c4e65b93079f",
      "carry_forward_policy": "reapply_patch",
      "dependencies": [
        "stage-2"
      ],
      "ground_truth_patch": "@@ -56,6 +56,10 @@ jobs:\n       - name: Install packages for Teachable when needed\n         run: |\n           pip install -e .[teachable]\n+      - name: Install packages for RetrieveChat with QDrant when needed\n+        if: matrix.python-version == '3.9'\n+        run: |\n+          pip install qdrant_client[fastembed]\n       - name: Coverage\n         if: matrix.python-version == '3.9'\n         env:\n@@ -18,10 +18,10 @@\n class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n     def __init__(\n         self,\n-        name=\"RetrieveChatAgent\",\n-        human_input_mode: str | None = \"ALWAYS\",\n-        is_termination_msg: Callable[[Dict], bool] | None = None,\n-        retrieve_config: Dict | None = None,\n+        name=\"RetrieveChatAgent\",  # default set to RetrieveChatAgent\n+        human_input_mode: Optional[str] = \"ALWAYS\",\n+        is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n+        retrieve_config: Optional[Dict] = None,  # config for the retrieve agent\n         **kwargs,\n     ):\n         \"\"\"\n@@ -361,7 +361,7 @@ def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str =\n         \"\"\"\n         if not self._collection or self._get_or_create:\n             print(\"Trying to create collection.\")\n-            create_vector_db_from_dir(\n+            self._client = create_vector_db_from_dir(\n                 dir_path=self._docs_path,\n                 max_tokens=self._chunk_token_size,\n                 client=self._client,\n@@ -5,7 +5,11 @@\n import glob\n import tiktoken\n import chromadb\n-from chromadb.api import API\n+\n+if chromadb.__version__ < \"0.4.15\":\n+    from chromadb.api import API\n+else:\n+    from chromadb.api import ClientAPI as API\n from chromadb.api.types import QueryResult\n import chromadb.utils.embedding_functions as ef\n import logging\n@@ -287,7 +291,7 @@ def create_vector_db_from_dir(\n     embedding_model: str = \"all-MiniLM-L6-v2\",\n     embedding_function: Callable = None,\n     custom_text_split_function: Callable = None,\n-):\n+) -> API:\n     \"\"\"Create a vector db from all the files in a given directory, the directory can also be a single file or a url to\n         a single file. We support chromadb compatible APIs to create the vector db, this function is not required if\n         you prepared your own vector db.\n@@ -307,6 +311,9 @@ def create_vector_db_from_dir(\n         embedding_function (Optional, Callable): the embedding function to use. Default is None, SentenceTransformer with\n             the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding\n             functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n+\n+    Returns:\n+        API: the chromadb client.\n     \"\"\"\n     if client is None:\n         client = chromadb.PersistentClient(path=db_path)\n@@ -344,6 +351,7 @@ def create_vector_db_from_dir(\n             )\n     except ValueError as e:\n         logger.warning(f\"{e}\")\n+    return client\n \n \n def query_vector_db(\n@@ -53,6 +53,7 @@\n             \"sympy\",\n             \"tiktoken\",\n             \"wolframalpha\",\n+            \"qdrant_client[fastembed]\",\n         ],\n         \"blendsearch\": [\"flaml[blendsearch]\"],\n         \"mathchat\": [\"sympy\", \"pydantic==1.10.9\", \"wolframalpha\"],\n",
      "head_commit_sha": "27e619e46e3abac5ba6a025698137d9f635c4dd8",
      "name": "Stage 3",
      "patches": [
        {
          "patch": "@@ -56,6 +56,10 @@ jobs:\n       - name: Install packages for Teachable when needed\n         run: |\n           pip install -e .[teachable]\n+      - name: Install packages for RetrieveChat with QDrant when needed\n+        if: matrix.python-version == '3.9'\n+        run: |\n+          pip install qdrant_client[fastembed]\n       - name: Coverage\n         if: matrix.python-version == '3.9'\n         env:",
          "path": ".github/workflows/openai.yml"
        },
        {
          "patch": "@@ -18,10 +18,10 @@\n class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n     def __init__(\n         self,\n-        name=\"RetrieveChatAgent\",\n-        human_input_mode: str | None = \"ALWAYS\",\n-        is_termination_msg: Callable[[Dict], bool] | None = None,\n-        retrieve_config: Dict | None = None,\n+        name=\"RetrieveChatAgent\",  # default set to RetrieveChatAgent\n+        human_input_mode: Optional[str] = \"ALWAYS\",\n+        is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n+        retrieve_config: Optional[Dict] = None,  # config for the retrieve agent\n         **kwargs,\n     ):\n         \"\"\"",
          "path": "autogen/agentchat/contrib/qdrant_retrieve_user_proxy_agent.py"
        },
        {
          "patch": "@@ -361,7 +361,7 @@ def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str =\n         \"\"\"\n         if not self._collection or self._get_or_create:\n             print(\"Trying to create collection.\")\n-            create_vector_db_from_dir(\n+            self._client = create_vector_db_from_dir(\n                 dir_path=self._docs_path,\n                 max_tokens=self._chunk_token_size,\n                 client=self._client,",
          "path": "autogen/agentchat/contrib/retrieve_user_proxy_agent.py"
        },
        {
          "patch": "@@ -5,7 +5,11 @@\n import glob\n import tiktoken\n import chromadb\n-from chromadb.api import API\n+\n+if chromadb.__version__ < \"0.4.15\":\n+    from chromadb.api import API\n+else:\n+    from chromadb.api import ClientAPI as API\n from chromadb.api.types import QueryResult\n import chromadb.utils.embedding_functions as ef\n import logging\n@@ -287,7 +291,7 @@ def create_vector_db_from_dir(\n     embedding_model: str = \"all-MiniLM-L6-v2\",\n     embedding_function: Callable = None,\n     custom_text_split_function: Callable = None,\n-):\n+) -> API:\n     \"\"\"Create a vector db from all the files in a given directory, the directory can also be a single file or a url to\n         a single file. We support chromadb compatible APIs to create the vector db, this function is not required if\n         you prepared your own vector db.\n@@ -307,6 +311,9 @@ def create_vector_db_from_dir(\n         embedding_function (Optional, Callable): the embedding function to use. Default is None, SentenceTransformer with\n             the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding\n             functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n+\n+    Returns:\n+        API: the chromadb client.\n     \"\"\"\n     if client is None:\n         client = chromadb.PersistentClient(path=db_path)\n@@ -344,6 +351,7 @@ def create_vector_db_from_dir(\n             )\n     except ValueError as e:\n         logger.warning(f\"{e}\")\n+    return client\n \n \n def query_vector_db(",
          "path": "autogen/retrieve_utils.py"
        },
        {
          "patch": "@@ -53,6 +53,7 @@\n             \"sympy\",\n             \"tiktoken\",\n             \"wolframalpha\",\n+            \"qdrant_client[fastembed]\",\n         ],\n         \"blendsearch\": [\"flaml[blendsearch]\"],\n         \"mathchat\": [\"sympy\", \"pydantic==1.10.9\", \"wolframalpha\"],",
          "path": "setup.py"
        }
      ],
      "pr_number": "435",
      "stage_id": "stage-3",
      "test_query": "You need to modify the files in this repository according to the requirements below. During the analysis process, do not directly read the complete file; try to read the first 10 lines using head or use grep instead. You are NOT allowed to directly run scripts, such as python, pip install, dockerfile, etc.\n\nThe repository requires enhancements to improve documentation clarity and extend vector storage capabilities. First, update the RetrieveChat notebook and installation documentation to eliminate confusion about package relationships and provide clearer environment setup guidance for both venv and conda users. The documentation should explicitly clarify that this project is not a subpackage of FLAML and streamline the onboarding process for new users.\n\nNext, implement support for Qdrant as an additional vector store option by creating a new agent class that extends the existing retrieve user proxy agent functionality. This new integration should be placed in the contrib directory alongside other retrieve agents. The implementation should leverage the existing retrieval utilities while adding Qdrant-specific connection and query logic.\n\nAdditionally, resolve compatibility issues with the ChromaDB integration by adapting to recent class renames in version 0.4.15. The fix should ensure proper collection creation when no client is explicitly provided.\n\nFinally, update the CI workflow configuration and package dependencies to include Qdrant testing requirements, ensuring that the new functionality can be properly validated in automated testing pipelines. The example documentation should also be updated to reflect these new capabilities and provide clearer guidance for users."
    }
  ],
  "validation_messages": []
}