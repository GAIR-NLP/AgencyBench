{
  "metadata": {
    "annotation_id": "auto-ann-microsoft/autogen:autogenerated-3605:5149627f",
    "chain_id": "microsoft/autogen:autogenerated-3605:5149627f",
    "generated_at": "2025-11-14T02:23:22.969880+00:00",
    "operator_notes": null,
    "sanitized_repository": "microsoft_autogen",
    "source_repository": "microsoft/autogen",
    "start_pr_id": "microsoft/autogen#3605",
    "template_checksum": "58cb965ba38dcf2e4a48441fa821ead396118a1066a3386030772d43a1488f37",
    "template_id": "query/query_template",
    "template_version": "2024-11-05",
    "topic": "autogenerated-3605",
    "validation_status": null
  },
  "stages": [
    {
      "base_commit_sha": "6ebf49a9b840e4663495816179322547274a83b0",
      "carry_forward_policy": "reapply_patch",
      "dependencies": [],
      "ground_truth_patch": "@@ -0,0 +1,18 @@\n+name: Label issues with needs-triage\n+on:\n+  issues:\n+    types:\n+      - reopened\n+      - opened\n+jobs:\n+  label_issues:\n+    runs-on: ubuntu-latest\n+    permissions:\n+      issues: write\n+    steps:\n+      - run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GH_REPO: ${{ github.repository }}\n+          NUMBER: ${{ github.event.issue.number }}\n+          LABELS: needs-triage\n",
      "head_commit_sha": "d4a09939d0b065a54658c5c09efdaf6e68c9eb1f",
      "name": "Stage 1",
      "patches": [
        {
          "patch": "@@ -0,0 +1,18 @@\n+name: Label issues with needs-triage\n+on:\n+  issues:\n+    types:\n+      - reopened\n+      - opened\n+jobs:\n+  label_issues:\n+    runs-on: ubuntu-latest\n+    permissions:\n+      issues: write\n+    steps:\n+      - run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GH_REPO: ${{ github.repository }}\n+          NUMBER: ${{ github.event.issue.number }}\n+          LABELS: needs-triage",
          "path": ".github/workflows/issue-needs-triage.yml"
        }
      ],
      "pr_number": "3605",
      "stage_id": "stage-1",
      "test_query": "You need to modify the files in this repository according to the requirements below. During the analysis process, do not directly read the complete file; try to read the first 10 lines using head or use grep instead. You are NOT allowed to directly run scripts, such as python, pip install, dockerfile, etc.\n\nThe repository requires implementation of a termination mechanism for agent chat teams to prevent infinite conversations. The primary focus should be on the agentchat teams module located in the python/packages/autogen-agentchat/src/autogen_agentchat/teams/ directory. You need to create a system where teams can evaluate whether to continue or stop their conversation based on specific conditions. The termination logic should be integrated into the team's execution flow, allowing the run method to check these conditions during message processing. When a termination condition is satisfied, the team should gracefully halt its operation. The implementation should support different types of termination strategies and be extensible for future condition types. Additionally, ensure the group chat implementations in the group_chat subdirectory properly utilize this termination mechanism, and update the relevant test files to validate the new functionality."
    },
    {
      "base_commit_sha": "9b7909489193e45ea691598b37bb52590d018ef6",
      "carry_forward_policy": "reapply_patch",
      "dependencies": [
        "stage-1"
      ],
      "ground_truth_patch": "@@ -1,57 +0,0 @@\n-### Description\n-<!-- A clear and concise description of the issue or feature request. -->\n-\n-### Environment\n-- AutoGen version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-- Python version: <!-- Specify the Python version (e.g., 3.8) -->\n-- Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-\n-### Steps to Reproduce (for bugs)\n-<!-- Provide detailed steps to reproduce the issue. Include code snippets, configuration files, or any other relevant information. -->\n-\n-1. Step 1\n-2. Step 2\n-3. ...\n-\n-### Expected Behavior\n-<!-- Describe what you expected to happen. -->\n-\n-### Actual Behavior\n-<!-- Describe what actually happened. Include any error messages, stack traces, or unexpected behavior. -->\n-\n-### Screenshots / Logs (if applicable)\n-<!-- If relevant, include screenshots or logs that help illustrate the issue. -->\n-\n-### Additional Information\n-<!-- Include any additional information that might be helpful, such as specific configurations, data samples, or context about the environment. -->\n-\n-### Possible Solution (if you have one)\n-<!-- If you have suggestions on how to address the issue, provide them here. -->\n-\n-### Is this a Bug or Feature Request?\n-<!-- Choose one: Bug | Feature Request -->\n-\n-### Priority\n-<!-- Choose one: High | Medium | Low -->\n-\n-### Difficulty\n-<!-- Choose one: Easy | Moderate | Hard -->\n-\n-### Any related issues?\n-<!-- If this is related to another issue, reference it here. -->\n-\n-### Any relevant discussions?\n-<!-- If there are any discussions or forum threads related to this issue, provide links. -->\n-\n-### Checklist\n-<!-- Please check the items that you have completed -->\n-- [ ] I have searched for similar issues and didn't find any duplicates.\n-- [ ] I have provided a clear and concise description of the issue.\n-- [ ] I have included the necessary environment details.\n-- [ ] I have outlined the steps to reproduce the issue.\n-- [ ] I have included any relevant logs or screenshots.\n-- [ ] I have indicated whether this is a bug or a feature request.\n-- [ ] I have set the priority and difficulty levels.\n-\n-### Additional Comments\n-<!-- Any additional comments or context that you think would be helpful. -->\n@@ -1,53 +1,55 @@\n name: Bug Report\n-description: File a bug report\n-title: \"[Bug]: \"\n+description: Report a bug\n labels: [\"bug\"]\n \n body:\n   - type: textarea\n-    id: description\n     attributes:\n-      label: Describe the bug\n-      description: A clear and concise description of what the bug is.\n-      placeholder: What went wrong?\n+      label: What happened?\n+      description: Please provide as much information as possible, this helps us address the issue.\n+    validations:\n+      required: true\n   - type: textarea\n-    id: reproduce\n     attributes:\n-      label: Steps to reproduce\n-      description: |\n-        Steps to reproduce the behavior:\n-\n-        1. Step 1\n-        2. Step 2\n-        3. ...\n-        4. See error\n-      placeholder: How can we replicate the issue?\n+      label: What did you expect to happen?\n+    validations:\n+      required: true\n   - type: textarea\n-    id: modelused\n     attributes:\n-      label: Model Used\n-      description: A description of the model that was used when the error was encountered\n+      label: How can we reproduce it (as minimally and precisely as possible)?\n+      description: Please provide steps to reproduce. Provide code that can be run if possible.\n+    validations:\n+      required: true\n+  - type: input\n+    attributes:\n+      label: AutoGen version\n+      description: What version or commit of the library was used\n+    validations:\n+      required: true\n+  - type: dropdown\n+    attributes:\n+      label: Which package was this bug in\n+      options:\n+        - Core\n+        - AgentChat\n+        - Extensions\n+        - AutoGen Studio\n+        - Magentic One\n+        - AutoGen Bench\n+        - Other\n+    validations:\n+      required: true\n+  - type: input\n+    attributes:\n+      label: Model used\n+      description: If a model was used, please describe it here, indicating whether it is a local model or a cloud-hosted model\n       placeholder: gpt-4, mistral-7B etc\n-  - type: textarea\n-    id: expected_behavior\n+  - type: input\n     attributes:\n-      label: Expected Behavior\n-      description: A clear and concise description of what you expected to happen.\n-      placeholder: What should have happened?\n-  - type: textarea\n-    id: screenshots\n+      label: Python version\n+  - type: input\n     attributes:\n-      label: Screenshots and logs\n-      description: If applicable, add screenshots and logs to help explain your problem.\n-      placeholder: Add screenshots here\n+      label: Operating system\n   - type: textarea\n-    id: additional_information\n-    attributes:\n-      label: Additional Information\n-      description: |\n-        - AutoGen Version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-        - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-        - Python Version: <!-- Specify the Python version (e.g., 3.8) -->\n-        - Related Issues: <!-- Link to any related issues here (e.g., #1) -->\n-        - Any other relevant information.\n-      placeholder: Any additional details\n+    attributes:\n+      label: Any additional info you think would be helpful for fixing this bug\n@@ -1 +1,5 @@\n blank_issues_enabled: true\n+contact_links:\n+  - name: Questions or general help \ud83d\udcac\n+    url: https://github.com/microsoft/autogen/discussions\n+    about: Please ask and answer questions here.\n@@ -1,26 +1,18 @@\n name: Feature Request\n-description: File a feature request\n+description: Request a new feature or enhancement\n labels: [\"enhancement\"]\n-title: \"[Feature Request]: \"\n \n body:\n   - type: textarea\n-    id: problem_description\n     attributes:\n-      label: Is your feature request related to a problem? Please describe.\n-      description: A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n-      placeholder: What problem are you trying to solve?\n+      label: What feature would you like to be added?\n+      description: Please describe the desired feature. Be descriptive, provide examples and if possible, provide a proposed solution.\n+    validations:\n+      required: true\n \n   - type: textarea\n-    id: solution_description\n     attributes:\n-      label: Describe the solution you'd like\n-      description: A clear and concise description of what you want to happen.\n-      placeholder: How do you envision the solution?\n-\n-  - type: textarea\n-    id: additional_context\n-    attributes:\n-      label: Additional context\n-      description: Add any other context or screenshots about the feature request here.\n-      placeholder: Any additional information\n+      label: Why is this needed?\n+      description: Why is it important that this feature is implemented? What problem or need does it solve?\n+    validations:\n+      required: true\n@@ -1,41 +0,0 @@\n-name: General Issue\n-description: File a general issue\n-title: \"[Issue]: \"\n-labels: []\n-\n-body:\n-  - type: textarea\n-    id: description\n-    attributes:\n-      label: Describe the issue\n-      description: A clear and concise description of what the issue is.\n-      placeholder: What went wrong?\n-  - type: textarea\n-    id: reproduce\n-    attributes:\n-      label: Steps to reproduce\n-      description: |\n-        Steps to reproduce the behavior:\n-\n-        1. Step 1\n-        2. Step 2\n-        3. ...\n-        4. See error\n-      placeholder: How can we replicate the issue?\n-  - type: textarea\n-    id: screenshots\n-    attributes:\n-      label: Screenshots and logs\n-      description: If applicable, add screenshots and logs to help explain your problem.\n-      placeholder: Add screenshots here\n-  - type: textarea\n-    id: additional_information\n-    attributes:\n-      label: Additional Information\n-      description: |\n-        - AutoGen Version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-        - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-        - Python Version: <!-- Specify the Python version (e.g., 3.8) -->\n-        - Related Issues: <!-- Link to any related issues here (e.g., #1) -->\n-        - Any other relevant information.\n-      placeholder: Any additional details\n@@ -0,0 +1,18 @@\n+name: Label issues with needs-triage\n+on:\n+  issues:\n+    types:\n+      - reopened\n+      - opened\n+jobs:\n+  label_issues:\n+    runs-on: ubuntu-latest\n+    permissions:\n+      issues: write\n+    steps:\n+      - run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GH_REPO: ${{ github.repository }}\n+          NUMBER: ${{ github.event.issue.number }}\n+          LABELS: needs-triage\n@@ -71,7 +71,7 @@ Today, we are delighted to share our progress and invite everyone to collaborate\n with us and provide feedback to evolve AutoGen and help shape the future of\n multi-agent systems.\n \n-As the first step, we are opening a [pull request](#) into the main branch with the\n+As the first step, we are opening a [pull request](https://github.com/microsoft/autogen/pull/3600) into the main branch with the\n current state of development of 0.4. After approximately a week, we plan to\n merge this into main and continue development. There's still a lot left to do\n before 0.4 is ready for release though, so keep in mind this is a work in\n@@ -82,12 +82,12 @@ Starting in AutoGen 0.4, the project will have three main libraries:\n - **Core** - the building blocks for an event-driven agentic system.\n - **AgentChat** - a task-driven, high-level API built with core, including group\n   chat, code execution, pre-built agents, and more. This is the most similar API\n-  to AutoGen 0.2 and will be the easiest API to migrate to.\n+  to AutoGen [0.2](https://github.com/microsoft/autogen/tree/0.2) and will be the easiest API to migrate to.\n - **Extensions** - implementations of core interfaces and third-party integrations\n   (e.g., Azure code executor and OpenAI model client).\n \n-AutoGen 0.2 is still available, developed and maintained out of the [0.2 branch](https://github.com/microsoft/autogen/tree/0.2).\n-For everyone looking for a stable version, we recommend continuing to use 0.2\n+AutoGen [0.2](https://github.com/microsoft/autogen/tree/0.2) is still available, developed and maintained out of the [0.2 branch](https://github.com/microsoft/autogen/tree/0.2).\n+For everyone looking for a stable version, we recommend continuing to use [0.2](https://github.com/microsoft/autogen/tree/0.2)\n for the time being. It can be installed using:\n \n ```sh\n@@ -728,7 +728,7 @@\n     \"If we were to use the `round_robin` strategy, this list would specify the order\\n\",\n     \"of the agents to be selected.\\n\",\n     \"We also initialize the group chat with an empty message list and a maximum\\n\",\n-    \"round of 6, which means there will be at most 6 iteratiosn of selecting speaker,\\n\",\n+    \"round of 6, which means there will be at most 6 iterations of selecting a speaker,\\n\",\n     \"agent speaks and broadcasting message.\"\n    ]\n   },\n",
      "head_commit_sha": "21a5a13965834d18682c5e174c68ad16ebff8806",
      "name": "Stage 2",
      "patches": [
        {
          "patch": "@@ -1,57 +0,0 @@\n-### Description\n-<!-- A clear and concise description of the issue or feature request. -->\n-\n-### Environment\n-- AutoGen version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-- Python version: <!-- Specify the Python version (e.g., 3.8) -->\n-- Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-\n-### Steps to Reproduce (for bugs)\n-<!-- Provide detailed steps to reproduce the issue. Include code snippets, configuration files, or any other relevant information. -->\n-\n-1. Step 1\n-2. Step 2\n-3. ...\n-\n-### Expected Behavior\n-<!-- Describe what you expected to happen. -->\n-\n-### Actual Behavior\n-<!-- Describe what actually happened. Include any error messages, stack traces, or unexpected behavior. -->\n-\n-### Screenshots / Logs (if applicable)\n-<!-- If relevant, include screenshots or logs that help illustrate the issue. -->\n-\n-### Additional Information\n-<!-- Include any additional information that might be helpful, such as specific configurations, data samples, or context about the environment. -->\n-\n-### Possible Solution (if you have one)\n-<!-- If you have suggestions on how to address the issue, provide them here. -->\n-\n-### Is this a Bug or Feature Request?\n-<!-- Choose one: Bug | Feature Request -->\n-\n-### Priority\n-<!-- Choose one: High | Medium | Low -->\n-\n-### Difficulty\n-<!-- Choose one: Easy | Moderate | Hard -->\n-\n-### Any related issues?\n-<!-- If this is related to another issue, reference it here. -->\n-\n-### Any relevant discussions?\n-<!-- If there are any discussions or forum threads related to this issue, provide links. -->\n-\n-### Checklist\n-<!-- Please check the items that you have completed -->\n-- [ ] I have searched for similar issues and didn't find any duplicates.\n-- [ ] I have provided a clear and concise description of the issue.\n-- [ ] I have included the necessary environment details.\n-- [ ] I have outlined the steps to reproduce the issue.\n-- [ ] I have included any relevant logs or screenshots.\n-- [ ] I have indicated whether this is a bug or a feature request.\n-- [ ] I have set the priority and difficulty levels.\n-\n-### Additional Comments\n-<!-- Any additional comments or context that you think would be helpful. -->",
          "path": ".github/ISSUE_TEMPLATE.md"
        },
        {
          "patch": "@@ -1,53 +1,55 @@\n name: Bug Report\n-description: File a bug report\n-title: \"[Bug]: \"\n+description: Report a bug\n labels: [\"bug\"]\n \n body:\n   - type: textarea\n-    id: description\n     attributes:\n-      label: Describe the bug\n-      description: A clear and concise description of what the bug is.\n-      placeholder: What went wrong?\n+      label: What happened?\n+      description: Please provide as much information as possible, this helps us address the issue.\n+    validations:\n+      required: true\n   - type: textarea\n-    id: reproduce\n     attributes:\n-      label: Steps to reproduce\n-      description: |\n-        Steps to reproduce the behavior:\n-\n-        1. Step 1\n-        2. Step 2\n-        3. ...\n-        4. See error\n-      placeholder: How can we replicate the issue?\n+      label: What did you expect to happen?\n+    validations:\n+      required: true\n   - type: textarea\n-    id: modelused\n     attributes:\n-      label: Model Used\n-      description: A description of the model that was used when the error was encountered\n+      label: How can we reproduce it (as minimally and precisely as possible)?\n+      description: Please provide steps to reproduce. Provide code that can be run if possible.\n+    validations:\n+      required: true\n+  - type: input\n+    attributes:\n+      label: AutoGen version\n+      description: What version or commit of the library was used\n+    validations:\n+      required: true\n+  - type: dropdown\n+    attributes:\n+      label: Which package was this bug in\n+      options:\n+        - Core\n+        - AgentChat\n+        - Extensions\n+        - AutoGen Studio\n+        - Magentic One\n+        - AutoGen Bench\n+        - Other\n+    validations:\n+      required: true\n+  - type: input\n+    attributes:\n+      label: Model used\n+      description: If a model was used, please describe it here, indicating whether it is a local model or a cloud-hosted model\n       placeholder: gpt-4, mistral-7B etc\n-  - type: textarea\n-    id: expected_behavior\n+  - type: input\n     attributes:\n-      label: Expected Behavior\n-      description: A clear and concise description of what you expected to happen.\n-      placeholder: What should have happened?\n-  - type: textarea\n-    id: screenshots\n+      label: Python version\n+  - type: input\n     attributes:\n-      label: Screenshots and logs\n-      description: If applicable, add screenshots and logs to help explain your problem.\n-      placeholder: Add screenshots here\n+      label: Operating system\n   - type: textarea\n-    id: additional_information\n-    attributes:\n-      label: Additional Information\n-      description: |\n-        - AutoGen Version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-        - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-        - Python Version: <!-- Specify the Python version (e.g., 3.8) -->\n-        - Related Issues: <!-- Link to any related issues here (e.g., #1) -->\n-        - Any other relevant information.\n-      placeholder: Any additional details\n+    attributes:\n+      label: Any additional info you think would be helpful for fixing this bug",
          "path": ".github/ISSUE_TEMPLATE/bug_report.yml"
        },
        {
          "patch": "@@ -1 +1,5 @@\n blank_issues_enabled: true\n+contact_links:\n+  - name: Questions or general help \ud83d\udcac\n+    url: https://github.com/microsoft/autogen/discussions\n+    about: Please ask and answer questions here.",
          "path": ".github/ISSUE_TEMPLATE/config.yml"
        },
        {
          "patch": "@@ -1,26 +1,18 @@\n name: Feature Request\n-description: File a feature request\n+description: Request a new feature or enhancement\n labels: [\"enhancement\"]\n-title: \"[Feature Request]: \"\n \n body:\n   - type: textarea\n-    id: problem_description\n     attributes:\n-      label: Is your feature request related to a problem? Please describe.\n-      description: A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n-      placeholder: What problem are you trying to solve?\n+      label: What feature would you like to be added?\n+      description: Please describe the desired feature. Be descriptive, provide examples and if possible, provide a proposed solution.\n+    validations:\n+      required: true\n \n   - type: textarea\n-    id: solution_description\n     attributes:\n-      label: Describe the solution you'd like\n-      description: A clear and concise description of what you want to happen.\n-      placeholder: How do you envision the solution?\n-\n-  - type: textarea\n-    id: additional_context\n-    attributes:\n-      label: Additional context\n-      description: Add any other context or screenshots about the feature request here.\n-      placeholder: Any additional information\n+      label: Why is this needed?\n+      description: Why is it important that this feature is implemented? What problem or need does it solve?\n+    validations:\n+      required: true",
          "path": ".github/ISSUE_TEMPLATE/feature_request.yml"
        },
        {
          "patch": "@@ -1,41 +0,0 @@\n-name: General Issue\n-description: File a general issue\n-title: \"[Issue]: \"\n-labels: []\n-\n-body:\n-  - type: textarea\n-    id: description\n-    attributes:\n-      label: Describe the issue\n-      description: A clear and concise description of what the issue is.\n-      placeholder: What went wrong?\n-  - type: textarea\n-    id: reproduce\n-    attributes:\n-      label: Steps to reproduce\n-      description: |\n-        Steps to reproduce the behavior:\n-\n-        1. Step 1\n-        2. Step 2\n-        3. ...\n-        4. See error\n-      placeholder: How can we replicate the issue?\n-  - type: textarea\n-    id: screenshots\n-    attributes:\n-      label: Screenshots and logs\n-      description: If applicable, add screenshots and logs to help explain your problem.\n-      placeholder: Add screenshots here\n-  - type: textarea\n-    id: additional_information\n-    attributes:\n-      label: Additional Information\n-      description: |\n-        - AutoGen Version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-        - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-        - Python Version: <!-- Specify the Python version (e.g., 3.8) -->\n-        - Related Issues: <!-- Link to any related issues here (e.g., #1) -->\n-        - Any other relevant information.\n-      placeholder: Any additional details",
          "path": ".github/ISSUE_TEMPLATE/general_issue.yml"
        },
        {
          "patch": "@@ -0,0 +1,18 @@\n+name: Label issues with needs-triage\n+on:\n+  issues:\n+    types:\n+      - reopened\n+      - opened\n+jobs:\n+  label_issues:\n+    runs-on: ubuntu-latest\n+    permissions:\n+      issues: write\n+    steps:\n+      - run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GH_REPO: ${{ github.repository }}\n+          NUMBER: ${{ github.event.issue.number }}\n+          LABELS: needs-triage",
          "path": ".github/workflows/issue-needs-triage.yml"
        },
        {
          "patch": "@@ -71,7 +71,7 @@ Today, we are delighted to share our progress and invite everyone to collaborate\n with us and provide feedback to evolve AutoGen and help shape the future of\n multi-agent systems.\n \n-As the first step, we are opening a [pull request](#) into the main branch with the\n+As the first step, we are opening a [pull request](https://github.com/microsoft/autogen/pull/3600) into the main branch with the\n current state of development of 0.4. After approximately a week, we plan to\n merge this into main and continue development. There's still a lot left to do\n before 0.4 is ready for release though, so keep in mind this is a work in\n@@ -82,12 +82,12 @@ Starting in AutoGen 0.4, the project will have three main libraries:\n - **Core** - the building blocks for an event-driven agentic system.\n - **AgentChat** - a task-driven, high-level API built with core, including group\n   chat, code execution, pre-built agents, and more. This is the most similar API\n-  to AutoGen 0.2 and will be the easiest API to migrate to.\n+  to AutoGen [0.2](https://github.com/microsoft/autogen/tree/0.2) and will be the easiest API to migrate to.\n - **Extensions** - implementations of core interfaces and third-party integrations\n   (e.g., Azure code executor and OpenAI model client).\n \n-AutoGen 0.2 is still available, developed and maintained out of the [0.2 branch](https://github.com/microsoft/autogen/tree/0.2).\n-For everyone looking for a stable version, we recommend continuing to use 0.2\n+AutoGen [0.2](https://github.com/microsoft/autogen/tree/0.2) is still available, developed and maintained out of the [0.2 branch](https://github.com/microsoft/autogen/tree/0.2).\n+For everyone looking for a stable version, we recommend continuing to use [0.2](https://github.com/microsoft/autogen/tree/0.2)\n for the time being. It can be installed using:\n \n ```sh",
          "path": "website/blog/2024-10-02-new-autogen-architecture-preview/index.mdx"
        },
        {
          "patch": "@@ -728,7 +728,7 @@\n     \"If we were to use the `round_robin` strategy, this list would specify the order\\n\",\n     \"of the agents to be selected.\\n\",\n     \"We also initialize the group chat with an empty message list and a maximum\\n\",\n-    \"round of 6, which means there will be at most 6 iteratiosn of selecting speaker,\\n\",\n+    \"round of 6, which means there will be at most 6 iterations of selecting a speaker,\\n\",\n     \"agent speaks and broadcasting message.\"\n    ]\n   },",
          "path": "website/docs/tutorial/conversation-patterns.ipynb"
        }
      ],
      "pr_number": "3650",
      "stage_id": "stage-2",
      "test_query": "You need to modify the files in this repository according to the requirements below. During the analysis process, do not directly read the complete file; try to read the first 10 lines using head or use grep instead. You are NOT allowed to directly run scripts, such as python, pip install, dockerfile, etc.\n\nThe repository requires implementation of a termination mechanism for agent chat teams to prevent infinite conversations. The primary focus should be on the agentchat teams module located in the python/packages/autogen-agentchat/src/autogen_agentchat/teams/ directory. You need to create a system where teams can evaluate whether to continue or stop their conversation based on specific conditions. The termination logic should be integrated into the team's execution flow, allowing the run method to check these conditions during message processing. When a termination condition is satisfied, the team should gracefully halt its operation. The implementation should support different types of termination strategies and be extensible for future condition types. Additionally, ensure the group chat implementations in the group_chat subdirectory properly utilize this termination mechanism, and update the relevant test files to validate the new functionality."
    },
    {
      "base_commit_sha": "333c95155cf86fa1a46e7ce92da182782a99279f",
      "carry_forward_policy": "reapply_patch",
      "dependencies": [
        "stage-2"
      ],
      "ground_truth_patch": "@@ -1,57 +0,0 @@\n-### Description\n-<!-- A clear and concise description of the issue or feature request. -->\n-\n-### Environment\n-- AutoGen version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-- Python version: <!-- Specify the Python version (e.g., 3.8) -->\n-- Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-\n-### Steps to Reproduce (for bugs)\n-<!-- Provide detailed steps to reproduce the issue. Include code snippets, configuration files, or any other relevant information. -->\n-\n-1. Step 1\n-2. Step 2\n-3. ...\n-\n-### Expected Behavior\n-<!-- Describe what you expected to happen. -->\n-\n-### Actual Behavior\n-<!-- Describe what actually happened. Include any error messages, stack traces, or unexpected behavior. -->\n-\n-### Screenshots / Logs (if applicable)\n-<!-- If relevant, include screenshots or logs that help illustrate the issue. -->\n-\n-### Additional Information\n-<!-- Include any additional information that might be helpful, such as specific configurations, data samples, or context about the environment. -->\n-\n-### Possible Solution (if you have one)\n-<!-- If you have suggestions on how to address the issue, provide them here. -->\n-\n-### Is this a Bug or Feature Request?\n-<!-- Choose one: Bug | Feature Request -->\n-\n-### Priority\n-<!-- Choose one: High | Medium | Low -->\n-\n-### Difficulty\n-<!-- Choose one: Easy | Moderate | Hard -->\n-\n-### Any related issues?\n-<!-- If this is related to another issue, reference it here. -->\n-\n-### Any relevant discussions?\n-<!-- If there are any discussions or forum threads related to this issue, provide links. -->\n-\n-### Checklist\n-<!-- Please check the items that you have completed -->\n-- [ ] I have searched for similar issues and didn't find any duplicates.\n-- [ ] I have provided a clear and concise description of the issue.\n-- [ ] I have included the necessary environment details.\n-- [ ] I have outlined the steps to reproduce the issue.\n-- [ ] I have included any relevant logs or screenshots.\n-- [ ] I have indicated whether this is a bug or a feature request.\n-- [ ] I have set the priority and difficulty levels.\n-\n-### Additional Comments\n-<!-- Any additional comments or context that you think would be helpful. -->\n@@ -1,53 +1,55 @@\n name: Bug Report\n-description: File a bug report\n-title: \"[Bug]: \"\n+description: Report a bug\n labels: [\"bug\"]\n \n body:\n   - type: textarea\n-    id: description\n     attributes:\n-      label: Describe the bug\n-      description: A clear and concise description of what the bug is.\n-      placeholder: What went wrong?\n+      label: What happened?\n+      description: Please provide as much information as possible, this helps us address the issue.\n+    validations:\n+      required: true\n   - type: textarea\n-    id: reproduce\n     attributes:\n-      label: Steps to reproduce\n-      description: |\n-        Steps to reproduce the behavior:\n-\n-        1. Step 1\n-        2. Step 2\n-        3. ...\n-        4. See error\n-      placeholder: How can we replicate the issue?\n+      label: What did you expect to happen?\n+    validations:\n+      required: true\n   - type: textarea\n-    id: modelused\n     attributes:\n-      label: Model Used\n-      description: A description of the model that was used when the error was encountered\n+      label: How can we reproduce it (as minimally and precisely as possible)?\n+      description: Please provide steps to reproduce. Provide code that can be run if possible.\n+    validations:\n+      required: true\n+  - type: input\n+    attributes:\n+      label: AutoGen version\n+      description: What version or commit of the library was used\n+    validations:\n+      required: true\n+  - type: dropdown\n+    attributes:\n+      label: Which package was this bug in\n+      options:\n+        - Core\n+        - AgentChat\n+        - Extensions\n+        - AutoGen Studio\n+        - Magentic One\n+        - AutoGen Bench\n+        - Other\n+    validations:\n+      required: true\n+  - type: input\n+    attributes:\n+      label: Model used\n+      description: If a model was used, please describe it here, indicating whether it is a local model or a cloud-hosted model\n       placeholder: gpt-4, mistral-7B etc\n-  - type: textarea\n-    id: expected_behavior\n+  - type: input\n     attributes:\n-      label: Expected Behavior\n-      description: A clear and concise description of what you expected to happen.\n-      placeholder: What should have happened?\n-  - type: textarea\n-    id: screenshots\n+      label: Python version\n+  - type: input\n     attributes:\n-      label: Screenshots and logs\n-      description: If applicable, add screenshots and logs to help explain your problem.\n-      placeholder: Add screenshots here\n+      label: Operating system\n   - type: textarea\n-    id: additional_information\n-    attributes:\n-      label: Additional Information\n-      description: |\n-        - AutoGen Version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-        - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-        - Python Version: <!-- Specify the Python version (e.g., 3.8) -->\n-        - Related Issues: <!-- Link to any related issues here (e.g., #1) -->\n-        - Any other relevant information.\n-      placeholder: Any additional details\n+    attributes:\n+      label: Any additional info you think would be helpful for fixing this bug\n@@ -1 +1,5 @@\n blank_issues_enabled: true\n+contact_links:\n+  - name: Questions or general help \ud83d\udcac\n+    url: https://github.com/microsoft/autogen/discussions\n+    about: Please ask and answer questions here.\n@@ -1,26 +1,18 @@\n name: Feature Request\n-description: File a feature request\n+description: Request a new feature or enhancement\n labels: [\"enhancement\"]\n-title: \"[Feature Request]: \"\n \n body:\n   - type: textarea\n-    id: problem_description\n     attributes:\n-      label: Is your feature request related to a problem? Please describe.\n-      description: A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n-      placeholder: What problem are you trying to solve?\n+      label: What feature would you like to be added?\n+      description: Please describe the desired feature. Be descriptive, provide examples and if possible, provide a proposed solution.\n+    validations:\n+      required: true\n \n   - type: textarea\n-    id: solution_description\n     attributes:\n-      label: Describe the solution you'd like\n-      description: A clear and concise description of what you want to happen.\n-      placeholder: How do you envision the solution?\n-\n-  - type: textarea\n-    id: additional_context\n-    attributes:\n-      label: Additional context\n-      description: Add any other context or screenshots about the feature request here.\n-      placeholder: Any additional information\n+      label: Why is this needed?\n+      description: Why is it important that this feature is implemented? What problem or need does it solve?\n+    validations:\n+      required: true\n@@ -1,41 +0,0 @@\n-name: General Issue\n-description: File a general issue\n-title: \"[Issue]: \"\n-labels: []\n-\n-body:\n-  - type: textarea\n-    id: description\n-    attributes:\n-      label: Describe the issue\n-      description: A clear and concise description of what the issue is.\n-      placeholder: What went wrong?\n-  - type: textarea\n-    id: reproduce\n-    attributes:\n-      label: Steps to reproduce\n-      description: |\n-        Steps to reproduce the behavior:\n-\n-        1. Step 1\n-        2. Step 2\n-        3. ...\n-        4. See error\n-      placeholder: How can we replicate the issue?\n-  - type: textarea\n-    id: screenshots\n-    attributes:\n-      label: Screenshots and logs\n-      description: If applicable, add screenshots and logs to help explain your problem.\n-      placeholder: Add screenshots here\n-  - type: textarea\n-    id: additional_information\n-    attributes:\n-      label: Additional Information\n-      description: |\n-        - AutoGen Version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-        - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-        - Python Version: <!-- Specify the Python version (e.g., 3.8) -->\n-        - Related Issues: <!-- Link to any related issues here (e.g., #1) -->\n-        - Any other relevant information.\n-      placeholder: Any additional details\n@@ -0,0 +1,18 @@\n+name: Label issues with needs-triage\n+on:\n+  issues:\n+    types:\n+      - reopened\n+      - opened\n+jobs:\n+  label_issues:\n+    runs-on: ubuntu-latest\n+    permissions:\n+      issues: write\n+    steps:\n+      - run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GH_REPO: ${{ github.repository }}\n+          NUMBER: ${{ github.event.issue.number }}\n+          LABELS: needs-triage\n@@ -107,7 +107,7 @@ and running on your machine.\n \n ```python\n from autogen_agentchat.agents import CodeExecutorAgent, CodingAssistantAgent\n-from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\n+from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n from autogen_core.components.code_executor import DockerCommandLineCodeExecutor\n from autogen_core.components.models import OpenAIChatCompletionClient\n \n@@ -118,9 +118,9 @@ async with DockerCommandLineCodeExecutor(work_dir=\"coding\") as code_executor:\n     )\n     group_chat = RoundRobinGroupChat([coding_assistant_agent, code_executor_agent])\n     result = await group_chat.run(\n-        task=\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\"\n+        task=\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\",\n+        termination_condition=StopMessageTermination(),\n     )\n-    print(result)\n ```\n \n ### C#\n@@ -36,7 +36,7 @@ def __init__(\n         registered_tools: List[Tool],\n         *,\n         description: str = \"An agent that provides assistance with ability to use tools.\",\n-        system_message: str = \"You are a helpful AI assistant. Solve tasks using your tools.\",\n+        system_message: str = \"You are a helpful AI assistant. Solve tasks using your tools. Reply with 'TERMINATE' when the task has been completed.\",\n     ):\n         super().__init__(name=name, description=description, registered_tools=registered_tools)\n         self._model_client = model_client\n@@ -1,8 +1,17 @@\n from ._logging import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME, ConsoleLogHandler, FileLogHandler\n+from ._termination import MaxMessageTermination, StopMessageTermination, TerminationCondition, TextMentionTermination\n+from .group_chat._round_robin_group_chat import RoundRobinGroupChat\n+from .group_chat._selector_group_chat import SelectorGroupChat\n \n __all__ = [\n     \"TRACE_LOGGER_NAME\",\n     \"EVENT_LOGGER_NAME\",\n     \"ConsoleLogHandler\",\n     \"FileLogHandler\",\n+    \"TerminationCondition\",\n+    \"MaxMessageTermination\",\n+    \"TextMentionTermination\",\n+    \"StopMessageTermination\",\n+    \"RoundRobinGroupChat\",\n+    \"SelectorGroupChat\",\n ]\n@@ -4,6 +4,7 @@\n \n from ..agents import ChatMessage\n from ._logging import EVENT_LOGGER_NAME, ConsoleLogHandler\n+from ._termination import TerminationCondition\n \n logger = logging.getLogger(EVENT_LOGGER_NAME)\n logger.setLevel(logging.INFO)\n@@ -14,9 +15,10 @@\n @dataclass\n class TeamRunResult:\n     messages: List[ChatMessage]\n+    \"\"\"The messages generated by the team.\"\"\"\n \n \n class BaseTeam(Protocol):\n-    async def run(self, task: str) -> TeamRunResult:\n-        \"\"\"Run the team and return the result.\"\"\"\n+    async def run(self, task: str, *, termination_condition: TerminationCondition | None = None) -> TeamRunResult:\n+        \"\"\"Run the team on a given task until the termination condition is met.\"\"\"\n         ...\n@@ -61,3 +61,15 @@ class SelectSpeakerEvent(BaseModel):\n     \"\"\"The agent ID that selected the speaker.\"\"\"\n \n     model_config = ConfigDict(arbitrary_types_allowed=True)\n+\n+\n+class TerminationEvent(BaseModel):\n+    \"\"\"An event for terminating a conversation.\"\"\"\n+\n+    agent_message: StopMessage\n+    \"\"\"The stop message that terminates the conversation.\"\"\"\n+\n+    source: AgentId\n+    \"\"\"The agent ID that triggered the termination.\"\"\"\n+\n+    model_config = ConfigDict(arbitrary_types_allowed=True)\n@@ -3,113 +3,85 @@\n import sys\n from dataclasses import asdict, is_dataclass\n from datetime import datetime\n-from typing import Any, Dict, List, Union\n+from typing import Any\n \n-from autogen_core.base import AgentId\n-from autogen_core.components import FunctionCall, Image\n-from autogen_core.components.models import FunctionExecutionResult\n-\n-from ..agents import ChatMessage, MultiModalMessage, StopMessage, TextMessage, ToolCallMessage, ToolCallResultMessage\n-from ._events import ContentPublishEvent, SelectSpeakerEvent, ToolCallEvent, ToolCallResultEvent\n+from ..agents import ChatMessage, StopMessage, TextMessage\n+from ._events import ContentPublishEvent, SelectSpeakerEvent, TerminationEvent, ToolCallEvent, ToolCallResultEvent\n \n TRACE_LOGGER_NAME = \"autogen_agentchat\"\n EVENT_LOGGER_NAME = \"autogen_agentchat.events\"\n-ContentType = Union[str, List[Union[str, Image]], List[FunctionCall], List[FunctionExecutionResult]]\n-\n \n-class BaseLogHandler(logging.Handler):\n-    def serialize_content(\n-        self,\n-        content: Union[ContentType, ChatMessage],\n-    ) -> Union[List[Any], Dict[str, Any], str]:\n-        if isinstance(content, (str, list)):\n-            return content\n-        elif isinstance(content, (TextMessage, MultiModalMessage, ToolCallMessage, ToolCallResultMessage, StopMessage)):\n-            return asdict(content)\n-        elif isinstance(content, Image):\n-            return {\"type\": \"image\", \"data\": content.data_uri}\n-        elif isinstance(content, FunctionCall):\n-            return {\"type\": \"function_call\", \"name\": content.name, \"arguments\": content.arguments}\n-        elif isinstance(content, FunctionExecutionResult):\n-            return {\"type\": \"function_execution_result\", \"content\": content.content}\n-        return str(content)\n \n+class ConsoleLogHandler(logging.Handler):\n     @staticmethod\n-    def json_serializer(obj: Any) -> Any:\n-        if is_dataclass(obj) and not isinstance(obj, type):\n-            return asdict(obj)\n-        elif isinstance(obj, type):\n-            return str(obj)\n-        return str(obj)\n-\n-\n-class ConsoleLogHandler(BaseLogHandler):\n-    def _format_chat_message(\n-        self,\n-        *,\n-        source_agent_id: AgentId | None,\n-        message: ChatMessage,\n-        timestamp: str,\n-    ) -> str:\n-        body = f\"{self.serialize_content(message.content)}\"\n-        if source_agent_id is None:\n-            console_message = f\"\\n{'-'*75} \\n\" f\"\\033[91m[{timestamp}]:\\033[0m\\n\" f\"\\n{body}\"\n+    def serialize_chat_message(message: ChatMessage) -> str:\n+        if isinstance(message, TextMessage | StopMessage):\n+            return message.content\n         else:\n-            # Display the source agent type rather than agent ID for better readability.\n-            # Also in AgentChat the agent type is unique for each agent.\n-            console_message = f\"\\n{'-'*75} \\n\" f\"\\033[91m[{timestamp}], {source_agent_id.type}:\\033[0m\\n\" f\"\\n{body}\"\n-        return console_message\n+            d = message.model_dump()\n+            assert \"content\" in d\n+            return json.dumps(d[\"content\"], indent=2)\n \n     def emit(self, record: logging.LogRecord) -> None:\n         ts = datetime.fromtimestamp(record.created).isoformat()\n         if isinstance(record.msg, ContentPublishEvent):\n-            sys.stdout.write(\n-                self._format_chat_message(\n-                    source_agent_id=record.msg.source,\n-                    message=record.msg.agent_message,\n-                    timestamp=ts,\n+            if record.msg.source is None:\n+                sys.stdout.write(\n+                    f\"\\n{'-'*75} \\n\"\n+                    f\"\\033[91m[{ts}]:\\033[0m\\n\"\n+                    f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n+                )\n+            else:\n+                sys.stdout.write(\n+                    f\"\\n{'-'*75} \\n\"\n+                    f\"\\033[91m[{ts}], {record.msg.source.type}:\\033[0m\\n\"\n+                    f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n                 )\n-            )\n             sys.stdout.flush()\n         elif isinstance(record.msg, ToolCallEvent):\n             sys.stdout.write(\n                 f\"\\n{'-'*75} \\n\"\n                 f\"\\033[91m[{ts}], Tool Call:\\033[0m\\n\"\n-                f\"\\n{self.serialize_content(record.msg.agent_message)}\"\n+                f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n             )\n             sys.stdout.flush()\n         elif isinstance(record.msg, ToolCallResultEvent):\n             sys.stdout.write(\n                 f\"\\n{'-'*75} \\n\"\n                 f\"\\033[91m[{ts}], Tool Call Result:\\033[0m\\n\"\n-                f\"\\n{self.serialize_content(record.msg.agent_message)}\"\n+                f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n             )\n             sys.stdout.flush()\n         elif isinstance(record.msg, SelectSpeakerEvent):\n+            sys.stdout.write(\n+                f\"\\n{'-'*75} \\n\" f\"\\033[91m[{ts}], Selected Next Speaker:\\033[0m\\n\" f\"\\n{record.msg.selected_speaker}\"\n+            )\n+            sys.stdout.flush()\n+        elif isinstance(record.msg, TerminationEvent):\n             sys.stdout.write(\n                 f\"\\n{'-'*75} \\n\"\n-                f\"\\033[91m[{ts}], {record.msg.source.type}:\\033[0m\\n\"\n-                f\"\\nSelected next speaker: {record.msg.selected_speaker}\"\n+                f\"\\033[91m[{ts}], Termination:\\033[0m\\n\"\n+                f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n             )\n             sys.stdout.flush()\n         else:\n             raise ValueError(f\"Unexpected log record: {record.msg}\")\n \n \n-class FileLogHandler(BaseLogHandler):\n+class FileLogHandler(logging.Handler):\n     def __init__(self, filename: str) -> None:\n         super().__init__()\n         self.filename = filename\n         self.file_handler = logging.FileHandler(filename)\n \n     def emit(self, record: logging.LogRecord) -> None:\n         ts = datetime.fromtimestamp(record.created).isoformat()\n-        if isinstance(record.msg, ContentPublishEvent | ToolCallEvent | ToolCallResultEvent):\n+        if isinstance(record.msg, ContentPublishEvent | ToolCallEvent | ToolCallResultEvent | TerminationEvent):\n             log_entry = json.dumps(\n                 {\n                     \"timestamp\": ts,\n                     \"source\": record.msg.source,\n-                    \"agent_message\": self.serialize_content(record.msg.agent_message),\n+                    \"agent_message\": record.msg.agent_message.model_dump(),\n                     \"type\": record.msg.__class__.__name__,\n                 },\n                 default=self.json_serializer,\n@@ -140,3 +112,11 @@ def emit(self, record: logging.LogRecord) -> None:\n     def close(self) -> None:\n         self.file_handler.close()\n         super().close()\n+\n+    @staticmethod\n+    def json_serializer(obj: Any) -> Any:\n+        if is_dataclass(obj) and not isinstance(obj, type):\n+            return asdict(obj)\n+        elif isinstance(obj, type):\n+            return str(obj)\n+        return str(obj)\n@@ -0,0 +1,215 @@\n+import asyncio\n+from abc import ABC, abstractmethod\n+from typing import List, Sequence\n+\n+from ..agents import ChatMessage, MultiModalMessage, StopMessage, TextMessage\n+\n+\n+class TerminatedException(BaseException): ...\n+\n+\n+class TerminationCondition(ABC):\n+    \"\"\"A stateful condition that determines when a conversation should be terminated.\n+\n+    A termination condition is a callable that takes a sequence of ChatMessage objects\n+    since the last time the condition was called, and returns a StopMessage if the\n+    conversation should be terminated, or None otherwise.\n+    Once a termination condition has been reached, it must be reset before it can be used again.\n+\n+    Termination conditions can be combined using the AND and OR operators.\n+\n+    Example:\n+\n+        .. code-block:: python\n+\n+            from autogen_agentchat.teams import MaxTurnsTermination, TextMentionTermination\n+\n+            # Terminate the conversation after 10 turns or if the text \"TERMINATE\" is mentioned.\n+            cond1 = MaxTurnsTermination(10) | TextMentionTermination(\"TERMINATE\")\n+\n+            # Terminate the conversation after 10 turns and if the text \"TERMINATE\" is mentioned.\n+            cond2 = MaxTurnsTermination(10) & TextMentionTermination(\"TERMINATE\")\n+\n+            ...\n+\n+            # Reset the termination condition.\n+            await cond1.reset()\n+            await cond2.reset()\n+    \"\"\"\n+\n+    @property\n+    @abstractmethod\n+    def terminated(self) -> bool:\n+        \"\"\"Check if the termination condition has been reached\"\"\"\n+        ...\n+\n+    @abstractmethod\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        \"\"\"Check if the conversation should be terminated based on the messages received\n+        since the last time the condition was called.\n+        Return a StopMessage if the conversation should be terminated, or None otherwise.\n+\n+        Args:\n+            messages: The messages received since the last time the condition was called.\n+\n+        Returns:\n+            StopMessage | None: A StopMessage if the conversation should be terminated, or None otherwise.\n+\n+        Raises:\n+            TerminatedException: If the termination condition has already been reached.\"\"\"\n+        ...\n+\n+    @abstractmethod\n+    async def reset(self) -> None:\n+        \"\"\"Reset the termination condition.\"\"\"\n+        ...\n+\n+    def __and__(self, other: \"TerminationCondition\") -> \"TerminationCondition\":\n+        \"\"\"Combine two termination conditions with an AND operation.\"\"\"\n+        return _AndTerminationCondition(self, other)\n+\n+    def __or__(self, other: \"TerminationCondition\") -> \"TerminationCondition\":\n+        \"\"\"Combine two termination conditions with an OR operation.\"\"\"\n+        return _OrTerminationCondition(self, other)\n+\n+\n+class _AndTerminationCondition(TerminationCondition):\n+    def __init__(self, *conditions: TerminationCondition) -> None:\n+        self._conditions = conditions\n+        self._stop_messages: List[StopMessage] = []\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return all(condition.terminated for condition in self._conditions)\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self.terminated:\n+            raise TerminatedException(\"Termination condition has already been reached.\")\n+        # Check all remaining conditions.\n+        stop_messages = await asyncio.gather(\n+            *[condition(messages) for condition in self._conditions if not condition.terminated]\n+        )\n+        # Collect stop messages.\n+        for stop_message in stop_messages:\n+            if stop_message is not None:\n+                self._stop_messages.append(stop_message)\n+        if any(stop_message is None for stop_message in stop_messages):\n+            # If any remaining condition has not reached termination, it is not terminated.\n+            return None\n+        content = \", \".join(stop_message.content for stop_message in self._stop_messages)\n+        source = \", \".join(stop_message.source for stop_message in self._stop_messages)\n+        return StopMessage(content=content, source=source)\n+\n+    async def reset(self) -> None:\n+        for condition in self._conditions:\n+            await condition.reset()\n+        self._stop_messages.clear()\n+\n+\n+class _OrTerminationCondition(TerminationCondition):\n+    def __init__(self, *conditions: TerminationCondition) -> None:\n+        self._conditions = conditions\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return any(condition.terminated for condition in self._conditions)\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self.terminated:\n+            raise RuntimeError(\"Termination condition has already been reached\")\n+        stop_messages = await asyncio.gather(*[condition(messages) for condition in self._conditions])\n+        if any(stop_message is not None for stop_message in stop_messages):\n+            content = \", \".join(stop_message.content for stop_message in stop_messages if stop_message is not None)\n+            source = \", \".join(stop_message.source for stop_message in stop_messages if stop_message is not None)\n+            return StopMessage(content=content, source=source)\n+        return None\n+\n+    async def reset(self) -> None:\n+        for condition in self._conditions:\n+            await condition.reset()\n+\n+\n+class StopMessageTermination(TerminationCondition):\n+    \"\"\"Terminate the conversation if a StopMessage is received.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self._terminated = False\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return self._terminated\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self._terminated:\n+            raise TerminatedException(\"Termination condition has already been reached\")\n+        for message in messages:\n+            if isinstance(message, StopMessage):\n+                self._terminated = True\n+                return StopMessage(content=\"Stop message received\", source=\"StopMessageTermination\")\n+        return None\n+\n+    async def reset(self) -> None:\n+        self._terminated = False\n+\n+\n+class MaxMessageTermination(TerminationCondition):\n+    \"\"\"Terminate the conversation after a maximum number of messages have been exchanged.\n+\n+    Args:\n+        max_messages: The maximum number of messages allowed in the conversation.\n+    \"\"\"\n+\n+    def __init__(self, max_messages: int) -> None:\n+        self._max_messages = max_messages\n+        self._message_count = 0\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return self._message_count >= self._max_messages\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self.terminated:\n+            raise TerminatedException(\"Termination condition has already been reached\")\n+        self._message_count += len(messages)\n+        if self._message_count >= self._max_messages:\n+            return StopMessage(\n+                content=f\"Maximal number of messages {self._max_messages} reached, current message count: {self._message_count}\",\n+                source=\"MaxMessageTermination\",\n+            )\n+        return None\n+\n+    async def reset(self) -> None:\n+        self._message_count = 0\n+\n+\n+class TextMentionTermination(TerminationCondition):\n+    \"\"\"Terminate the conversation if a specific text is mentioned.\n+\n+    Args:\n+        text: The text to look for in the messages.\n+    \"\"\"\n+\n+    def __init__(self, text: str) -> None:\n+        self._text = text\n+        self._terminated = False\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return self._terminated\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self._terminated:\n+            raise TerminatedException(\"Termination condition has already been reached\")\n+        for message in messages:\n+            if isinstance(message, TextMessage | StopMessage) and self._text in message.content:\n+                self._terminated = True\n+                return StopMessage(content=f\"Text '{self._text}' mentioned\", source=\"TextMentionTermination\")\n+            elif isinstance(message, MultiModalMessage):\n+                for item in message.content:\n+                    if isinstance(item, str) and self._text in item:\n+                        self._terminated = True\n+                        return StopMessage(content=f\"Text '{self._text}' mentioned\", source=\"TextMentionTermination\")\n+        return None\n+\n+    async def reset(self) -> None:\n+        self._terminated = False\n@@ -1,4 +0,0 @@\n-from ._round_robin_group_chat import RoundRobinGroupChat\n-from ._selector_group_chat import SelectorGroupChat\n-\n-__all__ = [\"RoundRobinGroupChat\", \"SelectorGroupChat\"]\n@@ -8,11 +8,10 @@\n from autogen_core.components.tool_agent import ToolAgent\n from autogen_core.components.tools import Tool\n \n-from autogen_agentchat.agents._base_chat_agent import ChatMessage\n-\n-from ...agents import BaseChatAgent, BaseToolUseChatAgent, TextMessage\n+from ...agents import BaseChatAgent, BaseToolUseChatAgent, ChatMessage, TextMessage\n from .._base_team import BaseTeam, TeamRunResult\n from .._events import ContentPublishEvent, ContentRequestEvent\n+from .._termination import TerminationCondition\n from ._base_chat_agent_container import BaseChatAgentContainer\n from ._base_group_chat_manager import BaseGroupChatManager\n \n@@ -45,6 +44,7 @@ def _create_group_chat_manager_factory(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n     ) -> Callable[[], BaseGroupChatManager]: ...\n \n     def _create_participant_factory(\n@@ -69,8 +69,10 @@ def _factory() -> ToolAgent:\n \n         return _factory\n \n-    async def run(self, task: str) -> TeamRunResult:\n+    async def run(self, task: str, *, termination_condition: TerminationCondition | None = None) -> TeamRunResult:\n         \"\"\"Run the team and return the result.\"\"\"\n+        # Create intervention handler for termination.\n+\n         # Create the runtime.\n         runtime = SingleThreadedAgentRuntime()\n \n@@ -122,6 +124,7 @@ async def run(self, task: str) -> TeamRunResult:\n                 group_topic_type=group_topic_type,\n                 participant_topic_types=participant_topic_types,\n                 participant_descriptions=participant_descriptions,\n+                termination_condition=termination_condition,\n             ),\n         )\n         # Add subscriptions for the group chat manager.\n@@ -147,7 +150,7 @@ async def collect_group_chat_messages(\n             type=\"collect_group_chat_messages\",\n             closure=collect_group_chat_messages,\n             subscriptions=lambda: [\n-                TypeSubscription(topic_type=group_topic_type, agent_type=\"collect_group_chat_messages\")\n+                TypeSubscription(topic_type=group_topic_type, agent_type=\"collect_group_chat_messages\"),\n             ],\n         )\n \n@@ -166,4 +169,5 @@ async def collect_group_chat_messages(\n         # Wait for the runtime to stop.\n         await runtime.stop_when_idle()\n \n+        # Return the result.\n         return TeamRunResult(messages=group_chat_messages)\n@@ -5,9 +5,9 @@\n from autogen_core.base import MessageContext, TopicId\n from autogen_core.components import event\n \n-from ...agents import StopMessage, TextMessage\n-from .._events import ContentPublishEvent, ContentRequestEvent\n+from .._events import ContentPublishEvent, ContentRequestEvent, TerminationEvent\n from .._logging import EVENT_LOGGER_NAME\n+from .._termination import TerminationCondition\n from ._sequential_routed_agent import SequentialRoutedAgent\n \n event_logger = logging.getLogger(EVENT_LOGGER_NAME)\n@@ -29,6 +29,7 @@ class BaseGroupChatManager(SequentialRoutedAgent, ABC):\n         group_topic_type (str): The topic type of the group chat.\n         participant_topic_types (List[str]): The topic types of the participants.\n         participant_descriptions (List[str]): The descriptions of the participants\n+        termination_condition (TerminationCondition, optional): The termination condition for the group chat. Defaults to None.\n \n     Raises:\n         ValueError: If the number of participant topic types, agent types, and descriptions are not the same.\n@@ -40,6 +41,7 @@ def __init__(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None = None,\n     ):\n         super().__init__(description=\"Group chat manager\")\n         self._parent_topic_type = parent_topic_type\n@@ -57,6 +59,7 @@ def __init__(\n         self._participant_topic_types = participant_topic_types\n         self._participant_descriptions = participant_descriptions\n         self._message_thread: List[ContentPublishEvent] = []\n+        self._termination_condition = termination_condition\n \n     @event\n     async def handle_content_publish(self, message: ContentPublishEvent, ctx: MessageContext) -> None:\n@@ -74,24 +77,25 @@ async def handle_content_publish(self, message: ContentPublishEvent, ctx: Messag\n         # Process event from parent.\n         if ctx.topic_id.type == self._parent_topic_type:\n             self._message_thread.append(message)\n-            await self.publish_message(message, topic_id=group_chat_topic_id)\n+            await self.publish_message(\n+                ContentPublishEvent(agent_message=message.agent_message, source=self.id), topic_id=group_chat_topic_id\n+            )\n             return\n \n         # Process event from the group chat this agent manages.\n         assert ctx.topic_id.type == self._group_topic_type\n         self._message_thread.append(message)\n \n-        # If the message is a stop message, publish the last message as a TextMessage to the parent topic.\n-        # TODO: custom handling the final message.\n-        if isinstance(message.agent_message, StopMessage):\n-            parent_topic_id = TopicId(type=self._parent_topic_type, source=ctx.topic_id.source)\n-            await self.publish_message(\n-                ContentPublishEvent(\n-                    agent_message=TextMessage(content=message.agent_message.content, source=self.metadata[\"type\"])\n-                ),\n-                topic_id=parent_topic_id,\n-            )\n-            return\n+        # Check if the conversation should be terminated.\n+        if self._termination_condition is not None:\n+            stop_message = await self._termination_condition([message.agent_message])\n+            if stop_message is not None:\n+                event_logger.info(TerminationEvent(agent_message=stop_message, source=self.id))\n+                # Reset the termination condition.\n+                await self._termination_condition.reset()\n+                # Stop the group chat.\n+                # TODO: this should be different if the group chat is nested.\n+                return\n \n         # Select a speaker to continue the conversation.\n         speaker_topic_type = await self.select_speaker(self._message_thread)\n@@ -2,6 +2,7 @@\n \n from ...agents import BaseChatAgent\n from .._events import ContentPublishEvent\n+from .._termination import TerminationCondition\n from ._base_group_chat import BaseGroupChat\n from ._base_group_chat_manager import BaseGroupChatManager\n \n@@ -15,12 +16,14 @@ def __init__(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n     ) -> None:\n         super().__init__(\n             parent_topic_type,\n             group_topic_type,\n             participant_topic_types,\n             participant_descriptions,\n+            termination_condition,\n         )\n         self._next_speaker_index = 0\n \n@@ -51,23 +54,23 @@ class RoundRobinGroupChat(BaseGroupChat):\n         .. code-block:: python\n \n             from autogen_agentchat.agents import ToolUseAssistantAgent\n-            from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\n+            from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n \n             assistant = ToolUseAssistantAgent(\"Assistant\", model_client=..., registered_tools=...)\n             team = RoundRobinGroupChat([assistant])\n-            await team.run(\"What's the weather in New York?\")\n+            await team.run(\"What's the weather in New York?\", termination_condition=StopMessageTermination())\n \n     A team with multiple participants:\n \n         .. code-block:: python\n \n             from autogen_agentchat.agents import CodingAssistantAgent, CodeExecutorAgent\n-            from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\n+            from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n \n             coding_assistant = CodingAssistantAgent(\"Coding_Assistant\", model_client=...)\n             executor_agent = CodeExecutorAgent(\"Code_Executor\", code_executor=...)\n             team = RoundRobinGroupChat([coding_assistant, executor_agent])\n-            await team.run(\"Write a program that prints 'Hello, world!'\")\n+            await team.run(\"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination())\n \n     \"\"\"\n \n@@ -80,10 +83,15 @@ def _create_group_chat_manager_factory(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n     ) -> Callable[[], RoundRobinGroupChatManager]:\n         def _factory() -> RoundRobinGroupChatManager:\n             return RoundRobinGroupChatManager(\n-                parent_topic_type, group_topic_type, participant_topic_types, participant_descriptions\n+                parent_topic_type,\n+                group_topic_type,\n+                participant_topic_types,\n+                participant_descriptions,\n+                termination_condition,\n             )\n \n         return _factory\n@@ -7,6 +7,7 @@\n from ...agents import BaseChatAgent, MultiModalMessage, StopMessage, TextMessage\n from .._events import ContentPublishEvent, SelectSpeakerEvent\n from .._logging import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME\n+from .._termination import TerminationCondition\n from ._base_group_chat import BaseGroupChat\n from ._base_group_chat_manager import BaseGroupChatManager\n \n@@ -24,6 +25,7 @@ def __init__(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n         model_client: ChatCompletionClient,\n         selector_prompt: str,\n         allow_repeated_speaker: bool,\n@@ -33,6 +35,7 @@ def __init__(\n             group_topic_type,\n             participant_topic_types,\n             participant_descriptions,\n+            termination_condition,\n         )\n         self._model_client = model_client\n         self._selector_prompt = selector_prompt\n@@ -164,13 +167,13 @@ class SelectorGroupChat(BaseGroupChat):\n         .. code-block:: python\n \n             from autogen_agentchat.agents import ToolUseAssistantAgent\n-            from autogen_agentchat.teams.group_chat import SelectorGroupChat\n+            from autogen_agentchat.teams import SelectorGroupChat, StopMessageTermination\n \n             travel_advisor = ToolUseAssistantAgent(\"Travel_Advisor\", model_client=..., registered_tools=...)\n             hotel_agent = ToolUseAssistantAgent(\"Hotel_Agent\", model_client=..., registered_tools=...)\n             flight_agent = ToolUseAssistantAgent(\"Flight_Agent\", model_client=..., registered_tools=...)\n             team = SelectorGroupChat([travel_advisor, hotel_agent, flight_agent], model_client=...)\n-            await team.run(\"Book a 3-day trip to new york.\")\n+            await team.run(\"Book a 3-day trip to new york.\", termination_condition=StopMessageTermination())\n     \"\"\"\n \n     def __init__(\n@@ -209,12 +212,14 @@ def _create_group_chat_manager_factory(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n     ) -> Callable[[], BaseGroupChatManager]:\n         return lambda: SelectorGroupChatManager(\n             parent_topic_type,\n             group_topic_type,\n             participant_topic_types,\n             participant_descriptions,\n+            termination_condition,\n             self._model_client,\n             self._selector_prompt,\n             self._allow_repeated_speaker,\n@@ -1,5 +1,6 @@\n import asyncio\n import json\n+import logging\n import tempfile\n from typing import Any, AsyncGenerator, List, Sequence\n \n@@ -13,7 +14,13 @@\n     TextMessage,\n     ToolUseAssistantAgent,\n )\n-from autogen_agentchat.teams.group_chat import RoundRobinGroupChat, SelectorGroupChat\n+from autogen_agentchat.teams import (\n+    EVENT_LOGGER_NAME,\n+    FileLogHandler,\n+    RoundRobinGroupChat,\n+    SelectorGroupChat,\n+    StopMessageTermination,\n+)\n from autogen_core.base import CancellationToken\n from autogen_core.components import FunctionCall\n from autogen_core.components.code_executor import LocalCommandLineCodeExecutor\n@@ -26,6 +33,10 @@\n from openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall, Function\n from openai.types.completion_usage import CompletionUsage\n \n+logger = logging.getLogger(EVENT_LOGGER_NAME)\n+logger.setLevel(logging.DEBUG)\n+logger.addHandler(FileLogHandler(\"test_group_chat.log\"))\n+\n \n class _MockChatCompletion:\n     def __init__(self, chat_completions: List[ChatCompletion]) -> None:\n@@ -119,7 +130,9 @@ async def test_round_robin_group_chat(monkeypatch: pytest.MonkeyPatch) -> None:\n             \"coding_assistant\", model_client=OpenAIChatCompletionClient(model=model, api_key=\"\")\n         )\n         team = RoundRobinGroupChat(participants=[coding_assistant_agent, code_executor_agent])\n-        result = await team.run(\"Write a program that prints 'Hello, world!'\")\n+        result = await team.run(\n+            \"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination()\n+        )\n         expected_messages = [\n             \"Write a program that prints 'Hello, world!'\",\n             'Here is the program\\n ```python\\nprint(\"Hello, world!\")\\n```',\n@@ -200,7 +213,7 @@ async def test_round_robin_group_chat_with_tools(monkeypatch: pytest.MonkeyPatch\n     )\n     echo_agent = _EchoAgent(\"echo_agent\", description=\"echo agent\")\n     team = RoundRobinGroupChat(participants=[tool_use_agent, echo_agent])\n-    await team.run(\"Write a program that prints 'Hello, world!'\")\n+    await team.run(\"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination())\n     context = tool_use_agent._model_context  # pyright: ignore\n     assert context[0].content == \"Write a program that prints 'Hello, world!'\"\n     assert isinstance(context[1].content, list)\n@@ -279,7 +292,9 @@ async def test_selector_group_chat(monkeypatch: pytest.MonkeyPatch) -> None:\n         participants=[agent1, agent2, agent3],\n         model_client=OpenAIChatCompletionClient(model=model, api_key=\"\"),\n     )\n-    result = await team.run(\"Write a program that prints 'Hello, world!'\")\n+    result = await team.run(\n+        \"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination()\n+    )\n     assert len(result.messages) == 6\n     assert result.messages[0].content == \"Write a program that prints 'Hello, world!'\"\n     assert result.messages[1].source == \"agent3\"\n@@ -313,7 +328,9 @@ async def test_selector_group_chat_two_speakers(monkeypatch: pytest.MonkeyPatch)\n         participants=[agent1, agent2],\n         model_client=OpenAIChatCompletionClient(model=model, api_key=\"\"),\n     )\n-    result = await team.run(\"Write a program that prints 'Hello, world!'\")\n+    result = await team.run(\n+        \"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination()\n+    )\n     assert len(result.messages) == 5\n     assert result.messages[0].content == \"Write a program that prints 'Hello, world!'\"\n     assert result.messages[1].source == \"agent2\"\n@@ -369,7 +386,9 @@ async def test_selector_group_chat_two_speakers_allow_repeated(monkeypatch: pyte\n         model_client=OpenAIChatCompletionClient(model=model, api_key=\"\"),\n         allow_repeated_speaker=True,\n     )\n-    result = await team.run(\"Write a program that prints 'Hello, world!'\")\n+    result = await team.run(\n+        \"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination()\n+    )\n     assert len(result.messages) == 4\n     assert result.messages[0].content == \"Write a program that prints 'Hello, world!'\"\n     assert result.messages[1].source == \"agent2\"\n@@ -0,0 +1,126 @@\n+import pytest\n+from autogen_agentchat.agents import StopMessage, TextMessage\n+from autogen_agentchat.teams import MaxMessageTermination, StopMessageTermination, TextMentionTermination\n+\n+\n+@pytest.mark.asyncio\n+async def test_stop_message_termination() -> None:\n+    termination = StopMessageTermination()\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert await termination([StopMessage(content=\"Stop\", source=\"user\")]) is not None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"World\", source=\"agent\")])\n+        is None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), StopMessage(content=\"Stop\", source=\"user\")])\n+        is not None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_max_message_termination() -> None:\n+    termination = MaxMessageTermination(2)\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"World\", source=\"agent\")])\n+        is not None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_mention_termination() -> None:\n+    termination = TextMentionTermination(\"stop\")\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"stop\", source=\"user\")]) is not None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"stop\", source=\"user\")])\n+        is not None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_and_termination() -> None:\n+    termination = MaxMessageTermination(2) & TextMentionTermination(\"stop\")\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"World\", source=\"agent\")])\n+        is None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"stop\", source=\"user\")])\n+        is not None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_or_termination() -> None:\n+    termination = MaxMessageTermination(3) | TextMentionTermination(\"stop\")\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"World\", source=\"agent\")])\n+        is None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"stop\", source=\"user\")])\n+        is not None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"Hello\", source=\"user\")])\n+        is None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination(\n+            [\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+            ]\n+        )\n+        is not None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination(\n+            [\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"stop\", source=\"user\"),\n+            ]\n+        )\n+        is not None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination(\n+            [\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"stop\", source=\"user\"),\n+            ]\n+        )\n+        is not None\n+    )\n@@ -23,7 +23,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from autogen_agentchat.agents import CodingAssistantAgent, ToolUseAssistantAgent\\n\",\n-    \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n+    \"from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n     \"from autogen_core.components.tools import FunctionTool\"\n    ]\n@@ -63,10 +63,11 @@\n     \"\\n\",\n     \"def google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:  # type: ignore[type-arg]\\n\",\n     \"    import os\\n\",\n+    \"    import time\\n\",\n+    \"\\n\",\n     \"    import requests\\n\",\n-    \"    from dotenv import load_dotenv\\n\",\n     \"    from bs4 import BeautifulSoup\\n\",\n-    \"    import time\\n\",\n+    \"    from dotenv import load_dotenv\\n\",\n     \"\\n\",\n     \"    load_dotenv()\\n\",\n     \"\\n\",\n@@ -115,13 +116,14 @@\n     \"\\n\",\n     \"\\n\",\n     \"def analyze_stock(ticker: str) -> dict:  # type: ignore[type-arg]\\n\",\n-    \"    import yfinance as yf\\n\",\n-    \"    import matplotlib.pyplot as plt\\n\",\n+    \"    import os\\n\",\n     \"    from datetime import datetime, timedelta\\n\",\n+    \"\\n\",\n+    \"    import matplotlib.pyplot as plt\\n\",\n     \"    import numpy as np\\n\",\n-    \"    from pytz import timezone  # type: ignore\\n\",\n     \"    import pandas as pd\\n\",\n-    \"    import os\\n\",\n+    \"    import yfinance as yf\\n\",\n+    \"    from pytz import timezone  # type: ignore\\n\",\n     \"\\n\",\n     \"    stock = yf.Ticker(ticker)\\n\",\n     \"\\n\",\n@@ -397,14 +399,14 @@\n     }\n    ],\n    \"source\": [\n-    \"result = await team.run(\\\"Write a financial report on American airlines\\\")\\n\",\n+    \"result = await team.run(\\\"Write a financial report on American airlines\\\", termination_condition=StopMessageTermination())\\n\",\n     \"print(result)\"\n    ]\n   }\n  ],\n  \"metadata\": {\n   \"kernelspec\": {\n-   \"display_name\": \"agnext\",\n+   \"display_name\": \".venv\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n   },\n@@ -418,7 +420,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.9\"\n+   \"version\": \"3.12.6\"\n   }\n  },\n  \"nbformat\": 4,\n@@ -23,7 +23,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from autogen_agentchat.agents import CodingAssistantAgent, ToolUseAssistantAgent\\n\",\n-    \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n+    \"from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n     \"from autogen_core.components.tools import FunctionTool\"\n    ]\n@@ -55,10 +55,11 @@\n    \"source\": [\n     \"def google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:  # type: ignore[type-arg]\\n\",\n     \"    import os\\n\",\n+    \"    import time\\n\",\n+    \"\\n\",\n     \"    import requests\\n\",\n-    \"    from dotenv import load_dotenv\\n\",\n     \"    from bs4 import BeautifulSoup\\n\",\n-    \"    import time\\n\",\n+    \"    from dotenv import load_dotenv\\n\",\n     \"\\n\",\n     \"    load_dotenv()\\n\",\n     \"\\n\",\n@@ -328,14 +329,16 @@\n     \"\\n\",\n     \"team = RoundRobinGroupChat(participants=[google_search_agent, arxiv_search_agent, report_agent])\\n\",\n     \"\\n\",\n-    \"result = await team.run(task=\\\"Write a literature review on no code tools for building multi agent ai systems\\\")\\n\",\n-    \"result\"\n+    \"result = await team.run(\\n\",\n+    \"    task=\\\"Write a literature review on no code tools for building multi agent ai systems\\\",\\n\",\n+    \"    termination_condition=StopMessageTermination(),\\n\",\n+    \")\"\n    ]\n   }\n  ],\n  \"metadata\": {\n   \"kernelspec\": {\n-   \"display_name\": \"agnext\",\n+   \"display_name\": \".venv\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n   },\n@@ -349,7 +352,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.9\"\n+   \"version\": \"3.12.6\"\n   }\n  },\n  \"nbformat\": 4,\n@@ -18,7 +18,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from autogen_agentchat.agents import CodingAssistantAgent\\n\",\n-    \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n+    \"from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\"\n    ]\n   },\n@@ -194,14 +194,14 @@\n    ],\n    \"source\": [\n     \"group_chat = RoundRobinGroupChat([planner_agent, local_agent, language_agent, travel_summary_agent])\\n\",\n-    \"result = await group_chat.run(task=\\\"Plan a 3 day trip to Nepal.\\\")\\n\",\n+    \"result = await group_chat.run(task=\\\"Plan a 3 day trip to Nepal.\\\", termination_condition=StopMessageTermination())\\n\",\n     \"print(result)\"\n    ]\n   }\n  ],\n  \"metadata\": {\n   \"kernelspec\": {\n-   \"display_name\": \"agnext\",\n+   \"display_name\": \".venv\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n   },\n@@ -215,7 +215,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.9\"\n+   \"version\": \"3.12.6\"\n   }\n  },\n  \"nbformat\": 4,\n@@ -314,7 +314,7 @@\n    ],\n    \"source\": [\n     \"from autogen_agentchat.agents import CodeExecutorAgent, CodingAssistantAgent\\n\",\n-    \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n+    \"from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.components.code_executor import DockerCommandLineCodeExecutor\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n     \"\\n\",\n@@ -325,7 +325,8 @@\n     \"    )\\n\",\n     \"    group_chat = RoundRobinGroupChat([coding_assistant_agent, code_executor_agent])\\n\",\n     \"    result = await group_chat.run(\\n\",\n-    \"        task=\\\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\\\"\\n\",\n+    \"        task=\\\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\\\",\\n\",\n+    \"        termination_condition=StopMessageTermination(),\\n\",\n     \"    )\\n\",\n     \"    print(result)\"\n    ]\n@@ -356,7 +357,7 @@\n  ],\n  \"metadata\": {\n   \"kernelspec\": {\n-   \"display_name\": \"agnext\",\n+   \"display_name\": \".venv\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n   },\n@@ -370,7 +371,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.9\"\n+   \"version\": \"3.12.6\"\n   }\n  },\n  \"nbformat\": 4,\n@@ -9,7 +9,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 13,\n+   \"execution_count\": 1,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -24,15 +24,15 @@\n     \"    TextMessage,\\n\",\n     \"    ToolUseAssistantAgent,\\n\",\n     \")\\n\",\n-    \"from autogen_agentchat.teams.group_chat import SelectorGroupChat\\n\",\n+    \"from autogen_agentchat.teams import SelectorGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.base import CancellationToken\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n     \"from autogen_core.components.tools import FunctionTool\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n+   \"execution_count\": 2,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -49,7 +49,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 15,\n+   \"execution_count\": 3,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -69,7 +69,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 16,\n+   \"execution_count\": 4,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n@@ -78,206 +78,113 @@\n      \"text\": [\n       \"\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:50.523469]:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:35:30.283450]:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Help user plan a trip and book a flight.\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:51.234858], group_chat_manager:\\u001b[0m\\n\",\n+      \"Help user plan a trip and book a flight.\"\n+     ]\n+    },\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n       \"\\n\",\n-      \"Selected next speaker: User\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:55.437051], User:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:35:48.275743], User:\\u001b[0m\\n\",\n       \"\\n\",\n       \"\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:55.957366], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:35:50.795496], TravelAssistant:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: TravelAssistant\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:58.291558], TravelAssistant:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Sure! I can help you plan your trip and provide information on booking a flight. Could you please provide me with the following details?\\n\",\n-      \"\\n\",\n-      \"1. Your departure city.\\n\",\n-      \"2. Your destination.\\n\",\n-      \"3. Travel dates (departure and return).\\n\",\n-      \"4. Number of travelers and their ages.\\n\",\n-      \"5. Any specific preferences or activities you would like to include in your trip?\\n\",\n+      \"I'd be happy to help you plan your trip! To get started, could you please provide me with the following details:\\n\",\n       \"\\n\",\n-      \"Once I have that information, we can get started!\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:58.827503], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: User\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:07.996036], User:\\u001b[0m\\n\",\n+      \"1. Your departure city and the destination city.\\n\",\n+      \"2. Your travel dates (departure and return).\\n\",\n+      \"3. The number of travelers and their ages (if any children are involved).\\n\",\n+      \"4. Your budget for flights and accommodations, if you have one in mind.\\n\",\n+      \"5. Any specific activities or attractions you're interested in at the destination.\\n\",\n       \"\\n\",\n-      \"Going to toronto from new york \\n\",\n+      \"Once I have this information, I can help you find the best options!\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:08.623692], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:35:59.701486], User:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: TravelAssistant\\n\",\n+      \"Traveling to toronto from new york\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:10.605232], TravelAssistant:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:02.325330], TravelAssistant:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Great! Here are a few more details I need to help you plan your trip:\\n\",\n+      \"Great choice! Toronto is a vibrant city with a lot to offer. Now, could you please provide the following additional details to help me assist you better?\\n\",\n       \"\\n\",\n-      \"1. **Departure dates:** When do you plan to leave New York and when will you return?\\n\",\n-      \"2. **Number of travelers:** How many people will be traveling with you, and what are their ages?\\n\",\n-      \"3. **Preferences:** Do you have any specific preferences for flight times or activities in Toronto? \\n\",\n+      \"1. What are your travel dates (departure and return)?\\n\",\n+      \"2. How many travelers will be going, and what are their ages?\\n\",\n+      \"3. Do you have a budget for the flight and accommodations?\\n\",\n+      \"4. Are there any specific activities or attractions you\u2019re interested in while in Toronto?\\n\",\n       \"\\n\",\n-      \"Once I have this information, I can assist you further!\\n\",\n+      \"Once I have this information, I can help you find the best flights and suggestions for your trip!\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:11.070495], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:20.633004], User:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: User\\n\",\n+      \"leaving on december 7 and returning on 12\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:26.768126], User:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:23.202871], TravelAssistant:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"leaving on december 12 and returning on 17, 2024\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:27.365051], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: TravelAssistant\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:30.335893], TravelAssistant:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Thank you for the details! Here\u2019s a summary of the trip so far:\\n\",\n+      \"Thank you for the details! Here's what I have so far:\\n\",\n       \"\\n\",\n       \"- **Departure City:** New York\\n\",\n-      \"- **Destination:** Toronto\\n\",\n-      \"- **Departure Date:** December 12, 2024\\n\",\n-      \"- **Return Date:** December 17, 2024\\n\",\n+      \"- **Destination City:** Toronto\\n\",\n+      \"- **Departure Date:** December 7\\n\",\n+      \"- **Return Date:** December 12\\n\",\n       \"\\n\",\n-      \"Now, could you please provide the following additional information?\\n\",\n+      \"Now, could you please provide:\\n\",\n       \"\\n\",\n-      \"1. **Number of travelers and their ages:** How many people will be traveling with you?\\n\",\n-      \"2. **Preferences for flights:** Any preferences for morning, afternoon, or evening flights?\\n\",\n-      \"3. **Activities:** Any specific activities or attractions you\u2019d like to prioritize in Toronto?\\n\",\n+      \"1. The number of travelers and their ages.\\n\",\n+      \"2. Your budget for flights and accommodations (if applicable).\\n\",\n+      \"3. Any specific activities or attractions you're interested in while in Toronto.\\n\",\n       \"\\n\",\n-      \"With this information, I can assist you in finding suitable flights and offer activity suggestions!\\n\",\n+      \"This will help me provide more tailored options for your trip!\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:30.822862], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:38.096554], User:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: User\\n\",\n+      \"just myself one adult\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:39.547965], User:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:40.307824], FlightBroker:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"just myself\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:40.110527], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: FlightBroker\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:45.764773], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Thank you for the information! Here\u2019s what I have so far for your trip:\\n\",\n+      \"Thanks for the information! Here's what I have:\\n\",\n       \"\\n\",\n       \"- **Departure City:** New York\\n\",\n-      \"- **Destination:** Toronto\\n\",\n-      \"- **Departure Date:** December 12, 2024\\n\",\n-      \"- **Return Date:** December 17, 2024\\n\",\n-      \"- **Number of Travelers:** 1 (yourself)\\n\",\n-      \"\\n\",\n-      \"Now, do you have any preferences for flight times (morning, afternoon, or evening)? Additionally, are there any specific activities or attractions you would like to include in your trip to Toronto? \\n\",\n+      \"- **Destination City:** Toronto\\n\",\n+      \"- **Departure Date:** December 7\\n\",\n+      \"- **Return Date:** December 12\\n\",\n+      \"- **Number of Travelers:** 1 Adult\\n\",\n       \"\\n\",\n-      \"Once I have that, I can start searching for flights!\\n\",\n+      \"Could you let me know if you have a budget for flights and accommodations? Additionally, are there any specific activities or attractions you're interested in while in Toronto? This will help me provide the best options for your trip!\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:46.363784], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:45.875280], User:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: User\\n\",\n+      \"that's it\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:59.508971], User:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:50.925624], FlightBroker:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"any time is fine\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:00.072654], group_chat_manager:\\u001b[0m\\n\",\n+      \"Your flights have been successfully booked! Here are the details:\\n\",\n       \"\\n\",\n-      \"Selected next speaker: FlightBroker\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:01.847222], FlightBroker:\\u001b[0m\\n\",\n+      \"- **Departure:** New York to Toronto\\n\",\n+      \"  - **Flight:** AL21\\n\",\n+      \"  - **Date:** December 7, 2023\\n\",\n       \"\\n\",\n-      \"[FunctionCall(id='call_lpuPUlo9k3p4VeX0h6jO8yJg', arguments='{\\\"start\\\":\\\"New York\\\",\\\"destination\\\":\\\"Toronto\\\",\\\"date\\\":\\\"2024-12-12\\\"}', name='flight_search')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:01.848179], tool_agent_for_FlightBroker:\\u001b[0m\\n\",\n+      \"- **Return:** Toronto to New York\\n\",\n+      \"  - **Flight:** AL21\\n\",\n+      \"  - **Date:** December 12, 2023\\n\",\n       \"\\n\",\n-      \"[FunctionExecutionResult(content='AC24 from New York to Toronto on 2024-12-12 is $500\\\\nUA23 from New York to Toronto on 2024-12-12 is $450\\\\nAL21 from New York to Toronto on 2024-12-12 is $400', call_id='call_lpuPUlo9k3p4VeX0h6jO8yJg')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:03.512522], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"[FunctionCall(id='call_dxxmiR6hVBL9QuneJGNnleR0', arguments='{\\\"start\\\":\\\"Toronto\\\",\\\"destination\\\":\\\"New York\\\",\\\"date\\\":\\\"2024-12-17\\\"}', name='flight_search')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:03.513405], tool_agent_for_FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"[FunctionExecutionResult(content='AC24 from Toronto to New York on 2024-12-17 is $500\\\\nUA23 from Toronto to New York on 2024-12-17 is $450\\\\nAL21 from Toronto to New York on 2024-12-17 is $400', call_id='call_dxxmiR6hVBL9QuneJGNnleR0')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:07.337638], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"I found some flight options for your trip from New York to Toronto and back. Here are the details:\\n\",\n-      \"\\n\",\n-      \"### Departure: New York to Toronto on December 12, 2024\\n\",\n-      \"1. **Flight AC24**\\n\",\n-      \"   - Price: $500\\n\",\n-      \"2. **Flight UA23**\\n\",\n-      \"   - Price: $450\\n\",\n-      \"3. **Flight AL21**\\n\",\n-      \"   - Price: $400\\n\",\n-      \"\\n\",\n-      \"### Return: Toronto to New York on December 17, 2024\\n\",\n-      \"1. **Flight AC24**\\n\",\n-      \"   - Price: $500\\n\",\n-      \"2. **Flight UA23**\\n\",\n-      \"   - Price: $450\\n\",\n-      \"3. **Flight AL21**\\n\",\n-      \"   - Price: $400\\n\",\n-      \"\\n\",\n-      \"Would you like to book any of these flights? If so, please let me know which flight you'd prefer for both the departure and return!\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:07.870903], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: User\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:22.910467], User:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Book the AC flight leaving and UA flight returning\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:23.450871], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: FlightBroker\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:27.273108], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"[FunctionCall(id='call_wqkaIBdYjWklWG0GQkYz7FZ0', arguments='{\\\"flight\\\": \\\"AC24\\\", \\\"date\\\": \\\"2024-12-12\\\"}', name='flight_booking'), FunctionCall(id='call_QZKtPHpbq2QzNi5y6OgfTcsd', arguments='{\\\"flight\\\": \\\"UA23\\\", \\\"date\\\": \\\"2024-12-17\\\"}', name='flight_booking')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:27.274111], tool_agent_for_FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"[FunctionExecutionResult(content='Booked flight AC24 on 2024-12-12', call_id='call_wqkaIBdYjWklWG0GQkYz7FZ0'), FunctionExecutionResult(content='Booked flight UA23 on 2024-12-17', call_id='call_QZKtPHpbq2QzNi5y6OgfTcsd')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:29.585911], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Your flights have been successfully booked!\\n\",\n-      \"\\n\",\n-      \"- **Departure Flight:** AC24 from New York to Toronto on December 12, 2024.\\n\",\n-      \"- **Return Flight:** UA23 from Toronto to New York on December 17, 2024.\\n\",\n-      \"\\n\",\n-      \"If you need any further assistance with your trip or have any questions about activities in Toronto, feel free to ask! Safe travels!\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:30.242282], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: User\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:39.715695], User:\\u001b[0m\\n\",\n+      \"If you need help with accommodations, activities, or anything else for your trip, feel free to let me know! \\n\",\n       \"\\n\",\n-      \"User has terminated the conversation.\"\n+      \"TERMINATE\"\n      ]\n     },\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"TeamRunResult(messages=[TextMessage(source='user', content='Help user plan a trip and book a flight.'), TextMessage(source='User', content=''), TextMessage(source='TravelAssistant', content='Sure! I can help you plan your trip and provide information on booking a flight. Could you please provide me with the following details?\\\\n\\\\n1. Your departure city.\\\\n2. Your destination.\\\\n3. Travel dates (departure and return).\\\\n4. Number of travelers and their ages.\\\\n5. Any specific preferences or activities you would like to include in your trip?\\\\n\\\\nOnce I have that information, we can get started!'), TextMessage(source='User', content='Going to toronto from new york '), TextMessage(source='TravelAssistant', content='Great! Here are a few more details I need to help you plan your trip:\\\\n\\\\n1. **Departure dates:** When do you plan to leave New York and when will you return?\\\\n2. **Number of travelers:** How many people will be traveling with you, and what are their ages?\\\\n3. **Preferences:** Do you have any specific preferences for flight times or activities in Toronto? \\\\n\\\\nOnce I have this information, I can assist you further!'), TextMessage(source='User', content='leaving on december 12 and returning on 17, 2024'), TextMessage(source='TravelAssistant', content='Thank you for the details! Here\u2019s a summary of the trip so far:\\\\n\\\\n- **Departure City:** New York\\\\n- **Destination:** Toronto\\\\n- **Departure Date:** December 12, 2024\\\\n- **Return Date:** December 17, 2024\\\\n\\\\nNow, could you please provide the following additional information?\\\\n\\\\n1. **Number of travelers and their ages:** How many people will be traveling with you?\\\\n2. **Preferences for flights:** Any preferences for morning, afternoon, or evening flights?\\\\n3. **Activities:** Any specific activities or attractions you\u2019d like to prioritize in Toronto?\\\\n\\\\nWith this information, I can assist you in finding suitable flights and offer activity suggestions!'), TextMessage(source='User', content='just myself'), TextMessage(source='FlightBroker', content='Thank you for the information! Here\u2019s what I have so far for your trip:\\\\n\\\\n- **Departure City:** New York\\\\n- **Destination:** Toronto\\\\n- **Departure Date:** December 12, 2024\\\\n- **Return Date:** December 17, 2024\\\\n- **Number of Travelers:** 1 (yourself)\\\\n\\\\nNow, do you have any preferences for flight times (morning, afternoon, or evening)? Additionally, are there any specific activities or attractions you would like to include in your trip to Toronto? \\\\n\\\\nOnce I have that, I can start searching for flights!'), TextMessage(source='User', content='any time is fine'), TextMessage(source='FlightBroker', content=\\\"I found some flight options for your trip from New York to Toronto and back. Here are the details:\\\\n\\\\n### Departure: New York to Toronto on December 12, 2024\\\\n1. **Flight AC24**\\\\n   - Price: $500\\\\n2. **Flight UA23**\\\\n   - Price: $450\\\\n3. **Flight AL21**\\\\n   - Price: $400\\\\n\\\\n### Return: Toronto to New York on December 17, 2024\\\\n1. **Flight AC24**\\\\n   - Price: $500\\\\n2. **Flight UA23**\\\\n   - Price: $450\\\\n3. **Flight AL21**\\\\n   - Price: $400\\\\n\\\\nWould you like to book any of these flights? If so, please let me know which flight you'd prefer for both the departure and return!\\\"), TextMessage(source='User', content='Book the AC flight leaving and UA flight returning'), TextMessage(source='FlightBroker', content='Your flights have been successfully booked!\\\\n\\\\n- **Departure Flight:** AC24 from New York to Toronto on December 12, 2024.\\\\n- **Return Flight:** UA23 from Toronto to New York on December 17, 2024.\\\\n\\\\nIf you need any further assistance with your trip or have any questions about activities in Toronto, feel free to ask! Safe travels!'), StopMessage(source='User', content='User has terminated the conversation.')])\"\n+       \"TeamRunResult(messages=[TextMessage(source='user', content='Help user plan a trip and book a flight.'), TextMessage(source='User', content=''), TextMessage(source='TravelAssistant', content=\\\"I'd be happy to help you plan your trip! To get started, could you please provide me with the following details:\\\\n\\\\n1. Your departure city and the destination city.\\\\n2. Your travel dates (departure and return).\\\\n3. The number of travelers and their ages (if any children are involved).\\\\n4. Your budget for flights and accommodations, if you have one in mind.\\\\n5. Any specific activities or attractions you're interested in at the destination.\\\\n\\\\nOnce I have this information, I can help you find the best options!\\\"), TextMessage(source='User', content='Traveling to toronto from new york'), TextMessage(source='TravelAssistant', content='Great choice! Toronto is a vibrant city with a lot to offer. Now, could you please provide the following additional details to help me assist you better?\\\\n\\\\n1. What are your travel dates (departure and return)?\\\\n2. How many travelers will be going, and what are their ages?\\\\n3. Do you have a budget for the flight and accommodations?\\\\n4. Are there any specific activities or attractions you\u2019re interested in while in Toronto?\\\\n\\\\nOnce I have this information, I can help you find the best flights and suggestions for your trip!'), TextMessage(source='User', content='leaving on december 7 and returning on 12'), TextMessage(source='TravelAssistant', content=\\\"Thank you for the details! Here's what I have so far:\\\\n\\\\n- **Departure City:** New York\\\\n- **Destination City:** Toronto\\\\n- **Departure Date:** December 7\\\\n- **Return Date:** December 12\\\\n\\\\nNow, could you please provide:\\\\n\\\\n1. The number of travelers and their ages.\\\\n2. Your budget for flights and accommodations (if applicable).\\\\n3. Any specific activities or attractions you're interested in while in Toronto.\\\\n\\\\nThis will help me provide more tailored options for your trip!\\\"), TextMessage(source='User', content='just myself one adult'), TextMessage(source='FlightBroker', content=\\\"Thanks for the information! Here's what I have:\\\\n\\\\n- **Departure City:** New York\\\\n- **Destination City:** Toronto\\\\n- **Departure Date:** December 7\\\\n- **Return Date:** December 12\\\\n- **Number of Travelers:** 1 Adult\\\\n\\\\nCould you let me know if you have a budget for flights and accommodations? Additionally, are there any specific activities or attractions you're interested in while in Toronto? This will help me provide the best options for your trip!\\\"), TextMessage(source='User', content=\\\"that's it\\\"), StopMessage(source='FlightBroker', content='Your flights have been successfully booked! Here are the details:\\\\n\\\\n- **Departure:** New York to Toronto\\\\n  - **Flight:** AL21\\\\n  - **Date:** December 7, 2023\\\\n\\\\n- **Return:** Toronto to New York\\\\n  - **Flight:** AL21\\\\n  - **Date:** December 12, 2023\\\\n\\\\nIf you need help with accommodations, activities, or anything else for your trip, feel free to let me know! \\\\n\\\\nTERMINATE'), StopMessage(source='StopMessageTermination', content='Stop message received')])\"\n       ]\n      },\n-     \"execution_count\": 16,\n+     \"execution_count\": 4,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -302,7 +209,7 @@\n     \"team = SelectorGroupChat(\\n\",\n     \"    [user_proxy, flight_broker, travel_assistant], model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\")\\n\",\n     \")\\n\",\n-    \"await team.run(\\\"Help user plan a trip and book a flight.\\\")\"\n+    \"await team.run(\\\"Help user plan a trip and book a flight.\\\", termination_condition=StopMessageTermination())\"\n    ]\n   }\n  ],\n@@ -1,236 +1,185 @@\n {\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"# Tool Use\\n\",\n-        \"\\n\",\n-        \"The `AgentChat` api provides a `ToolUseAssistantAgent` with presets for adding tools that the agent can call as part of it's response. \\n\",\n-        \"\\n\",\n-        \":::{note}\\n\",\n-        \"\\n\",\n-        \"The example presented here is a work in progress \ud83d\udea7. Also, tool uses here assumed the `model_client` used by the agent supports tool calling. \\n\",\n-        \"::: \"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 1,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"from autogen_agentchat.agents import ToolUseAssistantAgent\\n\",\n-        \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n-        \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n-        \"from autogen_core.components.tools import FunctionTool\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"In AgentChat, a Tool is a function wrapped in the `FunctionTool` class exported from `autogen_core.components.tools`.   \"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 2,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"async def get_weather(city: str) -> str:\\n\",\n-        \"    return f\\\"The weather in {city} is 72 degrees and Sunny.\\\"\\n\",\n-        \"\\n\",\n-        \"\\n\",\n-        \"get_weather_tool = FunctionTool(get_weather, description=\\\"Get the weather for a city\\\")\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"Finally, agents that use tools are defined in the following manner. \\n\",\n-        \"\\n\",\n-        \"-  An agent is instantiated based on the  `ToolUseAssistantAgent` class in AgentChat. The agent is aware of the tools it can use by passing a `tools_schema` attribute to the class, which is passed to the `model_client` when the agent generates a response.\\n\",\n-        \"-  An agent Team is defined that takes a list of `tools`.  Effectively, the `ToolUseAssistantAgent` can generate messages that call tools, and the team is responsible executing those tool calls and returning the results.\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 3,\n-      \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"name\": \"stdout\",\n-          \"output_type\": \"stream\",\n-          \"text\": [\n-            \"\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:50:13.202461]:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"What's the weather in New York?\\n\",\n-            \"From: user\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:50:14.090696], Weather_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionCall(id='call_wqkaIBdYjWklWG0GQkYz7FZ0', arguments='{\\\"city\\\":\\\"New York\\\"}', name='get_weather')]\\n\",\n-            \"From: Weather_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:50:14.092050], tool_agent_for_Weather_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionExecutionResult(content='The weather in New York is 72 degrees and Sunny.', call_id='call_wqkaIBdYjWklWG0GQkYz7FZ0')]\\n\",\n-            \"From: tool_agent_for_Weather_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:50:14.714470], Weather_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"The weather in New York is 72 degrees and sunny. \\n\",\n-            \"\\n\",\n-            \"TERMINATE\\n\",\n-            \"From: Weather_Assistant\"\n-          ]\n-        }\n-      ],\n-      \"source\": [\n-        \"assistant = ToolUseAssistantAgent(\\n\",\n-        \"    \\\"Weather_Assistant\\\",\\n\",\n-        \"    model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\"),\\n\",\n-        \"    registered_tools=[get_weather_tool],\\n\",\n-        \")\\n\",\n-        \"team = RoundRobinGroupChat([assistant])\\n\",\n-        \"result = await team.run(\\\"What's the weather in New York?\\\")\\n\",\n-        \"# print(result)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Using Langchain Tools \\n\",\n-        \"\\n\",\n-        \"AutoGen also provides direct support for tools from LangChain via the `autogen_ext`  package.\\n\",\n-        \"\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 4,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"# pip install langchain, langchain-community, wikipedia , autogen-ext\\n\",\n-        \"\\n\",\n-        \"from autogen_ext.tools.langchain import LangChainToolAdapter\\n\",\n-        \"from langchain.tools import WikipediaQueryRun\\n\",\n-        \"from langchain_community.utilities import WikipediaAPIWrapper\\n\",\n-        \"\\n\",\n-        \"api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\\n\",\n-        \"tool = WikipediaQueryRun(api_wrapper=api_wrapper)\\n\",\n-        \"\\n\",\n-        \"langchain_wikipedia_tool = LangChainToolAdapter(tool)\"\n-      ]\n-    },\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Tool Use\\n\",\n+    \"\\n\",\n+    \"The `AgentChat` api provides a `ToolUseAssistantAgent` with presets for adding tools that the agent can call as part of it's response. \\n\",\n+    \"\\n\",\n+    \":::{note}\\n\",\n+    \"\\n\",\n+    \"The example presented here is a work in progress \ud83d\udea7. Also, tool uses here assumed the `model_client` used by the agent supports tool calling. \\n\",\n+    \"::: \"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from autogen_agentchat.agents import ToolUseAssistantAgent\\n\",\n+    \"from autogen_agentchat.teams import EVENT_LOGGER_NAME, RoundRobinGroupChat, StopMessageTermination\\n\",\n+    \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n+    \"from autogen_core.components.tools import FunctionTool\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"In AgentChat, a Tool is a function wrapped in the `FunctionTool` class exported from `autogen_core.components.tools`.   \"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"async def get_weather(city: str) -> str:\\n\",\n+    \"    return f\\\"The weather in {city} is 72 degrees and Sunny.\\\"\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"get_weather_tool = FunctionTool(get_weather, description=\\\"Get the weather for a city\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Finally, agents that use tools are defined in the following manner. \\n\",\n+    \"\\n\",\n+    \"-  An agent is instantiated based on the  `ToolUseAssistantAgent` class in AgentChat. The agent is aware of the tools it can use by passing a `tools_schema` attribute to the class, which is passed to the `model_client` when the agent generates a response.\\n\",\n+    \"-  An agent Team is defined that takes a list of `tools`.  Effectively, the `ToolUseAssistantAgent` can generate messages that call tools, and the team is responsible executing those tool calls and returning the results.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 8,\n-      \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"name\": \"stdout\",\n-          \"output_type\": \"stream\",\n-          \"text\": [\n-            \"\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:36.869317]:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"Who is the receipient of the 2023 Nobel Prize in Physics?\\n\",\n-            \"From: user\"\n-          ]\n-        },\n-        {\n-          \"name\": \"stdout\",\n-          \"output_type\": \"stream\",\n-          \"text\": [\n-            \"\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:37.856066], WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionCall(id='call_bdLqS1msbHCy5IMGYaata5vs', arguments='{\\\"query\\\":\\\"2023 Nobel Prize in Physics\\\"}', name='wikipedia')]\\n\",\n-            \"From: WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:38.518288], tool_agent_for_WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionExecutionResult(content='Page: Nobel Prize in Physics\\\\nSummary: The Nobel Prize in Physics (Swedish: Nobelpriset i fysik) is a', call_id='call_bdLqS1msbHCy5IMGYaata5vs')]\\n\",\n-            \"From: tool_agent_for_WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:39.070911], WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionCall(id='call_BFXGGeuBbOQ1LPb4f0NiNva2', arguments='{\\\"query\\\":\\\"2023 Nobel Prize in Physics recipients\\\"}', name='wikipedia')]\\n\",\n-            \"From: WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:39.727147], tool_agent_for_WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionExecutionResult(content='Page: Nobel Prize in Physics\\\\nSummary: The Nobel Prize in Physics (Swedish: Nobelpriset i fysik) is a', call_id='call_BFXGGeuBbOQ1LPb4f0NiNva2')]\\n\",\n-            \"From: tool_agent_for_WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:40.746467], WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionCall(id='call_iH2gkY5A2LiQTiy2eh86XpP5', arguments='{\\\"query\\\": \\\"2023 Nobel Prize in Physics winners\\\"}', name='wikipedia'), FunctionCall(id='call_rJXgJQiAKoD7yrymNJCsQA9N', arguments='{\\\"query\\\": \\\"Nobel Prize in Physics\\\"}', name='wikipedia')]\\n\",\n-            \"From: WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:41.469348], tool_agent_for_WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionExecutionResult(content='Page: Nobel Prize in Physics\\\\nSummary: The Nobel Prize in Physics (Swedish: Nobelpriset i fysik) is a', call_id='call_iH2gkY5A2LiQTiy2eh86XpP5'), FunctionExecutionResult(content='Page: Nobel Prize in Physics\\\\nSummary: The Nobel Prize in Physics (Swedish: Nobelpriset i fysik) is a', call_id='call_rJXgJQiAKoD7yrymNJCsQA9N')]\\n\",\n-            \"From: tool_agent_for_WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:42.576718], WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"I couldn't find specific information about the recipients of the 2023 Nobel Prize in Physics. You might want to check a reliable news source or the official Nobel Prize website for the most accurate and up-to-date details. \\n\",\n-            \"\\n\",\n-            \"TERMINATE\\n\",\n-            \"From: WikiPedia_Assistant\"\n-          ]\n-        }\n-      ],\n-      \"source\": [\n-        \"wikipedia_assistant = ToolUseAssistantAgent(\\n\",\n-        \"    \\\"WikiPedia_Assistant\\\",\\n\",\n-        \"    model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\"),\\n\",\n-        \"    registered_tools=[langchain_wikipedia_tool],\\n\",\n-        \")\\n\",\n-        \"team = RoundRobinGroupChat([wikipedia_assistant])\\n\",\n-        \"result = await team.run(\\\"Who was the first president of the United States?\\\")\\n\",\n-        \"\\n\",\n-        \"# print(result)\"\n-      ]\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------- \\n\",\n+      \"\\u001b[91m[2024-10-08T20:34:31.935149]:\\u001b[0m\\n\",\n+      \"\\n\",\n+      \"What's the weather in New York?\"\n+     ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": []\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------- \\n\",\n+      \"\\u001b[91m[2024-10-08T20:34:33.080494], Weather_Assistant:\\u001b[0m\\n\",\n+      \"\\n\",\n+      \"The weather in New York is 72 degrees and sunny. \\n\",\n+      \"\\n\",\n+      \"TERMINATE\"\n+     ]\n     }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"agnext\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.11.9\"\n+   ],\n+   \"source\": [\n+    \"assistant = ToolUseAssistantAgent(\\n\",\n+    \"    \\\"Weather_Assistant\\\",\\n\",\n+    \"    model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\"),\\n\",\n+    \"    registered_tools=[get_weather_tool],\\n\",\n+    \")\\n\",\n+    \"team = RoundRobinGroupChat([assistant])\\n\",\n+    \"result = await team.run(\\\"What's the weather in New York?\\\", termination_condition=StopMessageTermination())\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Using Langchain Tools \\n\",\n+    \"\\n\",\n+    \"AutoGen also provides direct support for tools from LangChain via the `autogen_ext`  package.\\n\",\n+    \"\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 6,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# pip install langchain, langchain-community, wikipedia , autogen-ext\\n\",\n+    \"\\n\",\n+    \"import wikipedia\\n\",\n+    \"from autogen_ext.tools.langchain import LangChainToolAdapter\\n\",\n+    \"from langchain.tools import WikipediaQueryRun\\n\",\n+    \"from langchain_community.utilities import WikipediaAPIWrapper\\n\",\n+    \"\\n\",\n+    \"api_wrapper = WikipediaAPIWrapper(wiki_client=wikipedia, top_k_results=1, doc_content_chars_max=100)\\n\",\n+    \"tool = WikipediaQueryRun(api_wrapper=api_wrapper)\\n\",\n+    \"\\n\",\n+    \"langchain_wikipedia_tool = LangChainToolAdapter(tool)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 7,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------- \\n\",\n+      \"\\u001b[91m[2024-10-08T20:44:08.218758]:\\u001b[0m\\n\",\n+      \"\\n\",\n+      \"Who was the first president of the United States?\\n\",\n+      \"--------------------------------------------------------------------------- \\n\",\n+      \"\\u001b[91m[2024-10-08T20:44:11.240067], WikiPedia_Assistant:\\u001b[0m\\n\",\n+      \"\\n\",\n+      \"The first president of the United States was George Washington, who served from April 30, 1789, to March 4, 1797. \\n\",\n+      \"\\n\",\n+      \"TERMINATE\"\n+     ]\n     }\n+   ],\n+   \"source\": [\n+    \"wikipedia_assistant = ToolUseAssistantAgent(\\n\",\n+    \"    \\\"WikiPedia_Assistant\\\",\\n\",\n+    \"    model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\"),\\n\",\n+    \"    registered_tools=[langchain_wikipedia_tool],\\n\",\n+    \")\\n\",\n+    \"team = RoundRobinGroupChat([wikipedia_assistant])\\n\",\n+    \"result = await team.run(\\n\",\n+    \"    \\\"Who was the first president of the United States?\\\", termination_condition=StopMessageTermination()\\n\",\n+    \")\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \".venv\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n   },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 2\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.12.6\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n }\n@@ -3,7 +3,7 @@\n `````{tab-item} AgentChat (v0.4x)\n ```python\n from autogen_agentchat.agents import CodeExecutorAgent, CodingAssistantAgent\n-from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\n+from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n from autogen_core.components.code_executor import DockerCommandLineCodeExecutor\n from autogen_core.components.models import OpenAIChatCompletionClient\n \n@@ -14,9 +14,9 @@ async with DockerCommandLineCodeExecutor(work_dir=\"coding\") as code_executor:\n     )\n     group_chat = RoundRobinGroupChat([coding_assistant_agent, code_executor_agent])\n     result = await group_chat.run(\n-        task=\"Create a plot of NVIDIA and TESLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\"\n+        task=\"Create a plot of NVIDIA and TESLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\",\n+        termination_condition=StopMessageTermination(),\n     )\n-    print(result)\n ```\n `````\n \n@@ -0,0 +1,103 @@\n+---\n+title: New AutoGen Architecture Preview\n+authors:\n+  - autogen-team\n+tags: [AutoGen]\n+---\n+\n+# New AutoGen Architecture Preview\n+\n+<center>\n+\n+![What are they doing?](img/robots.jpeg)\n+\n+</center>\n+\n+One year ago, we launched AutoGen, a programming framework designed to build\n+agentic AI systems. The release of AutoGen sparked massive interest within the\n+developer community. As an early release, it provided us with a unique\n+opportunity to engage deeply with users, gather invaluable feedback, and learn\n+from a diverse range of use cases and contributions. By listening and engaging\n+with the community, we gained insights into what people were building or\n+attempting to build, how they were approaching the creation of agentic systems,\n+and where they were struggling. This experience was both humbling and\n+enlightening, revealing significant opportunities for improvement in our initial\n+design, especially for power users developing production-level applications with\n+AutoGen.\n+\n+Through engagements with the community, we learned many lessons:\n+\n+- Developers value modular and reusable agents. For example, our built-in agents\n+  that could be directly plugged in or easily customized for specific use cases\n+  were particularly popular. At the same time, there was a desire for more\n+  customizability, such as integrating custom agents built using other\n+  programming languages or frameworks.\n+- Chat-based agent-to-agent communication was an intuitive collaboration\n+  pattern, making it easy for developers to get started and involve humans in\n+  the loop. As developers began to employ agents in a wider range of scenarios,\n+  they sought more flexibility in collaboration patterns. For instance,\n+  developers wanted to build predictable, ordered workflows with agents, and to\n+  integrate them with new user interfaces that are not chat-based.\n+- Although it was easy for developers to get started with AutoGen, debugging and\n+  scaling agent teams applications proved more challenging.\n+- There were many opportunities for improving code quality.\n+\n+These learnings, along with many others from other agentic efforts across\n+Microsoft, prompted us to take a step back and lay the groundwork for a new\n+direction. A few months ago, we started dedicating time to distilling these\n+learnings into a roadmap for the future of AutoGen. This led to the development\n+of AutoGen 0.4, a complete redesign of the framework from the foundation up.\n+AutoGen 0.4 embraces the actor model of computing to support distributed, highly\n+scalable, event-driven agentic systems. This approach offers many advantages,\n+such as:\n+\n+- **Composability**. Systems designed in this way are more composable, allowing\n+  developers to bring their own agents implemented in different frameworks or\n+  programming languages and to build more powerful systems using complex agentic\n+  patterns.\n+- **Flexibility**. It allows for the creation of both deterministic, ordered\n+  workflows and event-driven or decentralized workflows, enabling customers to\n+  bring their own orchestration or integrate with other systems more easily. It\n+  also opens more opportunities for human-in-the-loop scenarios, both active and\n+  reactive.\n+- **Debugging and Observability**. Event-driven communication moves message delivery\n+  away from agents to a centralized component, making it easier to observe and\n+  debug their activities regardless of agent implementation.\n+- **Scalability**. An event-based architecture enables distributed and\n+  cloud-deployed agents, which is essential for building scalable AI services\n+  and applications.\n+\n+Today, we are delighted to share our progress and invite everyone to collaborate\n+with us and provide feedback to evolve AutoGen and help shape the future of\n+multi-agent systems.\n+\n+As the first step, we are opening a [pull request](https://github.com/microsoft/autogen/pull/3600) into the main branch with the\n+current state of development of 0.4. After approximately a week, we plan to\n+merge this into main and continue development. There's still a lot left to do\n+before 0.4 is ready for release though, so keep in mind this is a work in\n+progress.\n+\n+Starting in AutoGen 0.4, the project will have three main libraries:\n+\n+- **Core** - the building blocks for an event-driven agentic system.\n+- **AgentChat** - a task-driven, high-level API built with core, including group\n+  chat, code execution, pre-built agents, and more. This is the most similar API\n+  to AutoGen [0.2](https://github.com/microsoft/autogen/tree/0.2) and will be the easiest API to migrate to.\n+- **Extensions** - implementations of core interfaces and third-party integrations\n+  (e.g., Azure code executor and OpenAI model client).\n+\n+AutoGen [0.2](https://github.com/microsoft/autogen/tree/0.2) is still available, developed and maintained out of the [0.2 branch](https://github.com/microsoft/autogen/tree/0.2).\n+For everyone looking for a stable version, we recommend continuing to use [0.2](https://github.com/microsoft/autogen/tree/0.2)\n+for the time being. It can be installed using:\n+\n+```sh\n+pip install autogen-agentchat~=0.2\n+```\n+\n+This new package name was used to align with the new packages that will come with 0.4:\n+`autogen-core`, `autogen-agentchat`, and `autogen-ext`.\n+\n+Lastly, we will be using [GitHub\n+Discussion](https://github.com/microsoft/autogen/discussions) as the official\n+community forum for the new version and, going forward, all discussions related\n+to the AutoGen project. We look forward to meeting you there.\n",
      "head_commit_sha": "64365b6835ffb18c0f2abde26d70989b6ef66300",
      "name": "Stage 3",
      "patches": [
        {
          "patch": "@@ -1,57 +0,0 @@\n-### Description\n-<!-- A clear and concise description of the issue or feature request. -->\n-\n-### Environment\n-- AutoGen version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-- Python version: <!-- Specify the Python version (e.g., 3.8) -->\n-- Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-\n-### Steps to Reproduce (for bugs)\n-<!-- Provide detailed steps to reproduce the issue. Include code snippets, configuration files, or any other relevant information. -->\n-\n-1. Step 1\n-2. Step 2\n-3. ...\n-\n-### Expected Behavior\n-<!-- Describe what you expected to happen. -->\n-\n-### Actual Behavior\n-<!-- Describe what actually happened. Include any error messages, stack traces, or unexpected behavior. -->\n-\n-### Screenshots / Logs (if applicable)\n-<!-- If relevant, include screenshots or logs that help illustrate the issue. -->\n-\n-### Additional Information\n-<!-- Include any additional information that might be helpful, such as specific configurations, data samples, or context about the environment. -->\n-\n-### Possible Solution (if you have one)\n-<!-- If you have suggestions on how to address the issue, provide them here. -->\n-\n-### Is this a Bug or Feature Request?\n-<!-- Choose one: Bug | Feature Request -->\n-\n-### Priority\n-<!-- Choose one: High | Medium | Low -->\n-\n-### Difficulty\n-<!-- Choose one: Easy | Moderate | Hard -->\n-\n-### Any related issues?\n-<!-- If this is related to another issue, reference it here. -->\n-\n-### Any relevant discussions?\n-<!-- If there are any discussions or forum threads related to this issue, provide links. -->\n-\n-### Checklist\n-<!-- Please check the items that you have completed -->\n-- [ ] I have searched for similar issues and didn't find any duplicates.\n-- [ ] I have provided a clear and concise description of the issue.\n-- [ ] I have included the necessary environment details.\n-- [ ] I have outlined the steps to reproduce the issue.\n-- [ ] I have included any relevant logs or screenshots.\n-- [ ] I have indicated whether this is a bug or a feature request.\n-- [ ] I have set the priority and difficulty levels.\n-\n-### Additional Comments\n-<!-- Any additional comments or context that you think would be helpful. -->",
          "path": ".github/ISSUE_TEMPLATE.md"
        },
        {
          "patch": "@@ -1,53 +1,55 @@\n name: Bug Report\n-description: File a bug report\n-title: \"[Bug]: \"\n+description: Report a bug\n labels: [\"bug\"]\n \n body:\n   - type: textarea\n-    id: description\n     attributes:\n-      label: Describe the bug\n-      description: A clear and concise description of what the bug is.\n-      placeholder: What went wrong?\n+      label: What happened?\n+      description: Please provide as much information as possible, this helps us address the issue.\n+    validations:\n+      required: true\n   - type: textarea\n-    id: reproduce\n     attributes:\n-      label: Steps to reproduce\n-      description: |\n-        Steps to reproduce the behavior:\n-\n-        1. Step 1\n-        2. Step 2\n-        3. ...\n-        4. See error\n-      placeholder: How can we replicate the issue?\n+      label: What did you expect to happen?\n+    validations:\n+      required: true\n   - type: textarea\n-    id: modelused\n     attributes:\n-      label: Model Used\n-      description: A description of the model that was used when the error was encountered\n+      label: How can we reproduce it (as minimally and precisely as possible)?\n+      description: Please provide steps to reproduce. Provide code that can be run if possible.\n+    validations:\n+      required: true\n+  - type: input\n+    attributes:\n+      label: AutoGen version\n+      description: What version or commit of the library was used\n+    validations:\n+      required: true\n+  - type: dropdown\n+    attributes:\n+      label: Which package was this bug in\n+      options:\n+        - Core\n+        - AgentChat\n+        - Extensions\n+        - AutoGen Studio\n+        - Magentic One\n+        - AutoGen Bench\n+        - Other\n+    validations:\n+      required: true\n+  - type: input\n+    attributes:\n+      label: Model used\n+      description: If a model was used, please describe it here, indicating whether it is a local model or a cloud-hosted model\n       placeholder: gpt-4, mistral-7B etc\n-  - type: textarea\n-    id: expected_behavior\n+  - type: input\n     attributes:\n-      label: Expected Behavior\n-      description: A clear and concise description of what you expected to happen.\n-      placeholder: What should have happened?\n-  - type: textarea\n-    id: screenshots\n+      label: Python version\n+  - type: input\n     attributes:\n-      label: Screenshots and logs\n-      description: If applicable, add screenshots and logs to help explain your problem.\n-      placeholder: Add screenshots here\n+      label: Operating system\n   - type: textarea\n-    id: additional_information\n-    attributes:\n-      label: Additional Information\n-      description: |\n-        - AutoGen Version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-        - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-        - Python Version: <!-- Specify the Python version (e.g., 3.8) -->\n-        - Related Issues: <!-- Link to any related issues here (e.g., #1) -->\n-        - Any other relevant information.\n-      placeholder: Any additional details\n+    attributes:\n+      label: Any additional info you think would be helpful for fixing this bug",
          "path": ".github/ISSUE_TEMPLATE/bug_report.yml"
        },
        {
          "patch": "@@ -1 +1,5 @@\n blank_issues_enabled: true\n+contact_links:\n+  - name: Questions or general help \ud83d\udcac\n+    url: https://github.com/microsoft/autogen/discussions\n+    about: Please ask and answer questions here.",
          "path": ".github/ISSUE_TEMPLATE/config.yml"
        },
        {
          "patch": "@@ -1,26 +1,18 @@\n name: Feature Request\n-description: File a feature request\n+description: Request a new feature or enhancement\n labels: [\"enhancement\"]\n-title: \"[Feature Request]: \"\n \n body:\n   - type: textarea\n-    id: problem_description\n     attributes:\n-      label: Is your feature request related to a problem? Please describe.\n-      description: A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n-      placeholder: What problem are you trying to solve?\n+      label: What feature would you like to be added?\n+      description: Please describe the desired feature. Be descriptive, provide examples and if possible, provide a proposed solution.\n+    validations:\n+      required: true\n \n   - type: textarea\n-    id: solution_description\n     attributes:\n-      label: Describe the solution you'd like\n-      description: A clear and concise description of what you want to happen.\n-      placeholder: How do you envision the solution?\n-\n-  - type: textarea\n-    id: additional_context\n-    attributes:\n-      label: Additional context\n-      description: Add any other context or screenshots about the feature request here.\n-      placeholder: Any additional information\n+      label: Why is this needed?\n+      description: Why is it important that this feature is implemented? What problem or need does it solve?\n+    validations:\n+      required: true",
          "path": ".github/ISSUE_TEMPLATE/feature_request.yml"
        },
        {
          "patch": "@@ -1,41 +0,0 @@\n-name: General Issue\n-description: File a general issue\n-title: \"[Issue]: \"\n-labels: []\n-\n-body:\n-  - type: textarea\n-    id: description\n-    attributes:\n-      label: Describe the issue\n-      description: A clear and concise description of what the issue is.\n-      placeholder: What went wrong?\n-  - type: textarea\n-    id: reproduce\n-    attributes:\n-      label: Steps to reproduce\n-      description: |\n-        Steps to reproduce the behavior:\n-\n-        1. Step 1\n-        2. Step 2\n-        3. ...\n-        4. See error\n-      placeholder: How can we replicate the issue?\n-  - type: textarea\n-    id: screenshots\n-    attributes:\n-      label: Screenshots and logs\n-      description: If applicable, add screenshots and logs to help explain your problem.\n-      placeholder: Add screenshots here\n-  - type: textarea\n-    id: additional_information\n-    attributes:\n-      label: Additional Information\n-      description: |\n-        - AutoGen Version: <!-- Specify the AutoGen version (e.g., v0.2.0) -->\n-        - Operating System: <!-- Specify the OS (e.g., Windows 10, Ubuntu 20.04) -->\n-        - Python Version: <!-- Specify the Python version (e.g., 3.8) -->\n-        - Related Issues: <!-- Link to any related issues here (e.g., #1) -->\n-        - Any other relevant information.\n-      placeholder: Any additional details",
          "path": ".github/ISSUE_TEMPLATE/general_issue.yml"
        },
        {
          "patch": "@@ -0,0 +1,18 @@\n+name: Label issues with needs-triage\n+on:\n+  issues:\n+    types:\n+      - reopened\n+      - opened\n+jobs:\n+  label_issues:\n+    runs-on: ubuntu-latest\n+    permissions:\n+      issues: write\n+    steps:\n+      - run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GH_REPO: ${{ github.repository }}\n+          NUMBER: ${{ github.event.issue.number }}\n+          LABELS: needs-triage",
          "path": ".github/workflows/issue-needs-triage.yml"
        },
        {
          "patch": "@@ -107,7 +107,7 @@ and running on your machine.\n \n ```python\n from autogen_agentchat.agents import CodeExecutorAgent, CodingAssistantAgent\n-from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\n+from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n from autogen_core.components.code_executor import DockerCommandLineCodeExecutor\n from autogen_core.components.models import OpenAIChatCompletionClient\n \n@@ -118,9 +118,9 @@ async with DockerCommandLineCodeExecutor(work_dir=\"coding\") as code_executor:\n     )\n     group_chat = RoundRobinGroupChat([coding_assistant_agent, code_executor_agent])\n     result = await group_chat.run(\n-        task=\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\"\n+        task=\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\",\n+        termination_condition=StopMessageTermination(),\n     )\n-    print(result)\n ```\n \n ### C#",
          "path": "README.md"
        },
        {
          "patch": "@@ -36,7 +36,7 @@ def __init__(\n         registered_tools: List[Tool],\n         *,\n         description: str = \"An agent that provides assistance with ability to use tools.\",\n-        system_message: str = \"You are a helpful AI assistant. Solve tasks using your tools.\",\n+        system_message: str = \"You are a helpful AI assistant. Solve tasks using your tools. Reply with 'TERMINATE' when the task has been completed.\",\n     ):\n         super().__init__(name=name, description=description, registered_tools=registered_tools)\n         self._model_client = model_client",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/agents/_tool_use_assistant_agent.py"
        },
        {
          "patch": "@@ -1,8 +1,17 @@\n from ._logging import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME, ConsoleLogHandler, FileLogHandler\n+from ._termination import MaxMessageTermination, StopMessageTermination, TerminationCondition, TextMentionTermination\n+from .group_chat._round_robin_group_chat import RoundRobinGroupChat\n+from .group_chat._selector_group_chat import SelectorGroupChat\n \n __all__ = [\n     \"TRACE_LOGGER_NAME\",\n     \"EVENT_LOGGER_NAME\",\n     \"ConsoleLogHandler\",\n     \"FileLogHandler\",\n+    \"TerminationCondition\",\n+    \"MaxMessageTermination\",\n+    \"TextMentionTermination\",\n+    \"StopMessageTermination\",\n+    \"RoundRobinGroupChat\",\n+    \"SelectorGroupChat\",\n ]",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/__init__.py"
        },
        {
          "patch": "@@ -4,6 +4,7 @@\n \n from ..agents import ChatMessage\n from ._logging import EVENT_LOGGER_NAME, ConsoleLogHandler\n+from ._termination import TerminationCondition\n \n logger = logging.getLogger(EVENT_LOGGER_NAME)\n logger.setLevel(logging.INFO)\n@@ -14,9 +15,10 @@\n @dataclass\n class TeamRunResult:\n     messages: List[ChatMessage]\n+    \"\"\"The messages generated by the team.\"\"\"\n \n \n class BaseTeam(Protocol):\n-    async def run(self, task: str) -> TeamRunResult:\n-        \"\"\"Run the team and return the result.\"\"\"\n+    async def run(self, task: str, *, termination_condition: TerminationCondition | None = None) -> TeamRunResult:\n+        \"\"\"Run the team on a given task until the termination condition is met.\"\"\"\n         ...",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_base_team.py"
        },
        {
          "patch": "@@ -61,3 +61,15 @@ class SelectSpeakerEvent(BaseModel):\n     \"\"\"The agent ID that selected the speaker.\"\"\"\n \n     model_config = ConfigDict(arbitrary_types_allowed=True)\n+\n+\n+class TerminationEvent(BaseModel):\n+    \"\"\"An event for terminating a conversation.\"\"\"\n+\n+    agent_message: StopMessage\n+    \"\"\"The stop message that terminates the conversation.\"\"\"\n+\n+    source: AgentId\n+    \"\"\"The agent ID that triggered the termination.\"\"\"\n+\n+    model_config = ConfigDict(arbitrary_types_allowed=True)",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_events.py"
        },
        {
          "patch": "@@ -3,113 +3,85 @@\n import sys\n from dataclasses import asdict, is_dataclass\n from datetime import datetime\n-from typing import Any, Dict, List, Union\n+from typing import Any\n \n-from autogen_core.base import AgentId\n-from autogen_core.components import FunctionCall, Image\n-from autogen_core.components.models import FunctionExecutionResult\n-\n-from ..agents import ChatMessage, MultiModalMessage, StopMessage, TextMessage, ToolCallMessage, ToolCallResultMessage\n-from ._events import ContentPublishEvent, SelectSpeakerEvent, ToolCallEvent, ToolCallResultEvent\n+from ..agents import ChatMessage, StopMessage, TextMessage\n+from ._events import ContentPublishEvent, SelectSpeakerEvent, TerminationEvent, ToolCallEvent, ToolCallResultEvent\n \n TRACE_LOGGER_NAME = \"autogen_agentchat\"\n EVENT_LOGGER_NAME = \"autogen_agentchat.events\"\n-ContentType = Union[str, List[Union[str, Image]], List[FunctionCall], List[FunctionExecutionResult]]\n-\n \n-class BaseLogHandler(logging.Handler):\n-    def serialize_content(\n-        self,\n-        content: Union[ContentType, ChatMessage],\n-    ) -> Union[List[Any], Dict[str, Any], str]:\n-        if isinstance(content, (str, list)):\n-            return content\n-        elif isinstance(content, (TextMessage, MultiModalMessage, ToolCallMessage, ToolCallResultMessage, StopMessage)):\n-            return asdict(content)\n-        elif isinstance(content, Image):\n-            return {\"type\": \"image\", \"data\": content.data_uri}\n-        elif isinstance(content, FunctionCall):\n-            return {\"type\": \"function_call\", \"name\": content.name, \"arguments\": content.arguments}\n-        elif isinstance(content, FunctionExecutionResult):\n-            return {\"type\": \"function_execution_result\", \"content\": content.content}\n-        return str(content)\n \n+class ConsoleLogHandler(logging.Handler):\n     @staticmethod\n-    def json_serializer(obj: Any) -> Any:\n-        if is_dataclass(obj) and not isinstance(obj, type):\n-            return asdict(obj)\n-        elif isinstance(obj, type):\n-            return str(obj)\n-        return str(obj)\n-\n-\n-class ConsoleLogHandler(BaseLogHandler):\n-    def _format_chat_message(\n-        self,\n-        *,\n-        source_agent_id: AgentId | None,\n-        message: ChatMessage,\n-        timestamp: str,\n-    ) -> str:\n-        body = f\"{self.serialize_content(message.content)}\"\n-        if source_agent_id is None:\n-            console_message = f\"\\n{'-'*75} \\n\" f\"\\033[91m[{timestamp}]:\\033[0m\\n\" f\"\\n{body}\"\n+    def serialize_chat_message(message: ChatMessage) -> str:\n+        if isinstance(message, TextMessage | StopMessage):\n+            return message.content\n         else:\n-            # Display the source agent type rather than agent ID for better readability.\n-            # Also in AgentChat the agent type is unique for each agent.\n-            console_message = f\"\\n{'-'*75} \\n\" f\"\\033[91m[{timestamp}], {source_agent_id.type}:\\033[0m\\n\" f\"\\n{body}\"\n-        return console_message\n+            d = message.model_dump()\n+            assert \"content\" in d\n+            return json.dumps(d[\"content\"], indent=2)\n \n     def emit(self, record: logging.LogRecord) -> None:\n         ts = datetime.fromtimestamp(record.created).isoformat()\n         if isinstance(record.msg, ContentPublishEvent):\n-            sys.stdout.write(\n-                self._format_chat_message(\n-                    source_agent_id=record.msg.source,\n-                    message=record.msg.agent_message,\n-                    timestamp=ts,\n+            if record.msg.source is None:\n+                sys.stdout.write(\n+                    f\"\\n{'-'*75} \\n\"\n+                    f\"\\033[91m[{ts}]:\\033[0m\\n\"\n+                    f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n+                )\n+            else:\n+                sys.stdout.write(\n+                    f\"\\n{'-'*75} \\n\"\n+                    f\"\\033[91m[{ts}], {record.msg.source.type}:\\033[0m\\n\"\n+                    f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n                 )\n-            )\n             sys.stdout.flush()\n         elif isinstance(record.msg, ToolCallEvent):\n             sys.stdout.write(\n                 f\"\\n{'-'*75} \\n\"\n                 f\"\\033[91m[{ts}], Tool Call:\\033[0m\\n\"\n-                f\"\\n{self.serialize_content(record.msg.agent_message)}\"\n+                f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n             )\n             sys.stdout.flush()\n         elif isinstance(record.msg, ToolCallResultEvent):\n             sys.stdout.write(\n                 f\"\\n{'-'*75} \\n\"\n                 f\"\\033[91m[{ts}], Tool Call Result:\\033[0m\\n\"\n-                f\"\\n{self.serialize_content(record.msg.agent_message)}\"\n+                f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n             )\n             sys.stdout.flush()\n         elif isinstance(record.msg, SelectSpeakerEvent):\n+            sys.stdout.write(\n+                f\"\\n{'-'*75} \\n\" f\"\\033[91m[{ts}], Selected Next Speaker:\\033[0m\\n\" f\"\\n{record.msg.selected_speaker}\"\n+            )\n+            sys.stdout.flush()\n+        elif isinstance(record.msg, TerminationEvent):\n             sys.stdout.write(\n                 f\"\\n{'-'*75} \\n\"\n-                f\"\\033[91m[{ts}], {record.msg.source.type}:\\033[0m\\n\"\n-                f\"\\nSelected next speaker: {record.msg.selected_speaker}\"\n+                f\"\\033[91m[{ts}], Termination:\\033[0m\\n\"\n+                f\"\\n{self.serialize_chat_message(record.msg.agent_message)}\"\n             )\n             sys.stdout.flush()\n         else:\n             raise ValueError(f\"Unexpected log record: {record.msg}\")\n \n \n-class FileLogHandler(BaseLogHandler):\n+class FileLogHandler(logging.Handler):\n     def __init__(self, filename: str) -> None:\n         super().__init__()\n         self.filename = filename\n         self.file_handler = logging.FileHandler(filename)\n \n     def emit(self, record: logging.LogRecord) -> None:\n         ts = datetime.fromtimestamp(record.created).isoformat()\n-        if isinstance(record.msg, ContentPublishEvent | ToolCallEvent | ToolCallResultEvent):\n+        if isinstance(record.msg, ContentPublishEvent | ToolCallEvent | ToolCallResultEvent | TerminationEvent):\n             log_entry = json.dumps(\n                 {\n                     \"timestamp\": ts,\n                     \"source\": record.msg.source,\n-                    \"agent_message\": self.serialize_content(record.msg.agent_message),\n+                    \"agent_message\": record.msg.agent_message.model_dump(),\n                     \"type\": record.msg.__class__.__name__,\n                 },\n                 default=self.json_serializer,\n@@ -140,3 +112,11 @@ def emit(self, record: logging.LogRecord) -> None:\n     def close(self) -> None:\n         self.file_handler.close()\n         super().close()\n+\n+    @staticmethod\n+    def json_serializer(obj: Any) -> Any:\n+        if is_dataclass(obj) and not isinstance(obj, type):\n+            return asdict(obj)\n+        elif isinstance(obj, type):\n+            return str(obj)\n+        return str(obj)",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_logging.py"
        },
        {
          "patch": "@@ -0,0 +1,215 @@\n+import asyncio\n+from abc import ABC, abstractmethod\n+from typing import List, Sequence\n+\n+from ..agents import ChatMessage, MultiModalMessage, StopMessage, TextMessage\n+\n+\n+class TerminatedException(BaseException): ...\n+\n+\n+class TerminationCondition(ABC):\n+    \"\"\"A stateful condition that determines when a conversation should be terminated.\n+\n+    A termination condition is a callable that takes a sequence of ChatMessage objects\n+    since the last time the condition was called, and returns a StopMessage if the\n+    conversation should be terminated, or None otherwise.\n+    Once a termination condition has been reached, it must be reset before it can be used again.\n+\n+    Termination conditions can be combined using the AND and OR operators.\n+\n+    Example:\n+\n+        .. code-block:: python\n+\n+            from autogen_agentchat.teams import MaxTurnsTermination, TextMentionTermination\n+\n+            # Terminate the conversation after 10 turns or if the text \"TERMINATE\" is mentioned.\n+            cond1 = MaxTurnsTermination(10) | TextMentionTermination(\"TERMINATE\")\n+\n+            # Terminate the conversation after 10 turns and if the text \"TERMINATE\" is mentioned.\n+            cond2 = MaxTurnsTermination(10) & TextMentionTermination(\"TERMINATE\")\n+\n+            ...\n+\n+            # Reset the termination condition.\n+            await cond1.reset()\n+            await cond2.reset()\n+    \"\"\"\n+\n+    @property\n+    @abstractmethod\n+    def terminated(self) -> bool:\n+        \"\"\"Check if the termination condition has been reached\"\"\"\n+        ...\n+\n+    @abstractmethod\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        \"\"\"Check if the conversation should be terminated based on the messages received\n+        since the last time the condition was called.\n+        Return a StopMessage if the conversation should be terminated, or None otherwise.\n+\n+        Args:\n+            messages: The messages received since the last time the condition was called.\n+\n+        Returns:\n+            StopMessage | None: A StopMessage if the conversation should be terminated, or None otherwise.\n+\n+        Raises:\n+            TerminatedException: If the termination condition has already been reached.\"\"\"\n+        ...\n+\n+    @abstractmethod\n+    async def reset(self) -> None:\n+        \"\"\"Reset the termination condition.\"\"\"\n+        ...\n+\n+    def __and__(self, other: \"TerminationCondition\") -> \"TerminationCondition\":\n+        \"\"\"Combine two termination conditions with an AND operation.\"\"\"\n+        return _AndTerminationCondition(self, other)\n+\n+    def __or__(self, other: \"TerminationCondition\") -> \"TerminationCondition\":\n+        \"\"\"Combine two termination conditions with an OR operation.\"\"\"\n+        return _OrTerminationCondition(self, other)\n+\n+\n+class _AndTerminationCondition(TerminationCondition):\n+    def __init__(self, *conditions: TerminationCondition) -> None:\n+        self._conditions = conditions\n+        self._stop_messages: List[StopMessage] = []\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return all(condition.terminated for condition in self._conditions)\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self.terminated:\n+            raise TerminatedException(\"Termination condition has already been reached.\")\n+        # Check all remaining conditions.\n+        stop_messages = await asyncio.gather(\n+            *[condition(messages) for condition in self._conditions if not condition.terminated]\n+        )\n+        # Collect stop messages.\n+        for stop_message in stop_messages:\n+            if stop_message is not None:\n+                self._stop_messages.append(stop_message)\n+        if any(stop_message is None for stop_message in stop_messages):\n+            # If any remaining condition has not reached termination, it is not terminated.\n+            return None\n+        content = \", \".join(stop_message.content for stop_message in self._stop_messages)\n+        source = \", \".join(stop_message.source for stop_message in self._stop_messages)\n+        return StopMessage(content=content, source=source)\n+\n+    async def reset(self) -> None:\n+        for condition in self._conditions:\n+            await condition.reset()\n+        self._stop_messages.clear()\n+\n+\n+class _OrTerminationCondition(TerminationCondition):\n+    def __init__(self, *conditions: TerminationCondition) -> None:\n+        self._conditions = conditions\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return any(condition.terminated for condition in self._conditions)\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self.terminated:\n+            raise RuntimeError(\"Termination condition has already been reached\")\n+        stop_messages = await asyncio.gather(*[condition(messages) for condition in self._conditions])\n+        if any(stop_message is not None for stop_message in stop_messages):\n+            content = \", \".join(stop_message.content for stop_message in stop_messages if stop_message is not None)\n+            source = \", \".join(stop_message.source for stop_message in stop_messages if stop_message is not None)\n+            return StopMessage(content=content, source=source)\n+        return None\n+\n+    async def reset(self) -> None:\n+        for condition in self._conditions:\n+            await condition.reset()\n+\n+\n+class StopMessageTermination(TerminationCondition):\n+    \"\"\"Terminate the conversation if a StopMessage is received.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self._terminated = False\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return self._terminated\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self._terminated:\n+            raise TerminatedException(\"Termination condition has already been reached\")\n+        for message in messages:\n+            if isinstance(message, StopMessage):\n+                self._terminated = True\n+                return StopMessage(content=\"Stop message received\", source=\"StopMessageTermination\")\n+        return None\n+\n+    async def reset(self) -> None:\n+        self._terminated = False\n+\n+\n+class MaxMessageTermination(TerminationCondition):\n+    \"\"\"Terminate the conversation after a maximum number of messages have been exchanged.\n+\n+    Args:\n+        max_messages: The maximum number of messages allowed in the conversation.\n+    \"\"\"\n+\n+    def __init__(self, max_messages: int) -> None:\n+        self._max_messages = max_messages\n+        self._message_count = 0\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return self._message_count >= self._max_messages\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self.terminated:\n+            raise TerminatedException(\"Termination condition has already been reached\")\n+        self._message_count += len(messages)\n+        if self._message_count >= self._max_messages:\n+            return StopMessage(\n+                content=f\"Maximal number of messages {self._max_messages} reached, current message count: {self._message_count}\",\n+                source=\"MaxMessageTermination\",\n+            )\n+        return None\n+\n+    async def reset(self) -> None:\n+        self._message_count = 0\n+\n+\n+class TextMentionTermination(TerminationCondition):\n+    \"\"\"Terminate the conversation if a specific text is mentioned.\n+\n+    Args:\n+        text: The text to look for in the messages.\n+    \"\"\"\n+\n+    def __init__(self, text: str) -> None:\n+        self._text = text\n+        self._terminated = False\n+\n+    @property\n+    def terminated(self) -> bool:\n+        return self._terminated\n+\n+    async def __call__(self, messages: Sequence[ChatMessage]) -> StopMessage | None:\n+        if self._terminated:\n+            raise TerminatedException(\"Termination condition has already been reached\")\n+        for message in messages:\n+            if isinstance(message, TextMessage | StopMessage) and self._text in message.content:\n+                self._terminated = True\n+                return StopMessage(content=f\"Text '{self._text}' mentioned\", source=\"TextMentionTermination\")\n+            elif isinstance(message, MultiModalMessage):\n+                for item in message.content:\n+                    if isinstance(item, str) and self._text in item:\n+                        self._terminated = True\n+                        return StopMessage(content=f\"Text '{self._text}' mentioned\", source=\"TextMentionTermination\")\n+        return None\n+\n+    async def reset(self) -> None:\n+        self._terminated = False",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_termination.py"
        },
        {
          "patch": "@@ -1,4 +0,0 @@\n-from ._round_robin_group_chat import RoundRobinGroupChat\n-from ._selector_group_chat import SelectorGroupChat\n-\n-__all__ = [\"RoundRobinGroupChat\", \"SelectorGroupChat\"]",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/group_chat/__init__.py"
        },
        {
          "patch": "@@ -8,11 +8,10 @@\n from autogen_core.components.tool_agent import ToolAgent\n from autogen_core.components.tools import Tool\n \n-from autogen_agentchat.agents._base_chat_agent import ChatMessage\n-\n-from ...agents import BaseChatAgent, BaseToolUseChatAgent, TextMessage\n+from ...agents import BaseChatAgent, BaseToolUseChatAgent, ChatMessage, TextMessage\n from .._base_team import BaseTeam, TeamRunResult\n from .._events import ContentPublishEvent, ContentRequestEvent\n+from .._termination import TerminationCondition\n from ._base_chat_agent_container import BaseChatAgentContainer\n from ._base_group_chat_manager import BaseGroupChatManager\n \n@@ -45,6 +44,7 @@ def _create_group_chat_manager_factory(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n     ) -> Callable[[], BaseGroupChatManager]: ...\n \n     def _create_participant_factory(\n@@ -69,8 +69,10 @@ def _factory() -> ToolAgent:\n \n         return _factory\n \n-    async def run(self, task: str) -> TeamRunResult:\n+    async def run(self, task: str, *, termination_condition: TerminationCondition | None = None) -> TeamRunResult:\n         \"\"\"Run the team and return the result.\"\"\"\n+        # Create intervention handler for termination.\n+\n         # Create the runtime.\n         runtime = SingleThreadedAgentRuntime()\n \n@@ -122,6 +124,7 @@ async def run(self, task: str) -> TeamRunResult:\n                 group_topic_type=group_topic_type,\n                 participant_topic_types=participant_topic_types,\n                 participant_descriptions=participant_descriptions,\n+                termination_condition=termination_condition,\n             ),\n         )\n         # Add subscriptions for the group chat manager.\n@@ -147,7 +150,7 @@ async def collect_group_chat_messages(\n             type=\"collect_group_chat_messages\",\n             closure=collect_group_chat_messages,\n             subscriptions=lambda: [\n-                TypeSubscription(topic_type=group_topic_type, agent_type=\"collect_group_chat_messages\")\n+                TypeSubscription(topic_type=group_topic_type, agent_type=\"collect_group_chat_messages\"),\n             ],\n         )\n \n@@ -166,4 +169,5 @@ async def collect_group_chat_messages(\n         # Wait for the runtime to stop.\n         await runtime.stop_when_idle()\n \n+        # Return the result.\n         return TeamRunResult(messages=group_chat_messages)",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/group_chat/_base_group_chat.py"
        },
        {
          "patch": "@@ -5,9 +5,9 @@\n from autogen_core.base import MessageContext, TopicId\n from autogen_core.components import event\n \n-from ...agents import StopMessage, TextMessage\n-from .._events import ContentPublishEvent, ContentRequestEvent\n+from .._events import ContentPublishEvent, ContentRequestEvent, TerminationEvent\n from .._logging import EVENT_LOGGER_NAME\n+from .._termination import TerminationCondition\n from ._sequential_routed_agent import SequentialRoutedAgent\n \n event_logger = logging.getLogger(EVENT_LOGGER_NAME)\n@@ -29,6 +29,7 @@ class BaseGroupChatManager(SequentialRoutedAgent, ABC):\n         group_topic_type (str): The topic type of the group chat.\n         participant_topic_types (List[str]): The topic types of the participants.\n         participant_descriptions (List[str]): The descriptions of the participants\n+        termination_condition (TerminationCondition, optional): The termination condition for the group chat. Defaults to None.\n \n     Raises:\n         ValueError: If the number of participant topic types, agent types, and descriptions are not the same.\n@@ -40,6 +41,7 @@ def __init__(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None = None,\n     ):\n         super().__init__(description=\"Group chat manager\")\n         self._parent_topic_type = parent_topic_type\n@@ -57,6 +59,7 @@ def __init__(\n         self._participant_topic_types = participant_topic_types\n         self._participant_descriptions = participant_descriptions\n         self._message_thread: List[ContentPublishEvent] = []\n+        self._termination_condition = termination_condition\n \n     @event\n     async def handle_content_publish(self, message: ContentPublishEvent, ctx: MessageContext) -> None:\n@@ -74,24 +77,25 @@ async def handle_content_publish(self, message: ContentPublishEvent, ctx: Messag\n         # Process event from parent.\n         if ctx.topic_id.type == self._parent_topic_type:\n             self._message_thread.append(message)\n-            await self.publish_message(message, topic_id=group_chat_topic_id)\n+            await self.publish_message(\n+                ContentPublishEvent(agent_message=message.agent_message, source=self.id), topic_id=group_chat_topic_id\n+            )\n             return\n \n         # Process event from the group chat this agent manages.\n         assert ctx.topic_id.type == self._group_topic_type\n         self._message_thread.append(message)\n \n-        # If the message is a stop message, publish the last message as a TextMessage to the parent topic.\n-        # TODO: custom handling the final message.\n-        if isinstance(message.agent_message, StopMessage):\n-            parent_topic_id = TopicId(type=self._parent_topic_type, source=ctx.topic_id.source)\n-            await self.publish_message(\n-                ContentPublishEvent(\n-                    agent_message=TextMessage(content=message.agent_message.content, source=self.metadata[\"type\"])\n-                ),\n-                topic_id=parent_topic_id,\n-            )\n-            return\n+        # Check if the conversation should be terminated.\n+        if self._termination_condition is not None:\n+            stop_message = await self._termination_condition([message.agent_message])\n+            if stop_message is not None:\n+                event_logger.info(TerminationEvent(agent_message=stop_message, source=self.id))\n+                # Reset the termination condition.\n+                await self._termination_condition.reset()\n+                # Stop the group chat.\n+                # TODO: this should be different if the group chat is nested.\n+                return\n \n         # Select a speaker to continue the conversation.\n         speaker_topic_type = await self.select_speaker(self._message_thread)",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/group_chat/_base_group_chat_manager.py"
        },
        {
          "patch": "@@ -2,6 +2,7 @@\n \n from ...agents import BaseChatAgent\n from .._events import ContentPublishEvent\n+from .._termination import TerminationCondition\n from ._base_group_chat import BaseGroupChat\n from ._base_group_chat_manager import BaseGroupChatManager\n \n@@ -15,12 +16,14 @@ def __init__(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n     ) -> None:\n         super().__init__(\n             parent_topic_type,\n             group_topic_type,\n             participant_topic_types,\n             participant_descriptions,\n+            termination_condition,\n         )\n         self._next_speaker_index = 0\n \n@@ -51,23 +54,23 @@ class RoundRobinGroupChat(BaseGroupChat):\n         .. code-block:: python\n \n             from autogen_agentchat.agents import ToolUseAssistantAgent\n-            from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\n+            from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n \n             assistant = ToolUseAssistantAgent(\"Assistant\", model_client=..., registered_tools=...)\n             team = RoundRobinGroupChat([assistant])\n-            await team.run(\"What's the weather in New York?\")\n+            await team.run(\"What's the weather in New York?\", termination_condition=StopMessageTermination())\n \n     A team with multiple participants:\n \n         .. code-block:: python\n \n             from autogen_agentchat.agents import CodingAssistantAgent, CodeExecutorAgent\n-            from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\n+            from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n \n             coding_assistant = CodingAssistantAgent(\"Coding_Assistant\", model_client=...)\n             executor_agent = CodeExecutorAgent(\"Code_Executor\", code_executor=...)\n             team = RoundRobinGroupChat([coding_assistant, executor_agent])\n-            await team.run(\"Write a program that prints 'Hello, world!'\")\n+            await team.run(\"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination())\n \n     \"\"\"\n \n@@ -80,10 +83,15 @@ def _create_group_chat_manager_factory(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n     ) -> Callable[[], RoundRobinGroupChatManager]:\n         def _factory() -> RoundRobinGroupChatManager:\n             return RoundRobinGroupChatManager(\n-                parent_topic_type, group_topic_type, participant_topic_types, participant_descriptions\n+                parent_topic_type,\n+                group_topic_type,\n+                participant_topic_types,\n+                participant_descriptions,\n+                termination_condition,\n             )\n \n         return _factory",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/group_chat/_round_robin_group_chat.py"
        },
        {
          "patch": "@@ -7,6 +7,7 @@\n from ...agents import BaseChatAgent, MultiModalMessage, StopMessage, TextMessage\n from .._events import ContentPublishEvent, SelectSpeakerEvent\n from .._logging import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME\n+from .._termination import TerminationCondition\n from ._base_group_chat import BaseGroupChat\n from ._base_group_chat_manager import BaseGroupChatManager\n \n@@ -24,6 +25,7 @@ def __init__(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n         model_client: ChatCompletionClient,\n         selector_prompt: str,\n         allow_repeated_speaker: bool,\n@@ -33,6 +35,7 @@ def __init__(\n             group_topic_type,\n             participant_topic_types,\n             participant_descriptions,\n+            termination_condition,\n         )\n         self._model_client = model_client\n         self._selector_prompt = selector_prompt\n@@ -164,13 +167,13 @@ class SelectorGroupChat(BaseGroupChat):\n         .. code-block:: python\n \n             from autogen_agentchat.agents import ToolUseAssistantAgent\n-            from autogen_agentchat.teams.group_chat import SelectorGroupChat\n+            from autogen_agentchat.teams import SelectorGroupChat, StopMessageTermination\n \n             travel_advisor = ToolUseAssistantAgent(\"Travel_Advisor\", model_client=..., registered_tools=...)\n             hotel_agent = ToolUseAssistantAgent(\"Hotel_Agent\", model_client=..., registered_tools=...)\n             flight_agent = ToolUseAssistantAgent(\"Flight_Agent\", model_client=..., registered_tools=...)\n             team = SelectorGroupChat([travel_advisor, hotel_agent, flight_agent], model_client=...)\n-            await team.run(\"Book a 3-day trip to new york.\")\n+            await team.run(\"Book a 3-day trip to new york.\", termination_condition=StopMessageTermination())\n     \"\"\"\n \n     def __init__(\n@@ -209,12 +212,14 @@ def _create_group_chat_manager_factory(\n         group_topic_type: str,\n         participant_topic_types: List[str],\n         participant_descriptions: List[str],\n+        termination_condition: TerminationCondition | None,\n     ) -> Callable[[], BaseGroupChatManager]:\n         return lambda: SelectorGroupChatManager(\n             parent_topic_type,\n             group_topic_type,\n             participant_topic_types,\n             participant_descriptions,\n+            termination_condition,\n             self._model_client,\n             self._selector_prompt,\n             self._allow_repeated_speaker,",
          "path": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/group_chat/_selector_group_chat.py"
        },
        {
          "patch": "@@ -1,5 +1,6 @@\n import asyncio\n import json\n+import logging\n import tempfile\n from typing import Any, AsyncGenerator, List, Sequence\n \n@@ -13,7 +14,13 @@\n     TextMessage,\n     ToolUseAssistantAgent,\n )\n-from autogen_agentchat.teams.group_chat import RoundRobinGroupChat, SelectorGroupChat\n+from autogen_agentchat.teams import (\n+    EVENT_LOGGER_NAME,\n+    FileLogHandler,\n+    RoundRobinGroupChat,\n+    SelectorGroupChat,\n+    StopMessageTermination,\n+)\n from autogen_core.base import CancellationToken\n from autogen_core.components import FunctionCall\n from autogen_core.components.code_executor import LocalCommandLineCodeExecutor\n@@ -26,6 +33,10 @@\n from openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall, Function\n from openai.types.completion_usage import CompletionUsage\n \n+logger = logging.getLogger(EVENT_LOGGER_NAME)\n+logger.setLevel(logging.DEBUG)\n+logger.addHandler(FileLogHandler(\"test_group_chat.log\"))\n+\n \n class _MockChatCompletion:\n     def __init__(self, chat_completions: List[ChatCompletion]) -> None:\n@@ -119,7 +130,9 @@ async def test_round_robin_group_chat(monkeypatch: pytest.MonkeyPatch) -> None:\n             \"coding_assistant\", model_client=OpenAIChatCompletionClient(model=model, api_key=\"\")\n         )\n         team = RoundRobinGroupChat(participants=[coding_assistant_agent, code_executor_agent])\n-        result = await team.run(\"Write a program that prints 'Hello, world!'\")\n+        result = await team.run(\n+            \"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination()\n+        )\n         expected_messages = [\n             \"Write a program that prints 'Hello, world!'\",\n             'Here is the program\\n ```python\\nprint(\"Hello, world!\")\\n```',\n@@ -200,7 +213,7 @@ async def test_round_robin_group_chat_with_tools(monkeypatch: pytest.MonkeyPatch\n     )\n     echo_agent = _EchoAgent(\"echo_agent\", description=\"echo agent\")\n     team = RoundRobinGroupChat(participants=[tool_use_agent, echo_agent])\n-    await team.run(\"Write a program that prints 'Hello, world!'\")\n+    await team.run(\"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination())\n     context = tool_use_agent._model_context  # pyright: ignore\n     assert context[0].content == \"Write a program that prints 'Hello, world!'\"\n     assert isinstance(context[1].content, list)\n@@ -279,7 +292,9 @@ async def test_selector_group_chat(monkeypatch: pytest.MonkeyPatch) -> None:\n         participants=[agent1, agent2, agent3],\n         model_client=OpenAIChatCompletionClient(model=model, api_key=\"\"),\n     )\n-    result = await team.run(\"Write a program that prints 'Hello, world!'\")\n+    result = await team.run(\n+        \"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination()\n+    )\n     assert len(result.messages) == 6\n     assert result.messages[0].content == \"Write a program that prints 'Hello, world!'\"\n     assert result.messages[1].source == \"agent3\"\n@@ -313,7 +328,9 @@ async def test_selector_group_chat_two_speakers(monkeypatch: pytest.MonkeyPatch)\n         participants=[agent1, agent2],\n         model_client=OpenAIChatCompletionClient(model=model, api_key=\"\"),\n     )\n-    result = await team.run(\"Write a program that prints 'Hello, world!'\")\n+    result = await team.run(\n+        \"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination()\n+    )\n     assert len(result.messages) == 5\n     assert result.messages[0].content == \"Write a program that prints 'Hello, world!'\"\n     assert result.messages[1].source == \"agent2\"\n@@ -369,7 +386,9 @@ async def test_selector_group_chat_two_speakers_allow_repeated(monkeypatch: pyte\n         model_client=OpenAIChatCompletionClient(model=model, api_key=\"\"),\n         allow_repeated_speaker=True,\n     )\n-    result = await team.run(\"Write a program that prints 'Hello, world!'\")\n+    result = await team.run(\n+        \"Write a program that prints 'Hello, world!'\", termination_condition=StopMessageTermination()\n+    )\n     assert len(result.messages) == 4\n     assert result.messages[0].content == \"Write a program that prints 'Hello, world!'\"\n     assert result.messages[1].source == \"agent2\"",
          "path": "python/packages/autogen-agentchat/tests/test_group_chat.py"
        },
        {
          "patch": "@@ -0,0 +1,126 @@\n+import pytest\n+from autogen_agentchat.agents import StopMessage, TextMessage\n+from autogen_agentchat.teams import MaxMessageTermination, StopMessageTermination, TextMentionTermination\n+\n+\n+@pytest.mark.asyncio\n+async def test_stop_message_termination() -> None:\n+    termination = StopMessageTermination()\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert await termination([StopMessage(content=\"Stop\", source=\"user\")]) is not None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"World\", source=\"agent\")])\n+        is None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), StopMessage(content=\"Stop\", source=\"user\")])\n+        is not None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_max_message_termination() -> None:\n+    termination = MaxMessageTermination(2)\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"World\", source=\"agent\")])\n+        is not None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_mention_termination() -> None:\n+    termination = TextMentionTermination(\"stop\")\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"stop\", source=\"user\")]) is not None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"stop\", source=\"user\")])\n+        is not None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_and_termination() -> None:\n+    termination = MaxMessageTermination(2) & TextMentionTermination(\"stop\")\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"World\", source=\"agent\")])\n+        is None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"stop\", source=\"user\")])\n+        is not None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_or_termination() -> None:\n+    termination = MaxMessageTermination(3) | TextMentionTermination(\"stop\")\n+    assert await termination([]) is None\n+    await termination.reset()\n+    assert await termination([TextMessage(content=\"Hello\", source=\"user\")]) is None\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"World\", source=\"agent\")])\n+        is None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"stop\", source=\"user\")])\n+        is not None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination([TextMessage(content=\"Hello\", source=\"user\"), TextMessage(content=\"Hello\", source=\"user\")])\n+        is None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination(\n+            [\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+            ]\n+        )\n+        is not None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination(\n+            [\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"stop\", source=\"user\"),\n+            ]\n+        )\n+        is not None\n+    )\n+    await termination.reset()\n+    assert (\n+        await termination(\n+            [\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"Hello\", source=\"user\"),\n+                TextMessage(content=\"stop\", source=\"user\"),\n+            ]\n+        )\n+        is not None\n+    )",
          "path": "python/packages/autogen-agentchat/tests/test_termination_condition.py"
        },
        {
          "patch": "@@ -23,7 +23,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from autogen_agentchat.agents import CodingAssistantAgent, ToolUseAssistantAgent\\n\",\n-    \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n+    \"from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n     \"from autogen_core.components.tools import FunctionTool\"\n    ]\n@@ -63,10 +63,11 @@\n     \"\\n\",\n     \"def google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:  # type: ignore[type-arg]\\n\",\n     \"    import os\\n\",\n+    \"    import time\\n\",\n+    \"\\n\",\n     \"    import requests\\n\",\n-    \"    from dotenv import load_dotenv\\n\",\n     \"    from bs4 import BeautifulSoup\\n\",\n-    \"    import time\\n\",\n+    \"    from dotenv import load_dotenv\\n\",\n     \"\\n\",\n     \"    load_dotenv()\\n\",\n     \"\\n\",\n@@ -115,13 +116,14 @@\n     \"\\n\",\n     \"\\n\",\n     \"def analyze_stock(ticker: str) -> dict:  # type: ignore[type-arg]\\n\",\n-    \"    import yfinance as yf\\n\",\n-    \"    import matplotlib.pyplot as plt\\n\",\n+    \"    import os\\n\",\n     \"    from datetime import datetime, timedelta\\n\",\n+    \"\\n\",\n+    \"    import matplotlib.pyplot as plt\\n\",\n     \"    import numpy as np\\n\",\n-    \"    from pytz import timezone  # type: ignore\\n\",\n     \"    import pandas as pd\\n\",\n-    \"    import os\\n\",\n+    \"    import yfinance as yf\\n\",\n+    \"    from pytz import timezone  # type: ignore\\n\",\n     \"\\n\",\n     \"    stock = yf.Ticker(ticker)\\n\",\n     \"\\n\",\n@@ -397,14 +399,14 @@\n     }\n    ],\n    \"source\": [\n-    \"result = await team.run(\\\"Write a financial report on American airlines\\\")\\n\",\n+    \"result = await team.run(\\\"Write a financial report on American airlines\\\", termination_condition=StopMessageTermination())\\n\",\n     \"print(result)\"\n    ]\n   }\n  ],\n  \"metadata\": {\n   \"kernelspec\": {\n-   \"display_name\": \"agnext\",\n+   \"display_name\": \".venv\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n   },\n@@ -418,7 +420,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.9\"\n+   \"version\": \"3.12.6\"\n   }\n  },\n  \"nbformat\": 4,",
          "path": "python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/company-research.ipynb"
        },
        {
          "patch": "@@ -23,7 +23,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from autogen_agentchat.agents import CodingAssistantAgent, ToolUseAssistantAgent\\n\",\n-    \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n+    \"from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n     \"from autogen_core.components.tools import FunctionTool\"\n    ]\n@@ -55,10 +55,11 @@\n    \"source\": [\n     \"def google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:  # type: ignore[type-arg]\\n\",\n     \"    import os\\n\",\n+    \"    import time\\n\",\n+    \"\\n\",\n     \"    import requests\\n\",\n-    \"    from dotenv import load_dotenv\\n\",\n     \"    from bs4 import BeautifulSoup\\n\",\n-    \"    import time\\n\",\n+    \"    from dotenv import load_dotenv\\n\",\n     \"\\n\",\n     \"    load_dotenv()\\n\",\n     \"\\n\",\n@@ -328,14 +329,16 @@\n     \"\\n\",\n     \"team = RoundRobinGroupChat(participants=[google_search_agent, arxiv_search_agent, report_agent])\\n\",\n     \"\\n\",\n-    \"result = await team.run(task=\\\"Write a literature review on no code tools for building multi agent ai systems\\\")\\n\",\n-    \"result\"\n+    \"result = await team.run(\\n\",\n+    \"    task=\\\"Write a literature review on no code tools for building multi agent ai systems\\\",\\n\",\n+    \"    termination_condition=StopMessageTermination(),\\n\",\n+    \")\"\n    ]\n   }\n  ],\n  \"metadata\": {\n   \"kernelspec\": {\n-   \"display_name\": \"agnext\",\n+   \"display_name\": \".venv\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n   },\n@@ -349,7 +352,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.9\"\n+   \"version\": \"3.12.6\"\n   }\n  },\n  \"nbformat\": 4,",
          "path": "python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb"
        },
        {
          "patch": "@@ -18,7 +18,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from autogen_agentchat.agents import CodingAssistantAgent\\n\",\n-    \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n+    \"from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\"\n    ]\n   },\n@@ -194,14 +194,14 @@\n    ],\n    \"source\": [\n     \"group_chat = RoundRobinGroupChat([planner_agent, local_agent, language_agent, travel_summary_agent])\\n\",\n-    \"result = await group_chat.run(task=\\\"Plan a 3 day trip to Nepal.\\\")\\n\",\n+    \"result = await group_chat.run(task=\\\"Plan a 3 day trip to Nepal.\\\", termination_condition=StopMessageTermination())\\n\",\n     \"print(result)\"\n    ]\n   }\n  ],\n  \"metadata\": {\n   \"kernelspec\": {\n-   \"display_name\": \"agnext\",\n+   \"display_name\": \".venv\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n   },\n@@ -215,7 +215,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.9\"\n+   \"version\": \"3.12.6\"\n   }\n  },\n  \"nbformat\": 4,",
          "path": "python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/examples/travel-planning.ipynb"
        },
        {
          "patch": "@@ -314,7 +314,7 @@\n    ],\n    \"source\": [\n     \"from autogen_agentchat.agents import CodeExecutorAgent, CodingAssistantAgent\\n\",\n-    \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n+    \"from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.components.code_executor import DockerCommandLineCodeExecutor\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n     \"\\n\",\n@@ -325,7 +325,8 @@\n     \"    )\\n\",\n     \"    group_chat = RoundRobinGroupChat([coding_assistant_agent, code_executor_agent])\\n\",\n     \"    result = await group_chat.run(\\n\",\n-    \"        task=\\\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\\\"\\n\",\n+    \"        task=\\\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\\\",\\n\",\n+    \"        termination_condition=StopMessageTermination(),\\n\",\n     \"    )\\n\",\n     \"    print(result)\"\n    ]\n@@ -356,7 +357,7 @@\n  ],\n  \"metadata\": {\n   \"kernelspec\": {\n-   \"display_name\": \"agnext\",\n+   \"display_name\": \".venv\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n   },\n@@ -370,7 +371,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.9\"\n+   \"version\": \"3.12.6\"\n   }\n  },\n  \"nbformat\": 4,",
          "path": "python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/guides/code-execution.ipynb"
        },
        {
          "patch": "@@ -9,7 +9,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 13,\n+   \"execution_count\": 1,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -24,15 +24,15 @@\n     \"    TextMessage,\\n\",\n     \"    ToolUseAssistantAgent,\\n\",\n     \")\\n\",\n-    \"from autogen_agentchat.teams.group_chat import SelectorGroupChat\\n\",\n+    \"from autogen_agentchat.teams import SelectorGroupChat, StopMessageTermination\\n\",\n     \"from autogen_core.base import CancellationToken\\n\",\n     \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n     \"from autogen_core.components.tools import FunctionTool\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n+   \"execution_count\": 2,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -49,7 +49,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 15,\n+   \"execution_count\": 3,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n@@ -69,7 +69,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 16,\n+   \"execution_count\": 4,\n    \"metadata\": {},\n    \"outputs\": [\n     {\n@@ -78,206 +78,113 @@\n      \"text\": [\n       \"\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:50.523469]:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:35:30.283450]:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Help user plan a trip and book a flight.\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:51.234858], group_chat_manager:\\u001b[0m\\n\",\n+      \"Help user plan a trip and book a flight.\"\n+     ]\n+    },\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n       \"\\n\",\n-      \"Selected next speaker: User\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:55.437051], User:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:35:48.275743], User:\\u001b[0m\\n\",\n       \"\\n\",\n       \"\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:55.957366], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:35:50.795496], TravelAssistant:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: TravelAssistant\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:58.291558], TravelAssistant:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Sure! I can help you plan your trip and provide information on booking a flight. Could you please provide me with the following details?\\n\",\n-      \"\\n\",\n-      \"1. Your departure city.\\n\",\n-      \"2. Your destination.\\n\",\n-      \"3. Travel dates (departure and return).\\n\",\n-      \"4. Number of travelers and their ages.\\n\",\n-      \"5. Any specific preferences or activities you would like to include in your trip?\\n\",\n+      \"I'd be happy to help you plan your trip! To get started, could you please provide me with the following details:\\n\",\n       \"\\n\",\n-      \"Once I have that information, we can get started!\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:10:58.827503], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: User\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:07.996036], User:\\u001b[0m\\n\",\n+      \"1. Your departure city and the destination city.\\n\",\n+      \"2. Your travel dates (departure and return).\\n\",\n+      \"3. The number of travelers and their ages (if any children are involved).\\n\",\n+      \"4. Your budget for flights and accommodations, if you have one in mind.\\n\",\n+      \"5. Any specific activities or attractions you're interested in at the destination.\\n\",\n       \"\\n\",\n-      \"Going to toronto from new york \\n\",\n+      \"Once I have this information, I can help you find the best options!\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:08.623692], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:35:59.701486], User:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: TravelAssistant\\n\",\n+      \"Traveling to toronto from new york\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:10.605232], TravelAssistant:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:02.325330], TravelAssistant:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Great! Here are a few more details I need to help you plan your trip:\\n\",\n+      \"Great choice! Toronto is a vibrant city with a lot to offer. Now, could you please provide the following additional details to help me assist you better?\\n\",\n       \"\\n\",\n-      \"1. **Departure dates:** When do you plan to leave New York and when will you return?\\n\",\n-      \"2. **Number of travelers:** How many people will be traveling with you, and what are their ages?\\n\",\n-      \"3. **Preferences:** Do you have any specific preferences for flight times or activities in Toronto? \\n\",\n+      \"1. What are your travel dates (departure and return)?\\n\",\n+      \"2. How many travelers will be going, and what are their ages?\\n\",\n+      \"3. Do you have a budget for the flight and accommodations?\\n\",\n+      \"4. Are there any specific activities or attractions you\u2019re interested in while in Toronto?\\n\",\n       \"\\n\",\n-      \"Once I have this information, I can assist you further!\\n\",\n+      \"Once I have this information, I can help you find the best flights and suggestions for your trip!\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:11.070495], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:20.633004], User:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: User\\n\",\n+      \"leaving on december 7 and returning on 12\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:26.768126], User:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:23.202871], TravelAssistant:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"leaving on december 12 and returning on 17, 2024\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:27.365051], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: TravelAssistant\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:30.335893], TravelAssistant:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Thank you for the details! Here\u2019s a summary of the trip so far:\\n\",\n+      \"Thank you for the details! Here's what I have so far:\\n\",\n       \"\\n\",\n       \"- **Departure City:** New York\\n\",\n-      \"- **Destination:** Toronto\\n\",\n-      \"- **Departure Date:** December 12, 2024\\n\",\n-      \"- **Return Date:** December 17, 2024\\n\",\n+      \"- **Destination City:** Toronto\\n\",\n+      \"- **Departure Date:** December 7\\n\",\n+      \"- **Return Date:** December 12\\n\",\n       \"\\n\",\n-      \"Now, could you please provide the following additional information?\\n\",\n+      \"Now, could you please provide:\\n\",\n       \"\\n\",\n-      \"1. **Number of travelers and their ages:** How many people will be traveling with you?\\n\",\n-      \"2. **Preferences for flights:** Any preferences for morning, afternoon, or evening flights?\\n\",\n-      \"3. **Activities:** Any specific activities or attractions you\u2019d like to prioritize in Toronto?\\n\",\n+      \"1. The number of travelers and their ages.\\n\",\n+      \"2. Your budget for flights and accommodations (if applicable).\\n\",\n+      \"3. Any specific activities or attractions you're interested in while in Toronto.\\n\",\n       \"\\n\",\n-      \"With this information, I can assist you in finding suitable flights and offer activity suggestions!\\n\",\n+      \"This will help me provide more tailored options for your trip!\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:30.822862], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:38.096554], User:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: User\\n\",\n+      \"just myself one adult\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:39.547965], User:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:40.307824], FlightBroker:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"just myself\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:40.110527], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: FlightBroker\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:45.764773], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Thank you for the information! Here\u2019s what I have so far for your trip:\\n\",\n+      \"Thanks for the information! Here's what I have:\\n\",\n       \"\\n\",\n       \"- **Departure City:** New York\\n\",\n-      \"- **Destination:** Toronto\\n\",\n-      \"- **Departure Date:** December 12, 2024\\n\",\n-      \"- **Return Date:** December 17, 2024\\n\",\n-      \"- **Number of Travelers:** 1 (yourself)\\n\",\n-      \"\\n\",\n-      \"Now, do you have any preferences for flight times (morning, afternoon, or evening)? Additionally, are there any specific activities or attractions you would like to include in your trip to Toronto? \\n\",\n+      \"- **Destination City:** Toronto\\n\",\n+      \"- **Departure Date:** December 7\\n\",\n+      \"- **Return Date:** December 12\\n\",\n+      \"- **Number of Travelers:** 1 Adult\\n\",\n       \"\\n\",\n-      \"Once I have that, I can start searching for flights!\\n\",\n+      \"Could you let me know if you have a budget for flights and accommodations? Additionally, are there any specific activities or attractions you're interested in while in Toronto? This will help me provide the best options for your trip!\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:46.363784], group_chat_manager:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:45.875280], User:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"Selected next speaker: User\\n\",\n+      \"that's it\\n\",\n       \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:11:59.508971], User:\\u001b[0m\\n\",\n+      \"\\u001b[91m[2024-10-08T20:36:50.925624], FlightBroker:\\u001b[0m\\n\",\n       \"\\n\",\n-      \"any time is fine\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:00.072654], group_chat_manager:\\u001b[0m\\n\",\n+      \"Your flights have been successfully booked! Here are the details:\\n\",\n       \"\\n\",\n-      \"Selected next speaker: FlightBroker\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:01.847222], FlightBroker:\\u001b[0m\\n\",\n+      \"- **Departure:** New York to Toronto\\n\",\n+      \"  - **Flight:** AL21\\n\",\n+      \"  - **Date:** December 7, 2023\\n\",\n       \"\\n\",\n-      \"[FunctionCall(id='call_lpuPUlo9k3p4VeX0h6jO8yJg', arguments='{\\\"start\\\":\\\"New York\\\",\\\"destination\\\":\\\"Toronto\\\",\\\"date\\\":\\\"2024-12-12\\\"}', name='flight_search')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:01.848179], tool_agent_for_FlightBroker:\\u001b[0m\\n\",\n+      \"- **Return:** Toronto to New York\\n\",\n+      \"  - **Flight:** AL21\\n\",\n+      \"  - **Date:** December 12, 2023\\n\",\n       \"\\n\",\n-      \"[FunctionExecutionResult(content='AC24 from New York to Toronto on 2024-12-12 is $500\\\\nUA23 from New York to Toronto on 2024-12-12 is $450\\\\nAL21 from New York to Toronto on 2024-12-12 is $400', call_id='call_lpuPUlo9k3p4VeX0h6jO8yJg')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:03.512522], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"[FunctionCall(id='call_dxxmiR6hVBL9QuneJGNnleR0', arguments='{\\\"start\\\":\\\"Toronto\\\",\\\"destination\\\":\\\"New York\\\",\\\"date\\\":\\\"2024-12-17\\\"}', name='flight_search')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:03.513405], tool_agent_for_FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"[FunctionExecutionResult(content='AC24 from Toronto to New York on 2024-12-17 is $500\\\\nUA23 from Toronto to New York on 2024-12-17 is $450\\\\nAL21 from Toronto to New York on 2024-12-17 is $400', call_id='call_dxxmiR6hVBL9QuneJGNnleR0')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:07.337638], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"I found some flight options for your trip from New York to Toronto and back. Here are the details:\\n\",\n-      \"\\n\",\n-      \"### Departure: New York to Toronto on December 12, 2024\\n\",\n-      \"1. **Flight AC24**\\n\",\n-      \"   - Price: $500\\n\",\n-      \"2. **Flight UA23**\\n\",\n-      \"   - Price: $450\\n\",\n-      \"3. **Flight AL21**\\n\",\n-      \"   - Price: $400\\n\",\n-      \"\\n\",\n-      \"### Return: Toronto to New York on December 17, 2024\\n\",\n-      \"1. **Flight AC24**\\n\",\n-      \"   - Price: $500\\n\",\n-      \"2. **Flight UA23**\\n\",\n-      \"   - Price: $450\\n\",\n-      \"3. **Flight AL21**\\n\",\n-      \"   - Price: $400\\n\",\n-      \"\\n\",\n-      \"Would you like to book any of these flights? If so, please let me know which flight you'd prefer for both the departure and return!\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:07.870903], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: User\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:22.910467], User:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Book the AC flight leaving and UA flight returning\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:23.450871], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: FlightBroker\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:27.273108], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"[FunctionCall(id='call_wqkaIBdYjWklWG0GQkYz7FZ0', arguments='{\\\"flight\\\": \\\"AC24\\\", \\\"date\\\": \\\"2024-12-12\\\"}', name='flight_booking'), FunctionCall(id='call_QZKtPHpbq2QzNi5y6OgfTcsd', arguments='{\\\"flight\\\": \\\"UA23\\\", \\\"date\\\": \\\"2024-12-17\\\"}', name='flight_booking')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:27.274111], tool_agent_for_FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"[FunctionExecutionResult(content='Booked flight AC24 on 2024-12-12', call_id='call_wqkaIBdYjWklWG0GQkYz7FZ0'), FunctionExecutionResult(content='Booked flight UA23 on 2024-12-17', call_id='call_QZKtPHpbq2QzNi5y6OgfTcsd')]\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:29.585911], FlightBroker:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Your flights have been successfully booked!\\n\",\n-      \"\\n\",\n-      \"- **Departure Flight:** AC24 from New York to Toronto on December 12, 2024.\\n\",\n-      \"- **Return Flight:** UA23 from Toronto to New York on December 17, 2024.\\n\",\n-      \"\\n\",\n-      \"If you need any further assistance with your trip or have any questions about activities in Toronto, feel free to ask! Safe travels!\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:30.242282], group_chat_manager:\\u001b[0m\\n\",\n-      \"\\n\",\n-      \"Selected next speaker: User\\n\",\n-      \"--------------------------------------------------------------------------- \\n\",\n-      \"\\u001b[91m[2024-10-08T10:12:39.715695], User:\\u001b[0m\\n\",\n+      \"If you need help with accommodations, activities, or anything else for your trip, feel free to let me know! \\n\",\n       \"\\n\",\n-      \"User has terminated the conversation.\"\n+      \"TERMINATE\"\n      ]\n     },\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"TeamRunResult(messages=[TextMessage(source='user', content='Help user plan a trip and book a flight.'), TextMessage(source='User', content=''), TextMessage(source='TravelAssistant', content='Sure! I can help you plan your trip and provide information on booking a flight. Could you please provide me with the following details?\\\\n\\\\n1. Your departure city.\\\\n2. Your destination.\\\\n3. Travel dates (departure and return).\\\\n4. Number of travelers and their ages.\\\\n5. Any specific preferences or activities you would like to include in your trip?\\\\n\\\\nOnce I have that information, we can get started!'), TextMessage(source='User', content='Going to toronto from new york '), TextMessage(source='TravelAssistant', content='Great! Here are a few more details I need to help you plan your trip:\\\\n\\\\n1. **Departure dates:** When do you plan to leave New York and when will you return?\\\\n2. **Number of travelers:** How many people will be traveling with you, and what are their ages?\\\\n3. **Preferences:** Do you have any specific preferences for flight times or activities in Toronto? \\\\n\\\\nOnce I have this information, I can assist you further!'), TextMessage(source='User', content='leaving on december 12 and returning on 17, 2024'), TextMessage(source='TravelAssistant', content='Thank you for the details! Here\u2019s a summary of the trip so far:\\\\n\\\\n- **Departure City:** New York\\\\n- **Destination:** Toronto\\\\n- **Departure Date:** December 12, 2024\\\\n- **Return Date:** December 17, 2024\\\\n\\\\nNow, could you please provide the following additional information?\\\\n\\\\n1. **Number of travelers and their ages:** How many people will be traveling with you?\\\\n2. **Preferences for flights:** Any preferences for morning, afternoon, or evening flights?\\\\n3. **Activities:** Any specific activities or attractions you\u2019d like to prioritize in Toronto?\\\\n\\\\nWith this information, I can assist you in finding suitable flights and offer activity suggestions!'), TextMessage(source='User', content='just myself'), TextMessage(source='FlightBroker', content='Thank you for the information! Here\u2019s what I have so far for your trip:\\\\n\\\\n- **Departure City:** New York\\\\n- **Destination:** Toronto\\\\n- **Departure Date:** December 12, 2024\\\\n- **Return Date:** December 17, 2024\\\\n- **Number of Travelers:** 1 (yourself)\\\\n\\\\nNow, do you have any preferences for flight times (morning, afternoon, or evening)? Additionally, are there any specific activities or attractions you would like to include in your trip to Toronto? \\\\n\\\\nOnce I have that, I can start searching for flights!'), TextMessage(source='User', content='any time is fine'), TextMessage(source='FlightBroker', content=\\\"I found some flight options for your trip from New York to Toronto and back. Here are the details:\\\\n\\\\n### Departure: New York to Toronto on December 12, 2024\\\\n1. **Flight AC24**\\\\n   - Price: $500\\\\n2. **Flight UA23**\\\\n   - Price: $450\\\\n3. **Flight AL21**\\\\n   - Price: $400\\\\n\\\\n### Return: Toronto to New York on December 17, 2024\\\\n1. **Flight AC24**\\\\n   - Price: $500\\\\n2. **Flight UA23**\\\\n   - Price: $450\\\\n3. **Flight AL21**\\\\n   - Price: $400\\\\n\\\\nWould you like to book any of these flights? If so, please let me know which flight you'd prefer for both the departure and return!\\\"), TextMessage(source='User', content='Book the AC flight leaving and UA flight returning'), TextMessage(source='FlightBroker', content='Your flights have been successfully booked!\\\\n\\\\n- **Departure Flight:** AC24 from New York to Toronto on December 12, 2024.\\\\n- **Return Flight:** UA23 from Toronto to New York on December 17, 2024.\\\\n\\\\nIf you need any further assistance with your trip or have any questions about activities in Toronto, feel free to ask! Safe travels!'), StopMessage(source='User', content='User has terminated the conversation.')])\"\n+       \"TeamRunResult(messages=[TextMessage(source='user', content='Help user plan a trip and book a flight.'), TextMessage(source='User', content=''), TextMessage(source='TravelAssistant', content=\\\"I'd be happy to help you plan your trip! To get started, could you please provide me with the following details:\\\\n\\\\n1. Your departure city and the destination city.\\\\n2. Your travel dates (departure and return).\\\\n3. The number of travelers and their ages (if any children are involved).\\\\n4. Your budget for flights and accommodations, if you have one in mind.\\\\n5. Any specific activities or attractions you're interested in at the destination.\\\\n\\\\nOnce I have this information, I can help you find the best options!\\\"), TextMessage(source='User', content='Traveling to toronto from new york'), TextMessage(source='TravelAssistant', content='Great choice! Toronto is a vibrant city with a lot to offer. Now, could you please provide the following additional details to help me assist you better?\\\\n\\\\n1. What are your travel dates (departure and return)?\\\\n2. How many travelers will be going, and what are their ages?\\\\n3. Do you have a budget for the flight and accommodations?\\\\n4. Are there any specific activities or attractions you\u2019re interested in while in Toronto?\\\\n\\\\nOnce I have this information, I can help you find the best flights and suggestions for your trip!'), TextMessage(source='User', content='leaving on december 7 and returning on 12'), TextMessage(source='TravelAssistant', content=\\\"Thank you for the details! Here's what I have so far:\\\\n\\\\n- **Departure City:** New York\\\\n- **Destination City:** Toronto\\\\n- **Departure Date:** December 7\\\\n- **Return Date:** December 12\\\\n\\\\nNow, could you please provide:\\\\n\\\\n1. The number of travelers and their ages.\\\\n2. Your budget for flights and accommodations (if applicable).\\\\n3. Any specific activities or attractions you're interested in while in Toronto.\\\\n\\\\nThis will help me provide more tailored options for your trip!\\\"), TextMessage(source='User', content='just myself one adult'), TextMessage(source='FlightBroker', content=\\\"Thanks for the information! Here's what I have:\\\\n\\\\n- **Departure City:** New York\\\\n- **Destination City:** Toronto\\\\n- **Departure Date:** December 7\\\\n- **Return Date:** December 12\\\\n- **Number of Travelers:** 1 Adult\\\\n\\\\nCould you let me know if you have a budget for flights and accommodations? Additionally, are there any specific activities or attractions you're interested in while in Toronto? This will help me provide the best options for your trip!\\\"), TextMessage(source='User', content=\\\"that's it\\\"), StopMessage(source='FlightBroker', content='Your flights have been successfully booked! Here are the details:\\\\n\\\\n- **Departure:** New York to Toronto\\\\n  - **Flight:** AL21\\\\n  - **Date:** December 7, 2023\\\\n\\\\n- **Return:** Toronto to New York\\\\n  - **Flight:** AL21\\\\n  - **Date:** December 12, 2023\\\\n\\\\nIf you need help with accommodations, activities, or anything else for your trip, feel free to let me know! \\\\n\\\\nTERMINATE'), StopMessage(source='StopMessageTermination', content='Stop message received')])\"\n       ]\n      },\n-     \"execution_count\": 16,\n+     \"execution_count\": 4,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -302,7 +209,7 @@\n     \"team = SelectorGroupChat(\\n\",\n     \"    [user_proxy, flight_broker, travel_assistant], model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\")\\n\",\n     \")\\n\",\n-    \"await team.run(\\\"Help user plan a trip and book a flight.\\\")\"\n+    \"await team.run(\\\"Help user plan a trip and book a flight.\\\", termination_condition=StopMessageTermination())\"\n    ]\n   }\n  ],",
          "path": "python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/guides/selector-group-chat.ipynb"
        },
        {
          "patch": "@@ -1,236 +1,185 @@\n {\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"# Tool Use\\n\",\n-        \"\\n\",\n-        \"The `AgentChat` api provides a `ToolUseAssistantAgent` with presets for adding tools that the agent can call as part of it's response. \\n\",\n-        \"\\n\",\n-        \":::{note}\\n\",\n-        \"\\n\",\n-        \"The example presented here is a work in progress \ud83d\udea7. Also, tool uses here assumed the `model_client` used by the agent supports tool calling. \\n\",\n-        \"::: \"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 1,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"from autogen_agentchat.agents import ToolUseAssistantAgent\\n\",\n-        \"from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\\n\",\n-        \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n-        \"from autogen_core.components.tools import FunctionTool\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"In AgentChat, a Tool is a function wrapped in the `FunctionTool` class exported from `autogen_core.components.tools`.   \"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 2,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"async def get_weather(city: str) -> str:\\n\",\n-        \"    return f\\\"The weather in {city} is 72 degrees and Sunny.\\\"\\n\",\n-        \"\\n\",\n-        \"\\n\",\n-        \"get_weather_tool = FunctionTool(get_weather, description=\\\"Get the weather for a city\\\")\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"Finally, agents that use tools are defined in the following manner. \\n\",\n-        \"\\n\",\n-        \"-  An agent is instantiated based on the  `ToolUseAssistantAgent` class in AgentChat. The agent is aware of the tools it can use by passing a `tools_schema` attribute to the class, which is passed to the `model_client` when the agent generates a response.\\n\",\n-        \"-  An agent Team is defined that takes a list of `tools`.  Effectively, the `ToolUseAssistantAgent` can generate messages that call tools, and the team is responsible executing those tool calls and returning the results.\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 3,\n-      \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"name\": \"stdout\",\n-          \"output_type\": \"stream\",\n-          \"text\": [\n-            \"\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:50:13.202461]:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"What's the weather in New York?\\n\",\n-            \"From: user\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:50:14.090696], Weather_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionCall(id='call_wqkaIBdYjWklWG0GQkYz7FZ0', arguments='{\\\"city\\\":\\\"New York\\\"}', name='get_weather')]\\n\",\n-            \"From: Weather_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:50:14.092050], tool_agent_for_Weather_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionExecutionResult(content='The weather in New York is 72 degrees and Sunny.', call_id='call_wqkaIBdYjWklWG0GQkYz7FZ0')]\\n\",\n-            \"From: tool_agent_for_Weather_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:50:14.714470], Weather_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"The weather in New York is 72 degrees and sunny. \\n\",\n-            \"\\n\",\n-            \"TERMINATE\\n\",\n-            \"From: Weather_Assistant\"\n-          ]\n-        }\n-      ],\n-      \"source\": [\n-        \"assistant = ToolUseAssistantAgent(\\n\",\n-        \"    \\\"Weather_Assistant\\\",\\n\",\n-        \"    model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\"),\\n\",\n-        \"    registered_tools=[get_weather_tool],\\n\",\n-        \")\\n\",\n-        \"team = RoundRobinGroupChat([assistant])\\n\",\n-        \"result = await team.run(\\\"What's the weather in New York?\\\")\\n\",\n-        \"# print(result)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Using Langchain Tools \\n\",\n-        \"\\n\",\n-        \"AutoGen also provides direct support for tools from LangChain via the `autogen_ext`  package.\\n\",\n-        \"\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 4,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": [\n-        \"# pip install langchain, langchain-community, wikipedia , autogen-ext\\n\",\n-        \"\\n\",\n-        \"from autogen_ext.tools.langchain import LangChainToolAdapter\\n\",\n-        \"from langchain.tools import WikipediaQueryRun\\n\",\n-        \"from langchain_community.utilities import WikipediaAPIWrapper\\n\",\n-        \"\\n\",\n-        \"api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\\n\",\n-        \"tool = WikipediaQueryRun(api_wrapper=api_wrapper)\\n\",\n-        \"\\n\",\n-        \"langchain_wikipedia_tool = LangChainToolAdapter(tool)\"\n-      ]\n-    },\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Tool Use\\n\",\n+    \"\\n\",\n+    \"The `AgentChat` api provides a `ToolUseAssistantAgent` with presets for adding tools that the agent can call as part of it's response. \\n\",\n+    \"\\n\",\n+    \":::{note}\\n\",\n+    \"\\n\",\n+    \"The example presented here is a work in progress \ud83d\udea7. Also, tool uses here assumed the `model_client` used by the agent supports tool calling. \\n\",\n+    \"::: \"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from autogen_agentchat.agents import ToolUseAssistantAgent\\n\",\n+    \"from autogen_agentchat.teams import EVENT_LOGGER_NAME, RoundRobinGroupChat, StopMessageTermination\\n\",\n+    \"from autogen_core.components.models import OpenAIChatCompletionClient\\n\",\n+    \"from autogen_core.components.tools import FunctionTool\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"In AgentChat, a Tool is a function wrapped in the `FunctionTool` class exported from `autogen_core.components.tools`.   \"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 2,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"async def get_weather(city: str) -> str:\\n\",\n+    \"    return f\\\"The weather in {city} is 72 degrees and Sunny.\\\"\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"get_weather_tool = FunctionTool(get_weather, description=\\\"Get the weather for a city\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Finally, agents that use tools are defined in the following manner. \\n\",\n+    \"\\n\",\n+    \"-  An agent is instantiated based on the  `ToolUseAssistantAgent` class in AgentChat. The agent is aware of the tools it can use by passing a `tools_schema` attribute to the class, which is passed to the `model_client` when the agent generates a response.\\n\",\n+    \"-  An agent Team is defined that takes a list of `tools`.  Effectively, the `ToolUseAssistantAgent` can generate messages that call tools, and the team is responsible executing those tool calls and returning the results.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 3,\n+   \"metadata\": {},\n+   \"outputs\": [\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 8,\n-      \"metadata\": {},\n-      \"outputs\": [\n-        {\n-          \"name\": \"stdout\",\n-          \"output_type\": \"stream\",\n-          \"text\": [\n-            \"\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:36.869317]:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"Who is the receipient of the 2023 Nobel Prize in Physics?\\n\",\n-            \"From: user\"\n-          ]\n-        },\n-        {\n-          \"name\": \"stdout\",\n-          \"output_type\": \"stream\",\n-          \"text\": [\n-            \"\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:37.856066], WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionCall(id='call_bdLqS1msbHCy5IMGYaata5vs', arguments='{\\\"query\\\":\\\"2023 Nobel Prize in Physics\\\"}', name='wikipedia')]\\n\",\n-            \"From: WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:38.518288], tool_agent_for_WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionExecutionResult(content='Page: Nobel Prize in Physics\\\\nSummary: The Nobel Prize in Physics (Swedish: Nobelpriset i fysik) is a', call_id='call_bdLqS1msbHCy5IMGYaata5vs')]\\n\",\n-            \"From: tool_agent_for_WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:39.070911], WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionCall(id='call_BFXGGeuBbOQ1LPb4f0NiNva2', arguments='{\\\"query\\\":\\\"2023 Nobel Prize in Physics recipients\\\"}', name='wikipedia')]\\n\",\n-            \"From: WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:39.727147], tool_agent_for_WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionExecutionResult(content='Page: Nobel Prize in Physics\\\\nSummary: The Nobel Prize in Physics (Swedish: Nobelpriset i fysik) is a', call_id='call_BFXGGeuBbOQ1LPb4f0NiNva2')]\\n\",\n-            \"From: tool_agent_for_WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:40.746467], WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionCall(id='call_iH2gkY5A2LiQTiy2eh86XpP5', arguments='{\\\"query\\\": \\\"2023 Nobel Prize in Physics winners\\\"}', name='wikipedia'), FunctionCall(id='call_rJXgJQiAKoD7yrymNJCsQA9N', arguments='{\\\"query\\\": \\\"Nobel Prize in Physics\\\"}', name='wikipedia')]\\n\",\n-            \"From: WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:41.469348], tool_agent_for_WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"[FunctionExecutionResult(content='Page: Nobel Prize in Physics\\\\nSummary: The Nobel Prize in Physics (Swedish: Nobelpriset i fysik) is a', call_id='call_iH2gkY5A2LiQTiy2eh86XpP5'), FunctionExecutionResult(content='Page: Nobel Prize in Physics\\\\nSummary: The Nobel Prize in Physics (Swedish: Nobelpriset i fysik) is a', call_id='call_rJXgJQiAKoD7yrymNJCsQA9N')]\\n\",\n-            \"From: tool_agent_for_WikiPedia_Assistant\\n\",\n-            \"--------------------------------------------------------------------------- \\n\",\n-            \"\\u001b[91m[2024-10-08T09:51:42.576718], WikiPedia_Assistant:\\u001b[0m\\n\",\n-            \"\\n\",\n-            \"I couldn't find specific information about the recipients of the 2023 Nobel Prize in Physics. You might want to check a reliable news source or the official Nobel Prize website for the most accurate and up-to-date details. \\n\",\n-            \"\\n\",\n-            \"TERMINATE\\n\",\n-            \"From: WikiPedia_Assistant\"\n-          ]\n-        }\n-      ],\n-      \"source\": [\n-        \"wikipedia_assistant = ToolUseAssistantAgent(\\n\",\n-        \"    \\\"WikiPedia_Assistant\\\",\\n\",\n-        \"    model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\"),\\n\",\n-        \"    registered_tools=[langchain_wikipedia_tool],\\n\",\n-        \")\\n\",\n-        \"team = RoundRobinGroupChat([wikipedia_assistant])\\n\",\n-        \"result = await team.run(\\\"Who was the first president of the United States?\\\")\\n\",\n-        \"\\n\",\n-        \"# print(result)\"\n-      ]\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------- \\n\",\n+      \"\\u001b[91m[2024-10-08T20:34:31.935149]:\\u001b[0m\\n\",\n+      \"\\n\",\n+      \"What's the weather in New York?\"\n+     ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {},\n-      \"outputs\": [],\n-      \"source\": []\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------- \\n\",\n+      \"\\u001b[91m[2024-10-08T20:34:33.080494], Weather_Assistant:\\u001b[0m\\n\",\n+      \"\\n\",\n+      \"The weather in New York is 72 degrees and sunny. \\n\",\n+      \"\\n\",\n+      \"TERMINATE\"\n+     ]\n     }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"agnext\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.11.9\"\n+   ],\n+   \"source\": [\n+    \"assistant = ToolUseAssistantAgent(\\n\",\n+    \"    \\\"Weather_Assistant\\\",\\n\",\n+    \"    model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\"),\\n\",\n+    \"    registered_tools=[get_weather_tool],\\n\",\n+    \")\\n\",\n+    \"team = RoundRobinGroupChat([assistant])\\n\",\n+    \"result = await team.run(\\\"What's the weather in New York?\\\", termination_condition=StopMessageTermination())\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Using Langchain Tools \\n\",\n+    \"\\n\",\n+    \"AutoGen also provides direct support for tools from LangChain via the `autogen_ext`  package.\\n\",\n+    \"\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 6,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# pip install langchain, langchain-community, wikipedia , autogen-ext\\n\",\n+    \"\\n\",\n+    \"import wikipedia\\n\",\n+    \"from autogen_ext.tools.langchain import LangChainToolAdapter\\n\",\n+    \"from langchain.tools import WikipediaQueryRun\\n\",\n+    \"from langchain_community.utilities import WikipediaAPIWrapper\\n\",\n+    \"\\n\",\n+    \"api_wrapper = WikipediaAPIWrapper(wiki_client=wikipedia, top_k_results=1, doc_content_chars_max=100)\\n\",\n+    \"tool = WikipediaQueryRun(api_wrapper=api_wrapper)\\n\",\n+    \"\\n\",\n+    \"langchain_wikipedia_tool = LangChainToolAdapter(tool)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 7,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\n\",\n+      \"--------------------------------------------------------------------------- \\n\",\n+      \"\\u001b[91m[2024-10-08T20:44:08.218758]:\\u001b[0m\\n\",\n+      \"\\n\",\n+      \"Who was the first president of the United States?\\n\",\n+      \"--------------------------------------------------------------------------- \\n\",\n+      \"\\u001b[91m[2024-10-08T20:44:11.240067], WikiPedia_Assistant:\\u001b[0m\\n\",\n+      \"\\n\",\n+      \"The first president of the United States was George Washington, who served from April 30, 1789, to March 4, 1797. \\n\",\n+      \"\\n\",\n+      \"TERMINATE\"\n+     ]\n     }\n+   ],\n+   \"source\": [\n+    \"wikipedia_assistant = ToolUseAssistantAgent(\\n\",\n+    \"    \\\"WikiPedia_Assistant\\\",\\n\",\n+    \"    model_client=OpenAIChatCompletionClient(model=\\\"gpt-4o-mini\\\"),\\n\",\n+    \"    registered_tools=[langchain_wikipedia_tool],\\n\",\n+    \")\\n\",\n+    \"team = RoundRobinGroupChat([wikipedia_assistant])\\n\",\n+    \"result = await team.run(\\n\",\n+    \"    \\\"Who was the first president of the United States?\\\", termination_condition=StopMessageTermination()\\n\",\n+    \")\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \".venv\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n   },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 2\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.12.6\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n }",
          "path": "python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/guides/tool_use.ipynb"
        },
        {
          "patch": "@@ -3,7 +3,7 @@\n `````{tab-item} AgentChat (v0.4x)\n ```python\n from autogen_agentchat.agents import CodeExecutorAgent, CodingAssistantAgent\n-from autogen_agentchat.teams.group_chat import RoundRobinGroupChat\n+from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n from autogen_core.components.code_executor import DockerCommandLineCodeExecutor\n from autogen_core.components.models import OpenAIChatCompletionClient\n \n@@ -14,9 +14,9 @@ async with DockerCommandLineCodeExecutor(work_dir=\"coding\") as code_executor:\n     )\n     group_chat = RoundRobinGroupChat([coding_assistant_agent, code_executor_agent])\n     result = await group_chat.run(\n-        task=\"Create a plot of NVIDIA and TESLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\"\n+        task=\"Create a plot of NVIDIA and TESLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\",\n+        termination_condition=StopMessageTermination(),\n     )\n-    print(result)\n ```\n `````\n ",
          "path": "python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/stocksnippet.md"
        },
        {
          "patch": "@@ -0,0 +1,103 @@\n+---\n+title: New AutoGen Architecture Preview\n+authors:\n+  - autogen-team\n+tags: [AutoGen]\n+---\n+\n+# New AutoGen Architecture Preview\n+\n+<center>\n+\n+![What are they doing?](img/robots.jpeg)\n+\n+</center>\n+\n+One year ago, we launched AutoGen, a programming framework designed to build\n+agentic AI systems. The release of AutoGen sparked massive interest within the\n+developer community. As an early release, it provided us with a unique\n+opportunity to engage deeply with users, gather invaluable feedback, and learn\n+from a diverse range of use cases and contributions. By listening and engaging\n+with the community, we gained insights into what people were building or\n+attempting to build, how they were approaching the creation of agentic systems,\n+and where they were struggling. This experience was both humbling and\n+enlightening, revealing significant opportunities for improvement in our initial\n+design, especially for power users developing production-level applications with\n+AutoGen.\n+\n+Through engagements with the community, we learned many lessons:\n+\n+- Developers value modular and reusable agents. For example, our built-in agents\n+  that could be directly plugged in or easily customized for specific use cases\n+  were particularly popular. At the same time, there was a desire for more\n+  customizability, such as integrating custom agents built using other\n+  programming languages or frameworks.\n+- Chat-based agent-to-agent communication was an intuitive collaboration\n+  pattern, making it easy for developers to get started and involve humans in\n+  the loop. As developers began to employ agents in a wider range of scenarios,\n+  they sought more flexibility in collaboration patterns. For instance,\n+  developers wanted to build predictable, ordered workflows with agents, and to\n+  integrate them with new user interfaces that are not chat-based.\n+- Although it was easy for developers to get started with AutoGen, debugging and\n+  scaling agent teams applications proved more challenging.\n+- There were many opportunities for improving code quality.\n+\n+These learnings, along with many others from other agentic efforts across\n+Microsoft, prompted us to take a step back and lay the groundwork for a new\n+direction. A few months ago, we started dedicating time to distilling these\n+learnings into a roadmap for the future of AutoGen. This led to the development\n+of AutoGen 0.4, a complete redesign of the framework from the foundation up.\n+AutoGen 0.4 embraces the actor model of computing to support distributed, highly\n+scalable, event-driven agentic systems. This approach offers many advantages,\n+such as:\n+\n+- **Composability**. Systems designed in this way are more composable, allowing\n+  developers to bring their own agents implemented in different frameworks or\n+  programming languages and to build more powerful systems using complex agentic\n+  patterns.\n+- **Flexibility**. It allows for the creation of both deterministic, ordered\n+  workflows and event-driven or decentralized workflows, enabling customers to\n+  bring their own orchestration or integrate with other systems more easily. It\n+  also opens more opportunities for human-in-the-loop scenarios, both active and\n+  reactive.\n+- **Debugging and Observability**. Event-driven communication moves message delivery\n+  away from agents to a centralized component, making it easier to observe and\n+  debug their activities regardless of agent implementation.\n+- **Scalability**. An event-based architecture enables distributed and\n+  cloud-deployed agents, which is essential for building scalable AI services\n+  and applications.\n+\n+Today, we are delighted to share our progress and invite everyone to collaborate\n+with us and provide feedback to evolve AutoGen and help shape the future of\n+multi-agent systems.\n+\n+As the first step, we are opening a [pull request](https://github.com/microsoft/autogen/pull/3600) into the main branch with the\n+current state of development of 0.4. After approximately a week, we plan to\n+merge this into main and continue development. There's still a lot left to do\n+before 0.4 is ready for release though, so keep in mind this is a work in\n+progress.\n+\n+Starting in AutoGen 0.4, the project will have three main libraries:\n+\n+- **Core** - the building blocks for an event-driven agentic system.\n+- **AgentChat** - a task-driven, high-level API built with core, including group\n+  chat, code execution, pre-built agents, and more. This is the most similar API\n+  to AutoGen [0.2](https://github.com/microsoft/autogen/tree/0.2) and will be the easiest API to migrate to.\n+- **Extensions** - implementations of core interfaces and third-party integrations\n+  (e.g., Azure code executor and OpenAI model client).\n+\n+AutoGen [0.2](https://github.com/microsoft/autogen/tree/0.2) is still available, developed and maintained out of the [0.2 branch](https://github.com/microsoft/autogen/tree/0.2).\n+For everyone looking for a stable version, we recommend continuing to use [0.2](https://github.com/microsoft/autogen/tree/0.2)\n+for the time being. It can be installed using:\n+\n+```sh\n+pip install autogen-agentchat~=0.2\n+```\n+\n+This new package name was used to align with the new packages that will come with 0.4:\n+`autogen-core`, `autogen-agentchat`, and `autogen-ext`.\n+\n+Lastly, we will be using [GitHub\n+Discussion](https://github.com/microsoft/autogen/discussions) as the official\n+community forum for the new version and, going forward, all discussions related\n+to the AutoGen project. We look forward to meeting you there.",
          "path": "website/blog/2024-10-02-new-autogen-architecture-preview/index.mdx"
        }
      ],
      "pr_number": "3696",
      "stage_id": "stage-3",
      "test_query": "You need to modify the files in this repository according to the requirements below. During the analysis process, do not directly read the complete file; try to read the first 10 lines using head or use grep instead. You are NOT allowed to directly run scripts, such as python, pip install, dockerfile, etc.\n\nThe repository requires implementation of a termination mechanism for agent chat teams to prevent infinite conversations. The primary focus should be on the agentchat teams module located in the python/packages/autogen-agentchat/src/autogen_agentchat/teams/ directory. You need to create a system where teams can evaluate whether to continue or stop their conversation based on specific conditions. The termination logic should be integrated into the team's execution flow, allowing the run method to check these conditions during message processing. When a termination condition is satisfied, the team should gracefully halt its operation. The implementation should support different types of termination strategies and be extensible for future condition types. Additionally, ensure the group chat implementations in the group_chat subdirectory properly utilize this termination mechanism, and update the relevant test files to validate the new functionality."
    }
  ],
  "validation_messages": []
}