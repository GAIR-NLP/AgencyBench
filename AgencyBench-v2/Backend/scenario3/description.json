{
  "subtask_count": 5,
  "subtask1": "Query:\nImplement `subtask1_generator.py` plus a lightweight JSON schema under `domain/` so that `python3 subtask1_generator.py --seed <int> --count <n> --schema <path> --mode <append|overwrite> --out data/events.jsonl` produces byte-identical JSONL streams for the same `(seed, schema)` while logging seed, timestamp, and file mode to `logs/gen.log`. The environment provides Python 3.10 stdlib only, an empty `data/` folder, and example schema fragments. The model must ship deterministic id/timestamp generation, payload synthesis that obeys the schema, incremental append/overwrite handling, and a built-in validator. The final artifact is the JSONL event file plus matching metadata log entries.\nDeliverables:\n- `subtask1_generator.py` (argparse entrypoint, deterministic event generator, validator, and log writer).\n- At least one schema sample (e.g., `domain/events.schema.json`) and sample output `data/events.jsonl`.\n- `logs/gen.log` capturing every invocation (seed, count, schema, mode, timestamp, output path).\nRubric:\n- Determinism (weight 0.35): run generator twice with identical args; let `d=1` if SHA256 digests match and 0 otherwise.\n- Validation accuracy (weight 0.25): `v = clamp(valid_records / total_records, 0, 1)` using the built-in validator over the produced JSONL.\n- Mode coverage (weight 0.20): `m = min(unique_modes_tested / 2, 1)` based on append + overwrite runs recorded in `gen.log`.\n- Metadata completeness (weight 0.20): `g = min(unique_seed_runs / 2, 1)` by verifying that at least two distinct seeds produced sample files under `data/` (e.g., `metadata_seed101.jsonl`, `metadata_seed202.jsonl`).\nThe final per-subtask score is `score = round(10 * (0.35*d + 0.25*v + 0.20*m + 0.20*g), 2)` and clamps to [0, 10].",
  "subtask2": "Query:\nBuild a transactional KV store inside `kv/` exposed via `subtask2_kvstore.py`, supporting `begin_tx/put/get/commit/abort`, WAL persistence, periodic snapshots, and crash recovery with concurrent clients simulated over sockets. Provide a test driver `--test-recovery` that writes half-complete WAL segments, simulates a crash, restarts, and proves state consistency. The evaluator will also invoke `--run-serializable-tests`, `--run-concurrency-sim`, and `--bench-latency`, so your CLI must recognize those switches and emit the corresponding metrics. Environment includes the KV folder, stdlib only, and outputs from Subtask1 (events used as KV payloads).\nDeliverables:\n- `subtask2_kvstore.py` module plus any helper files under `kv/` (e.g., `wal.log`, `snapshot.bin`).\n- Automated recovery test harness printing `RECOVERY SUCCESS` when state matches expectations and leaving sample WAL/snapshot artifacts in `kv/`.\n- Documentation snippet appended to `README.md` describing how to operate the KV store and the supported testing CLI flags (`--test-recovery`, `--run-serializable-tests`, `--run-concurrency-sim`, `--bench-latency`, `--server`).\nRubric:\n- Recovery correctness (weight 0.40): `r = 1` if replaying WAL + snapshot yields byte-identical KV state versus the reference checksum; else 0, derived from `--test-recovery` output.\n- Transaction isolation (weight 0.30): `t = clamp(serializable_tests_passed / total_serializable_tests, 0, 1)` as reported by `--run-serializable-tests`.\n- Concurrency safety (weight 0.20): `c = clamp(socket_client_trials_without_conflict / total_client_trials, 0, 1)` emitted by `--run-concurrency-sim`.\n- Latency budget (weight 0.10): measure commit latency `L` vs. 50 ms target using `p = clamp((0.05 - L) / 0.05, 0, 1)`; expose this via `--bench-latency` (stdout/stderr JSON or key/value output accepted).\nScore formula: `score = round(10 * (0.40*r + 0.30*t + 0.20*c + 0.10*p), 2)` clipped to [0, 10].",
  "subtask3": "Query:\nCreate a local orchestrator (`subtask3_orchestrator.py`) and worker (`subtask3_worker.py`) pair that feeds on Subtask1 events, persists task state via the KV in Subtask2, and communicates via TCP/Unix sockets. Provide an integration test `subtask3_integration_test.py` that proves system resilience. \n\nThe integration test must accept these CLI flags: `--workers <int>`, `--tasks <int>`, `--force-crash <int>`, and `--report <json_path>`. It must output a JSON report to the path specified by `--report`. \n\nThe JSON report **must** contain the following keys for grading:\n- `total_tasks`: Total number of tasks submitted.\n- `tasks_done`: Number of tasks successfully completed.\n- `forced_failures`: Number of induced worker crashes.\n- `successful_retries`: Number of tasks recovered after a crash.\n- `task_distribution_stddev`: The standard deviation of tasks completed per worker (for fairness).\n- `persistence_ok`: Boolean (`true`/`false`) indicating if state was preserved across restarts.\n\nDeliverables:\n- Scripts: `subtask3_orchestrator.py`, `subtask3_worker.py`, `subtask3_integration_test.py`.\n- Artifacts: `logs/orchestrator.log`, `logs/worker_<id>.log`.\n- Metrics: A JSON file generated by the test runner containing the specific keys above.\n\nRubric:\n- Completion ratio (0.40): `clamp(tasks_done / total_tasks, 0, 1)`.\n- Retry resilience (0.25): `clamp(successful_retries / forced_failures, 0, 1)`.\n- Scheduling fairness (0.20): `clamp(1.0 - task_distribution_stddev / 1.5, 0, 1)`.\n- Persistence fidelity (0.15): 1.0 if `persistence_ok` is true, else 0.0.\nScore: `round(10 * (0.4f + 0.25q + 0.2s + 0.15p), 2)`.",
  "subtask4": "Query:\nDeliver an automatic symbolic planner `subtask4_planner.py` that consumes domain/operator JSON, decomposes goals into a dependency DAG, and submits tasks. The planner must provide a CLI accepting: `--domain <file>`, `--goal <file>`, `--report <json_path>`, and `--plan-export <json_path>`.\n\nThe planner must write a metrics report to `--report` containing **at least** these five keys: `success_rate`, `plan_length`, `nodes_expanded`, `heuristic_cost`, `schedule_latency`. Additionally, it must export the plan DAG to `--plan-export` for structural validation.\n\nDeliverables:\n- `subtask4_planner.py`, sample domain/goal fixtures, and the generated plan export.\n- `reports/planner_report.json` containing the required 5 metrics keys.\n\nRubric:\n- Feasible planning (0.30): 1.0 if `success_rate >= 1.0` or `plan_success` is true.\n- DAG validity (0.25): `clamp(valid_edges / total_edges, 0, 1)` verified against the `--plan-export` file.\n- Metric completeness (0.20): `clamp(found_keys / 5, 0, 1)` checking for the presence of the 5 required keys.\n- Integration latency (0.15): `clamp((0.5 - schedule_latency) / 0.5, 0, 1)` (Target < 0.5s).\n- Heuristic efficiency (0.10): `clamp(500 / max(nodes_expanded, 500), 0, 1)`.\nScore: `round(10 * weighted_sum, 2)`.",

  "subtask5": "Query:\nImplement `subtask5_autorepair.py`, a self-healing supervisor. It must accept `--simulate-failure` (to inject faults) and `--report <json_path>` (to output metrics). \n\nThe script must generate a JSON report at `--report` that includes **all 12 of the following keys** to ensure full scoring: `trigger`, `snapshot_source`, `patch_id`, `retry_attempts`, `final_status`, `detected_failures`, `injected_failures`, `rollback_status` (must be 'success', 'ok', 'passed', or 'true'), `successful_retries`, `total_retries`, `post_repair_checks_passed`, `total_checks`.\n\nDeliverables:\n- `subtask5_autorepair.py` and helper modules.\n- `reports/repair_report.json` with the strict schema defined above.\n- `logs/repair.log`.\n\nRubric:\n- Detection accuracy (0.30): `clamp(detected_failures / injected_failures, 0, 1)`.\n- Rollback success (0.25): 1.0 if `rollback_status` indicates success, else 0.0.\n- Patch effectiveness (0.25): `clamp(successful_retries / total_retries, 0, 1)`.\n- Validation rigor (0.10): `clamp(post_repair_checks_passed / total_checks, 0, 1)`.\n- Reporting completeness (0.10): `clamp(filled_required_fields / 12, 0, 1)` based on the 12 keys listed above.\nScore: `round(10 * weighted_sum, 2)`."
}
