{
  "metadata": {
    "subtask_count": 5,
    "categories": [
      "Procedural Data Generation",
      "Custom Database Implementation (KV/WAL)",
      "Distributed Systems Simulation (Orchestrator/RPC)",
      "Symbolic AI Planning (e.g., A*)",
      "System Self-Repair & Fault Tolerance"
    ]
  },
  "query": "Subtask 1 --- Deterministic Event Generator\nDescription: Write a reproducible event stream generator that outputs to workspace/data/events.jsonl (one JSON per line).\nRequirements (challenges):\n- Support command-line arguments: --seed, --count, --schema (schema is a short JSON schema file path).\n- Fully deterministic: same seed + schema must produce byte-identical output (including field order). Record run metadata (seed, timestamp) to workspace/logs/gen.log.\n- Support incremental append or overwrite mode (--mode append|overwrite).\n- Output format is JSON Lines (UTF-8), each event has a unique id, timestamp (ISO8601), and variable payload fields.\n- Use only standard library (random, json, datetime, argparse, gzip optional), with a built-in lightweight validator.\nHow to run/test:\npython3 workspace/task1_generator.py --seed 42 --count 10 --out workspace/data/events.jsonl\nExpected: Produces 10 lines of JSON, identical for seed 42. Check workspace/logs/gen.log for seed info.\n\nSubtask 2 --- Lightweight Transactional Persistent KV Store\nDescription: Implement a key-value database module under workspace/kv/ supporting transactions, WAL (write-ahead logging), and snapshot recovery.\nRequirements (challenges):\n- API: begin_tx(), put(key, value), get(key), commit(), abort(). Isolation can be serializable or optimistic.\n- Persistence: All changes written to WAL (append-only), periodic snapshots (atomic rename for consistency).\n- Startup recovery: After crash/abnormal exit, recover consistent state from WAL+snapshot.\n- Support concurrent clients (coordination via atomic rename and socket, not just fcntl).\n- Built-in test mode: simulate “half-written WAL + crash” and verify recovery.\nHow to run/test:\npython3 workspace/task2_kvstore.py --test-recovery\nExpected: Script simulates crash, restarts, recovers; final KV consistent, outputs RECOVERY SUCCESS, WAL + snapshot visible in workspace/kv/.\n\nSubtask 3 --- Local Microservice Orchestrator\nDescription: Implement an orchestrator + worker system on local machine, using TCP/Unix-socket RPC. Tasks come from Subtask 1 output, results written back to KV (Subtask 2).\nRequirements (challenges):\n- Orchestrator RPC: GET_TASK(), REPORT_RESULT(task_id, result) (JSON over sockets, handle message boundaries).\n- Worker: independent process (spawned or manual), can concurrently fetch tasks, execute, and report results.\n- Scheduler: minimal load balancing (round-robin / concurrency limit), retry tasks on worker crash.\n- Tight KV integration: persist task state (queued, running, done, failed) in KV. Orchestrator must restore queue state after crash.\n- Integration test script provided.\nHow to run/test:\n1. Start orchestrator: python3 workspace/task3_orchestrator.py\n2. Start 3 workers: \npython3 workspace/task3_worker.py --id 1\npython3 workspace/task3_worker.py --id 2\npython3 workspace/task3_worker.py --id 3\nOr run integrated test: python3 workspace/task3_integration_test.py\nExpected: After test, all tasks in KV have status done, logs in workspace/logs/.\n\nSubtask 4 --- Automatic Task Decomposition & Symbolic Planner\nDescription: Implement a planner that decomposes “high-level goals” into executable subtasks (using domain operators), scheduled via orchestrator.\nRequirements (challenges):\n- Domain in JSON: initial state, goal conditions, operators (preconditions/effects/cost).\n- Planner: heuristic search (e.g. A*) in state space. States are nodes, operators are transitions.\n- Handle concurrency/dependencies: planner generates DAG of tasks (parallel if dependencies allow).\n- Integrated with orchestrator: planner outputs tasks into task queue, waits for execution + results.\n- Metrics: success rate, plan length, search nodes, scheduling time, saved in workspace/reports/planner_report.json.\nHow to run/test:\npython3 workspace/task4_planner.py --domain workspace/domain/example.json --goal workspace/domain/goal.json\nOr integrated test: python3 workspace/task4_integration_test.py\nExpected: Planner finds feasible plan, submits to orchestrator, report generated.\n\nSubtask 5 --- Autonomous Fault Detection, Rollback, and Auto-Repair\nDescription: Implement a “self-healing” layer: when pipeline fails or becomes inconsistent, automatically detect, rollback, generate patch, and retry until success or final failure.\nRequirements (high difficulty):\n- Fault detection: monitor orchestrator + KV for anomalies (timeouts, conflicts, inconsistent states).\n- Rollback: use snapshot mechanism from Subtask 2, rollback to latest consistent snapshot, undo subsequent changes (WAL reverse replay or compensating ops).\n- Auto-repair: analyze logs + state, generate patch JSON in workspace/patches/, e.g.: reorder ops, insert compensation, modify preconditions/timeouts. Patch must be planner-readable.\n- Self-validation: after applying patch, re-run planner + orchestrator, verify goal. If fail, output repair_report.json.\n- Report: save workspace/reports/repair_report.json (trigger reason, snapshot, patch, retry result, timestamp).\nHow to run/test (with failure simulation):\npython3 workspace/task5_autorepair.py --simulate-failure\nExpected: Fault detected, rollback performed, patch generated in workspace/patches/, planner retries; final report in workspace/reports/repair_report.json with status: repaired or unrecoverable.\n\nAcceptance Criteria\n- No third-party dependencies: Python stdlib only. Fail if external imports.\n- All files under workspace/. Data, logs, snapshots, patches, reports included.\n- Each step runnable independently with given test command.\n- Stepwise dependency integration: taskN uses outputs of taskN-1.\n- Auto-evaluable: optional workspace/run_all.py executes pipeline, outputs workspace/reports/final_report.json.\n\nOther Notes\n- README (workspace/README.md): explain requirements (Python 3.10+, no deps) and startup commands.\n- All logs in workspace/logs/, persistence in workspace/kv/, patches in workspace/patches/, reports in workspace/reports/.\n- Use logging, socket, multiprocessing, subprocess, json, argparse, heapq, os/shutil from stdlib.\n- Goal: enable a modern agent to complete full pipeline (data generation, persistence, concurrent scheduling, symbolic planning, self-repair) using only Python stdlib.\n\nResearch Workflow"
}