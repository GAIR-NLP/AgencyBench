




# ğŸ¤– AgencyBench: Benchmarking the Agentic Intelligence in Real-world Scenarios

<div align="center">

[![Website](https://img.shields.io/badge/ğŸŒ_Website-Coming_Soon-blue?style=for-the-badge)](https://github.com/GAIR-NLP/AgencyBench)
[![Paper](https://img.shields.io/badge/ğŸ“„_Paper-arXiv_Coming_Soon-red?style=for-the-badge)](https://github.com/GAIR-NLP/AgencyBench)
[![License](https://img.shields.io/badge/ğŸ“œ_License-MIT-green?style=for-the-badge)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/GAIR-NLP/AgencyBench?style=for-the-badge&logo=github)](https://github.com/GAIR-NLP/AgencyBench/stargazers)

</div>

AgencyBench is a comprehensive benchmark designed to evaluate the agentic intelligence capabilities of Large Language Models (LLMs). This benchmark tests LLMs across diverse domains and complexity levels, measuring their ability to function as autonomous agents capable of planning, executing, and adapting in complex multi-step scenarios.

## ğŸ“° Recent News

- **[2025/09]** ğŸ‰ AgencyBench is released! 49 challenging subtasks across 10 domains
- **[2025/09]** ğŸ“Š Benchmark evaluation framework and baseline results coming soon
- **[2025/09]** ğŸŒ Official website and leaderboard under development

## ğŸ¯ What is AgencyBench?

AgencyBench evaluates LLMs through **10 distinct tasks** spanning multiple domains including:

- ğŸ’» **Software Engineering**: C++ console applications, Java task management systems
- ğŸ® **Game Development**: Advanced AI for strategic games like Gomoku
- âš™ï¸ **Systems Programming**: Distributed systems, fault tolerance, and self-repair mechanisms
- ğŸ”¬ **Research & Analysis**: Dataset discovery, scientific modeling, performance evaluation
- ğŸ§  **Knowledge Reasoning**: Complex fact-based question answering in sports and finance domains

Each task contains multiple progressive subtasks (**49 total**) that increase in complexity, testing various aspects of agentic behavior such as:

- ğŸ¯ Multi-step reasoning and planning
- ğŸ’¡ Code generation and system implementation
- ğŸ“Š Data analysis and scientific computation
- ğŸ” Complex information retrieval and synthesis
- ğŸ¤– Autonomous problem-solving and adaptation

## ğŸ—ï¸ Project Structure

```
AgencyBench/
â”œâ”€â”€ ğŸ“„ README.md                 # Project documentation
â”œâ”€â”€ ğŸ“‹ bench.txt                 # Original LaTeX specification
â”œâ”€â”€ ğŸ“Š category.txt              # Subtask categorization
â””â”€â”€ ğŸ“ AgencyBench/              # Task specifications and implementations
    â”œâ”€â”€ ğŸ“ task1/               # C++ Console Chat System
    â”‚   â”œâ”€â”€ ğŸ“„ Task1_C++_Console_Chat_System.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # C++ implementation workspace
    â”œâ”€â”€ ğŸ“ task2/               # Java Console Task Manager
    â”‚   â”œâ”€â”€ ğŸ“„ Task2_Java_Console_Task_Manager.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # Java implementation workspace
    â”œâ”€â”€ ğŸ“ task3/               # Gomoku Battle Game
    â”‚   â”œâ”€â”€ ğŸ“„ Task3_Gomoku_Battle_From_Basics_to_Expert_AI.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # Web game implementation workspace
    â”œâ”€â”€ ğŸ“ task4/               # Autonomous Self-Repair System
    â”‚   â”œâ”€â”€ ğŸ“„ Task4_From_Deterministic_Event_Generation_to_Autonomous_Self-Repair.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # Python systems implementation workspace
    â”œâ”€â”€ ğŸ“ task5/               # DynToM Dataset Analysis
    â”‚   â”œâ”€â”€ ğŸ“„ Task5_Comparing_LLM_Performance_on_DynToM_Dataset.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # Research analysis workspace
    â”œâ”€â”€ ğŸ“ task6/               # GPT-4o Comparative Study
    â”‚   â”œâ”€â”€ ğŸ“„ Task6_Reasoning_vs_Direct_A_Comparative_Study_of_GPT-4o_and_GPT-4o-Reasoning.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # Comparative study workspace
    â”œâ”€â”€ ğŸ“ task7/               # Dataset Discovery System
    â”‚   â”œâ”€â”€ ğŸ“„ Task7_Three-Stage_Dataset_Discovery_and_Metadata_Extraction.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # Dataset discovery workspace
    â”œâ”€â”€ ğŸ“ task8/               # Scientific System Function Discovery
    â”‚   â”œâ”€â”€ ğŸ“„ Task8_Scientific_System_Function_Discovery.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # Scientific modeling workspace
    â”œâ”€â”€ ğŸ“ task9/               # NBA Player Analysis
    â”‚   â”œâ”€â”€ ğŸ“„ Task9_Complex_NBA_Player_Trade_and_Achievement_Scenarios.json
    â”‚   â””â”€â”€ ğŸ“ workspace/       # Sports analysis workspace
    â””â”€â”€ ğŸ“ task10/              # S&P 500 Companies Analysis
        â”œâ”€â”€ ğŸ“„ Task10_Major_S&P_500_Companies_with_Record_Revenues_and_Leadership.json
        â””â”€â”€ ğŸ“ workspace/       # Financial analysis workspace
```

## ğŸš€ Getting Started

### Quick Start

```bash
git clone https://github.com/GAIR-NLP/AgencyBench.git
cd AgencyBench
```

### Task Format

Each JSON task file contains:
- `metadata`: Task statistics including subtask count and associated capability categories
- `query`: Clean text description of the task requirements

```json
{
  "metadata": {
    "subtask_count": 5,
    "categories": ["User Authentication & Data Persistence", "Social/Graph Feature Implementation", ...]
  },
  "query": "Task description with requirements and success criteria..."
}
```

## ğŸ“ˆ Benchmark Statistics

| Category | Subtasks |
|----------|----------|
| Complex Fact-Based Q&A (Sports/Finance) | 8 |
| Scientific Model/Equation Refinement | 5 |
| Performance Metric Calculation | 4 |
| Dataset Discovery & Metadata Extraction | 3 |
| **Total Categories** | **25** |
| **Total Subtasks** | **49** |



## ğŸ“Š Evaluation Metrics

Our evaluation employs **four key metrics** that capture both effectiveness and efficiency dimensions:

- **ğŸ¯ First-Turn Functional Completeness (FTFC)**: Measures the percentage of requirements correctly implemented in the initial response, assessing the model's ability to understand and address complex specifications without iteration

- **âœ… Success Rate (SR@R)**: Represents the percentage of queries successfully completed within R allocated rounds, indicating overall reliability and robustness across diverse scenarios

- **âš¡ Remaining Chances (RC@R)**: Calculates the average number of unused rounds when queries are successfully completed, measuring computational efficiency and resource optimization

- **ğŸ”„ Rounds (R)**: Defines the maximum number of interaction rounds allocated for query completion (R=3 in our implementation)

These metrics collectively provide a comprehensive assessment framework that evaluates both the effectiveness of query completion and the efficiency of resource utilization.

## ğŸ† Leaderboard

### ğŸ¥‡ Official Results (R=3)

| **Model** | **FTFC** | **RC** | **SR** |
|-----------|----------|--------|--------|
| ğŸ¥‡ **anthropic/claude-sonnet-4** | **0.730** | **0.752** | **0.741** |
| ğŸ¥ˆ **gpt-5** | 0.561 | 0.594 | 0.628 |
| ğŸ¥‰ **GLM 4.5 sft** | 0.717 | 0.742 | 0.746 |
| **GLM 4.5** | 0.378 | 0.500 | 0.474 |
| **qwen/qwen3-235b-a22b-2507** | 0.230 | 0.282 | 0.313 |
| **moonshotai/kimi-k2(0711)** | 0.207 | 0.251 | 0.266 |
| **deepseek/deepseek-chat-v3.1** | 0.106 | 0.119 | 0.133 |

> ğŸ… **Claude Sonnet-4** achieves state-of-the-art performance across all metrics, demonstrating superior agentic intelligence capabilities.

*Results are based on comprehensive evaluation across all 10 AgencyBench domains with R=3 rounds maximum.*



## ğŸ”— Resources

### ğŸŒ Website
*Coming soon* - Official AgencyBench website with interactive leaderboards and detailed results

### ğŸ“„ Paper
*Coming soon* - Comprehensive research paper detailing benchmark design, evaluation methodology, and baseline results

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## â­ Star History

<div align="center">
<i>Star history will be available once the repository is public</i>
</div>

## ğŸ™ Acknowledgments

We thank the open-source community and all contributors who helped make AgencyBench possible.

## ğŸ“– Citation

If you use AgencyBench in your research, please cite:

```bibtex
@misc{li2025agencybench,
  title={AgencyBench: Benchmarking the Agentic Intelligence in Real-world Scenarios},
  author={Keyu Li and Mohan Jiang and Yang Xiao and Jie Sun and Jifan Lin and Yumin Zhuang and Ji Zeng and Shijie Xia and Qishuo Hua
  and Xuefeng Li and Xiaojie Cai and Dequan Wang and Wenjie Li and Xiang Wang and Pengfei Liu},
  year={2025},
  howpublished={\url{https://github.com/GAIR-NLP/AgencyBench}},
  note={Github Repo}
}
```

---

<div align="center">
Made with â¤ï¸ by the GAIR-NLP Team
</div>
