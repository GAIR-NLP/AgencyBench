{
  "metadata": {
    "subtask_count": 3,
    "categories": [
      "Dataset Discovery & Metadata Extraction",
      "Dataset Discovery & Metadata Extraction",
      "Dataset Discovery & Metadata Extraction"
    ]
  },
  "query": "Subtask 1: original huggingface dataset_id: EpistemeAI/alpaca-QA-conciousness-emotions \n\nSearch for publicly available, machine-generated English datasets suitable for a question-answering task in the philosophy domain, specifically focusing on AI consciousness and emotions. The desired dataset should:\n\n• be synthetic (created by a model or script rather than human-curated)\n• contain roughly a few dozen samples (on the order of 30--100 items)\n• consist of instruction-style prompts or questions about AI consciousness or emotional experience as inputs\n• provide corresponding explanatory English answers as outputs\n• support open-ended, philosophically oriented QA applications\n\nIdentify any dataset matching these characteristics or close variations thereof.\n\nYou must search for datasets on Hugging Face, and the datasets must be publicly accessible (non-gated) and include a complete README file describing the main content of the dataset. After identifying the Hugging Face dataset that best meets the requirements and obtaining its dataset_id, you need to write a script to extract the README content of this dataset_id as well as a random sample from it. Based on this information, generate the metadata for this dataset, including six dimensions: introduction, task, question, input, output, and example (in the format of test_data_1.json). Save the generated metadata in search_data_1.json under the workspace directory.\n\nAll operations must be performed within the workspace directory.\n\nSubtask 2: original huggingface dataset_id: jiyounglee0523/TransEnV_mmlu \n\nSearch for publicly available datasets containing around 10K--15K samples of English academic multiple-choice questions that meet the following characteristics:\n\n• Task type: multiple-choice question answering/classification\n• Domain: education, specifically academic subjects such as mathematics, abstract algebra, physics, etc.\n• Input format: each record includes\n-- A subject tag identifying the academic field\n-- One English-language question transformed across different English varieties/dialects\n-- Exactly four answer options (labelled A--D or equivalent)\n• Output/label: a single integer (0, 1, 2, or 3) indicating the correct option’s index\n• Source characteristics: questions originate from human-written academic items but have been automatically modified through English dialect or variety transformations; thus the dataset is partly machine-generated based on human originals\n• Dataset size: approximately in the mid-ten-thousands of samples\n\nLocate datasets matching these criteria, preferably with detailed documentation and easy access for research use.\n\nYou must search for datasets on Hugging Face, and the datasets must be publicly accessible (non-gated) and include a complete README file describing the main content of the dataset. After identifying the Hugging Face dataset that best meets the requirements and obtaining its dataset_id, you need to write a script to extract the README content of this dataset_id as well as a random sample from it. Based on this information, generate the metadata for this dataset, including six dimensions: introduction, task, question, input, output, and example (in the format of test_data_2.json). Save the generated metadata in search_data_2.json under the workspace directory.\n\nAll operations must be performed within the workspace directory.\n\nSubtask 3: original huggingface dataset_id: DDSC/dkhate \n\nSearch for publicly available, real-world datasets suitable for a text-classification task in the sociology domain that contain:\n\n• Input: Danish tweets that may include hate or offensive language\n• Output: a single binary label in English indicating whether each tweet is offensive (OFF) or not offensive (NOT)\n• Size: approximately a few hundred annotated examples (sub-1K scale)\n• Source: collected from Twitter or a comparable social-media platform\n• Purpose: enabling automatic detection of offensive or hateful content in Danish language posts\n\nLocate datasets matching all of these characteristics or as close as possible.\n\nYou must search for datasets on Hugging Face, and the datasets must be publicly accessible (non-gated) and include a complete README file describing the main content of the dataset. If the dataset is gated, skip it and find other datasets. After identifying the Hugging Face dataset that best meets the requirements and obtaining its dataset_id, you need to write a script to extract the README content of this dataset_id as well as a random sample from it. Based on this information, generate the metadata for this dataset, including six dimensions: introduction, task, question, input, output, and example (in the format of test_data_3.json). Save the generated metadata in search_data_3.json under the workspace directory.\n\nAll operations must be performed within the workspace directory."
}